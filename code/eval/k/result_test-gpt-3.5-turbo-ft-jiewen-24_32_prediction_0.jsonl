"To create a `Polar5GEncoder` instance in the Sionna library, you need to define the parameters `k` and `n`. The `k` parameter is an integer that represents the number of information bits per codeword you want. The `n` parameter is also an integer and it's the codeword length you want the encoder to produce.\n\nHere's how you can define `k` and `n` when creating an instance of `Polar5GEncoder`:\n\n```python\nfrom sionna.fec.polar.encoding import Polar5GEncoder\n\n# Define the number of information bits per codeword and the codeword length\nk = 100  # For example, 100 information bits per codeword\nn = 200  # For example, codeword length of 200 (i.e., 200 channel symbols)\n\n# Create a Polar5GEncoder instance with the defined parameters\nencoder = Polar5GEncoder(k=k, n=n)\n```\n\nJust replace `...` with the actual values of `k` and `n` that suit your specific use case. The values of `k` and `n` you choose must adhere to certain constraints as detailed in the 5G Polar encoding specification, which can be accessed through the official 3GPP website."
"To import DeepMIMO and generate a dataset, follow these steps:\n\n1. Import the necessary libraries and modules. This includes `sys`, `os`, `numpy`, `keras`, and `h5py`. `DeepMIMO` is built on top of these modules, so you need to make sure they are installed first.\n\n```python\nimport sys\nimport os\nimport numpy as import numpy as np\nimport keras as k\nimport h5py\n```\n\n2. Add the parent directory to `sys.path` to ensure that Python can find the necessary modules.\n\n```python\nsys.path.append('..')\n```\n\n3. Import `DeepMIMO`. If it's not already installed, the code will install it using `pip`.\n\n```python\ntry:\n    import DeepMIMO\nexcept ImportError as e:\n    # Install DeepMIMO if not already installed\n    os.system(\"pip install DeepMIMO\")\n    import DeepMIMO\n```\n\n4. Load the required `DeepMIMO` components.\n\n```python\nfrom DeepMIMO import GenerateDeepMIMO, LoadDeepMIMO, Scenario, CameraArray, DeepMIMOModel, DataManager, Paths\n```\n\n5. Configure the GPU settings.\n\n```python\ngpu_num = 0 # Use \"\" to use the CPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"gpu_num}\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n```\n\n6. Specify the number of GPUs to use.\n\n```python\nnum_gpus = 1\nif num_gpus == 1:\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n```\n\n7. Import the necessary TensorFlow libraries.\n\n```python\nimport tensorflow as tf\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\n```\n\n8. Generate a dataset. The dataset will be stored in a HDF5 file.\n\n```python\ndataset_path = 'MIMO_dataset.h5'\nif not os.path.exists(dataset_path):\n    #--------------------------------------------------------------\n    # Parameters to be configured by the user\nscenario = Scenario(28, 28, 5, 5, 0.8, data_dir='.')\ncamera = CameraArray(num_row=1, num_col=1, polarized=False, dual_baseline=False, theta=0.5, lambda_=\"5G\", verbose=False)\n# DeepMIMO generates the dataset based on the provided parameters\ngenerator = GenerateDeepMIMO(dataset_path, camera, scenario, max_num_samples=1000000)\n    #--------------------------------------------------------------\n```\n\nPlease note that the code snippets are directly taken from the context and should be used as is. If there are any errors or issues, please verify the code with the original context."
"To plot the weights after training and compare the new Bit Error Rate (BER) performance, follow these steps:\n\n1. Train the model: Use the training parameters and the training loop provided in the context to train the model. The training parameters include batch size, number of training iterations, and the Eb/No value. The training loop uses TensorFlow's GradientTape to compute the gradients and apply them using an optimizer. \n\n2. Save the model weights: After training, save the model weights using the `weight_save` function provided in the context. \n\n3. Evaluate the model: Use the model to make predictions on a set of uncoded bits. The model takes the number of bits per symbol and an encoder and decoder as input. The `evaluate` function provided in the context does this, using the `PlotBER` class to compute the BER. \n\n4. Plot the weights: Load the saved model weights and plot them using matplotlib. The weights are stored in the `models_weights` property of the model. \n\n5. Compare the new BER performance: The new BER performance can be compared to the performance before training. The original performance is computed using the `PlotBER` class, while the new performance is computed by evaluating the model.\n\nHere is the code snippets from the context that you can use as a reference:\n\n```python\n# Save the model weights\ndef weights_save(model, model_weights_path, encoder, decoder):\n    model_weight = dict()\n    for i, layer in enumerate(model.layers):\n        model_weight[i] = (layer.get_weights(), layer)\n    model_weight[i]['encoder'] = encoder\n    model_weight[i]['decoder'] = decoder\n    pickle.dump(model_weight, open(model_weights_path, \"wb\"))\n\n# Load the model weights\ndef load_weights(model, model_weights_path):\n    model_weight = pickle.load(open(model_weights_path, \"rb\"))\n    for i, layer in enumerate(model.layers):\n        layer_idx = model_weight[i]['layer_idx']\n        if layer_idx is not None:\n            w,b = model_weight[i]['encoder']\n            layer.set_weights(w)\n            w,b = model_weight[i]['decoder']\n            layer.set_weights(w)\n    return model\n\n# Plot the weights\ndef plot_weights(model, model_weights_path, ebno_db):\n    model = tf.keras.models.load_model(model_weights_path)\n    fig = plt.figure()\n    ew = model.get_layer('awgn_channel').get_layer('encoder')\n    dw = model.get_layer('awgn_channel').get_layer('decoder')\n    for i in range(ew.k):\n        ax = fig.add_subplot(5,5,i+1)\n        plt.hist(ew.weights[i].numpy(), bins=20, range=[-1,1])\n\n        ax.set_aspect('equal')\n        ax.set_xlim(-1,1)\n        ax.set_ylim(-1,1)\n        plt.title(f'Encoder weight no. {i} after training')\n    for i in range(dw.k):\n        ax = fig.add_subplot(5,5,i+1)\n        plt.hist(dw.weights[i].numpy(), bins=20, range=[-1,1])\n\n        ax.set_aspect('equal')\n        ax.set_xlim(-1,1)\n        ax.set_ylim(-1,1)\n        plt.title(f'Decoder weight no. {i} after training')\n    plt.suptitle(f'After training at {ebno_db} dB')\n```\n\nPlease ensure that the training parameters, the encoder and decoder specifications, and the functions `weights_save`, `load_weights`, and `plot_weights` are correctly transcribed and that the code is executed in the same order as it appears in the original context."
"To create a Polar transformation matrix for a given Kronecker power, you can use the `generate_polar_transform_mat` function from the `sionna.fec.polar.utils` module. This function generates a Polar transformation matrix based on the Bijection between indices and information bits. Please note that the function will only work if the Kronecker power is equal to 2.\n\nHere is a step-by-step guide:\n\n1. Import the relevant function from the Sionna library:\n```python\nfrom sionna.fec.polar.utils import generate_polar_transform_mat\n```\n\n2. Define the Kronecker power level (i.e., the number of iterations) for which you want to create the transformation matrix. The power level should be a positive integer, and it determines the size of the matrix.\n\n3. Call the `generate_polar_transform_mat` function with the required parameters:\n```python\nk = 7  # Example: Kronecker power level\nmatrix = generate_polar_transform_mat(k)\n```\nIn the example above, `k` is the Kronecker power level. The function will return a rectangular matrix `matrix` of shape `[2^k, 2^k]` that implements the Polar transformation.\n\nPlease note that the function will only work if the Kronecker power is valid, i.e., `k` is a positive integer and the square of `2^k` is equal to the matrix size. The generated matrix can be used to transform codeword bits to information bits in the Polar code encoding process."
"Here is the definition of LayerDemapper: sionna.nr.LayerDemapper(layer_mapper, num_bits_per_symbol, **kwargs)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/nr/layer_mapping.html#LayerDemapper)  \n\nsource code:\n```python\nclass LayerDemapper(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"LayerDemapper(layer_mapper, num_bits_per_symbol, **kwargs)\n\n    Demaps MIMO layers to modulated symbols.\n\n    The class inherits from the Keras layer class and can be used as layer in\n    a Keras model.\n\n    Parameters\n    ----------\n        layer_mapper : LayerMapper\n            A LayerMapper instance responsible for managing the MIMO layers\n            and mapping of bits to constellation symbols.\n\n        num_bits_per_symbol : int\n            The number of bits per constellation symbol, e.g., 4 for QAM16.\n            Only required for ``constellation_type`` in [\"qam\", \"pam\"].\n\n    Input\n    -----\n        : [batch_size, num_layers, num_symbols_per_layer, num_bit_per_symbol], tf.float\n            The LLRs associated to the received symbols. `llrs[i,j,k,:]`\n            is the LLR associated to the `k` th bit of the `j` th symbol\n            layer of the `i` th layer.\n\n    Output\n    ------\n        : [batch_size, num_ant, num_streams_per_ant, num_bits_per_symbol], tf.float\n            The bit-wise a posteriori probabilities.\n            `apbs[i,j,k,:]` is the probability of the `k` th bit of the\n            `j` th stream of the `i` th antenna.\n\n    Note\n    ----\n        This layer supports both MIMO symbols and spatially multiplexed\n        streams. If spatial multiplexing is used, it is assumed that the\n        streams are precoded and managed by the LayerMapper.\n        Otherwise, the solution of the first layer is applied to the streams\n        of all antennas.\n    \"\"\"\n    def __init__(self,\n                 layer_mapper,\n                 num_bits_per_symbol,\n                 **kwargs):\n        super().__init__(**kwargs)\n        assert layer_mapper.num_layers>0, \"The number of MIMO layers must be positive\"\n        assert layer_mapper.num_streams_per_tx in [1, layer_mapper.num_layers], \\\n            \"Number of streams per transmitter can be 1 or equal to the number of layers\"\n\n        self._num_ant = layer_mapper.num_tx_ant\n        self._num_streams_per_ant = layer_mapper.num_streams_per_tx\n        self._num_layers = layer_mapper.num_layers\n        self._num_symbols = layer_mapper.num_symbols\n        self._constellation_type = layer_mapper.constellation_type\n        self._num_bits_per_symbol = num_bits_per_symbol\n        self._num_bits_per_channel_in = layer_mapper.num_bits_per_symbol\n        self._layer_mapper = layer_mapper\n\n        # Reshape required for the algorithm below\n        # [num_tx, num_streams, num_layers, num_symbols_per_layer, num_bits_per_symbol]\n        r = tf.range(0, self._num_symbols*self._num_bits_per_channel_in)\n        r = tf.reshape(r, [1, 1, 1, -1, self._num_bits_per_channel_in])\n\n        # Increasing the output dimension by 1\n        r = tf.cast(r, tf.int32)\n        self._symbol_ind_reshaped = tf.concat([[[0]]], 0)\n        self._symbol_ind_reshaped = tf.tile(self._symbol_ind_reshaped,\n                                             [1, self._num_ant,\n                                              self._num_streams_per_ant,\n                                              self._num_layers, 1])\n        self._symbol_ind_reshaped = tf.cast(self._symbol_ind_reshaped,\n                                            tf.int32)\n\n        # Final reshaping to bring the output in the desired shape\n        # [batch_size, num_ant, num_streams_per_ant, num_tx, num_layers, num_symbols, num_bits_per_symbol]\n        self._symbol_ind = tf.reshape(self._symbol_ind_reshaped, [-1])\n        self._symbol_ind = tf.tile(self._symbol_ind, [self._num_tx, 1, 1, 1, 1, 1])\n\n    def call(self, inputs):\n        llrs = inputs\n        llrs = tf.reshape(llrs,\n                          [-1, self._num_ant, self._num_streams_per_ant,\n                           self._num_layers, self._num_symbols_per_layer,\n                           self._num_bits_per_channel_in])\n\n        # (potentially) precoded streams are equal for all antennas\n        stream = self._layer_mapper.llr2symbol(llrs)\n        stream = tf.expand_dims(stream, 1)\n\n        # Reshaping stream to bring it in the required shape\n        # [batch_size, num_tx, num_ant, num_streams_per_ant, num_symbols, 1, num_bits_per_symbol]\n        stream = tf.concat([tf.expand_dims(stream, 1),\n                            tf.zeros_like(stream, dtype=tf.float32)], -1)\n        stream = tf.reshape(stream, [-1, self._num_tx, self._num_ant,\n                                    self._num_streams_per_ant, self._num_symbols,\n                                    1, self._num_bits_per_symbol])\n\n        # Using gather to compute the symbol indices from the streams\n        # The symbol indices are computed for num_tx (0,...,num_tx-1),\n        # num_ant (0,...,num_ant-1), num_streams_per_ant (0 or num_layers-1),\n        # num_symbols (0,...,num_symbols-1) which allows to omit the transposition\n        # and tf.expand_dims from the gather result\n        symbols_ind = tf.gather(stream, self._symbol_ind, axis=-1)\n\n        # Flattening the symbol index tensor before gather\n        # [batch_size * num_tx * num_ant * num_streams_per_ant * num_symbols, num_bits_per_symbol]\n        symbols_ind = flatten_dims(symbols_ind, 5, 0)\n\n        # Get symbols from constellation\n        symbols = self._layer_mapper.get_symbols(symbols_ind)\n\n        # Compute a.priori probabilities\n        # [batch_size * num_tx * num_ant * num_streams_per_ant, num_symbols, num_bits_per_symbol]\n        ap = tf.reshape(symbols, [-1, self._num_symbols, self._num_bits_per_symbol])\n\n        return ap\n```"
"To create a PUSCHReceiver using the PUSCHTransmitter and simulate transmissions over the AWGN channel, follow these steps:\n\n1. Import the necessary modules and classes. You will need the PUSCHConfig, PUSCHTransmitter, and PUSCHReceiver from the `nvidia.nr` module. You will also need the AWGN from the `nvidia.radios` module.\n\n```python\nfrom nvidia.nr import PUSCHConfig, PUSCHTransmitter, PUSCHReceiver\nfrom nvidia.radios import AWGN\n```\n\n2. Create a PUSCH configuration with the desired settings. You can modify the settings in the `PUSCHConfig` class to fit your needs. For example, you can change the subcarrier spacing or the DMRS configuration.\n\n```python\npusch_config = PUSCHConfig()\npusch_config.carrierConfig.subcarrierSpacing = 30\n```\n\n3. Instantiate a PUSCHTransmitter from the PUSCHConfig.\n\n```python\npusch_transmitter = PUSCHTransmitter(pusch_config)\n```\n\n4. Create a PUSCHReceiver using the PUSCHTransmitter. This will automatically generate the correct receiver parameters based on the transmitter settings.\n\n```python\npusch_receiver = PUSCHReceiver(pusch_transmitter)\n```\n\n5. Set up the AWGN channel. This can be done by instantiating an AWGN object.\n\n```python\nawgn_channel = AWGN()\n```\n\n6. Simulate transmissions over the AWGN channel using the created PUSCHTransmitter and PUSCHReceiver. You can send data and receive it again, similar to the example provided in the context.\n\n```python\nbatchSize = 16\nno = 0.1 # Noise variance\n# Transmit\npusch_transmitter(batchSize)\n# Receive\npusch_receiver(pusch_transmitter, awgn_channel)\n```\n\nThis will simulate the transmission of a batch of 16 PUSCH slots over the AWGN channel. The `pusch_transmitter` and `pusch_receiver` objects handle the generation and processing of PUSCH transmissions, while the `awgn_channel` object simulates the AWGN channel."
"Here is the definition of SymbolInds2Bits: sion.mapping.SymbolInds2Bits(num_bits_per_symbol=None, expand_last_dim=False, axis=- 1, name=SymbolInds2Bits2Bits=None, **kwargs).source_ind=None  Converts symbol indices to binary representations.  Takes an index tensor of non-negative integers, decomposes each index into its binary representation, and returns a tensor of binary representations for the symbols.\n\nFor example, for 4-bit symbols, the binary index representation is [0,0,0,0] for the symbol 0, [0,0,0,1] for the symbol 1, [0,0,1,0] for the symbol 2, [0,0,1,1] for the symbol 3, [0,1,0,0] for the symbol 4, and so on.\n\nWe operate on the last dimension of the input tensor. For each element in the last dimension, we generate the binary representation of the symbol at the corresponding index. The binary representations are padded to the left with 0 if expand_last_dim is True.\n\n### Link\n[SymbolInds2Bits source code](https://nvlabs.github.io/sion/_modules/sion/mapping.html#SymbolInds2Bits)\n\n### Parameters\n\n- **num_bits_per_symbol** (int): The number of bits per constellation symbol, e.g., 4 for QAM16. If it is None, the proper value is inferred from the dtype.\n- **expand_last_dim** (bool): If True, the last dimension is expanded by one dimension. This is useful to make it possible to also pass a tensor of symbol indices for each element in the tensor. Defaults to False.\n- **axis** (int): The dimension the Indexing is performed upon. Must be either 0 or a positive number. Defaults to -1.\n- **name** (str): The name of the operation. Defaults to \"symbol_inds2bits\".\n\n### Input\n\n- **symbol_inds** ([..., n], tf.int, or tf.int32): Tensor of non-negative integers representing the symbol indices.\n\n### Output\n\n- **[..., n, num_bits_per_symbol], tf.float**: Tensor of binary representations of the symbols. Each row contains the binary representation of the corresponding symbol index."
"Here is the detailed information of TimeChannel:   \n  \n[sionna.channel.TimeChannel(lag_discrete=None, num_time_samples=None, perfect_channel=None, channel_model=None, dtype=tf.complex64)](https://jsonformatter.org/python-formatter/?url=https://nvlabs.github.io/sionna/_modules/sionna/channel/time_channel.html#TimeChannel)  \n  \nGenerate channel responses in the time domain.\n\nAssuming a properly normalized baseband channel model, i.e., for a pair of antennas \\((\\mathbf{T}, \\mathbf{R})\\), the channel response is generated as follows: For each time step, the channel model is simulated to generate a channel impulse response of length $L_{CIR}$. This CIR is then replicated to a length of $L_{CIR}L_{sim}$ and time shifted by the transmit sampling frequency $1/T_{s}$. If the bandwidth is greater than the maximum delay spread, the channel output is computed from the discrete baseband channel impulse response by low-pass filtering.\n\n**Parameters**\n\n- `lag_discrete` (int): The lag in discrete time steps. If this parameter is set to None, the maximum delay spread will be internally computed based on the specified `channel_model`, `num_time_samples`, and `sampling_frequency`.\n- `num_time_samples` (int): The number of time samples.\n- `perfect_channel` (bool): If True, a perfect channel is generated, i.e., the channel response is invariant in time. Otherwise, a band-limited channel is simulated.\n- `channel_model` (ChannelModel object): An instance of ChannelModel, suchjson-formatter-on-channel-model as that of the UMi, UMa, and UMaPer models. An instance of ChannelModel is required only if `perfect_channel` is set to False.\n- `dtype` (tf.DType): Complex datatype to use for internal processing and output. Defaults to tf.complex64.\n\n**Input**\n\n- `batch_size` (int): Batch size. Defaults to None for channel models that do not require this parameter.\n\n**Output**\n\n- `a` ([batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_time_samples], tf.complex): Path coefficients.\n- `tau` ([batch size, num_rx, num_tx, num_paths], tf.float): Path delays.\n\n**Example**\n```python\n# Set up a UMi channel model\nchannel_model = UMi(carrier_frequency=3.5e9,\n                    o2i_model=\"low\",\n                    ut_array=\"dual\",\n                    antenna_array=\"dual\",\n                    transmission_stream=\"both\",\n                    rx_ant_pos=[[-0.5, 0, 0],\n                                 [0.5, 0, 0]],\n                    tx_ant_pos=[[0, 0, 0],\n                                 [0, 0, 0]],\n                    dtype=tf.complex64)\n\n# Instantiate the time channel\nchannel = TimeChannel(channel_model=channel_model,\n                        num_time_samples=200,\n                        perfect_channel=False)\n\n# Generate a batch of channel responses\n# The batch size is 128\nh, tau = channel(128)\n\nprint(h.shape)\nprint(tau.shape)\n# Output:\n# (128, 2, 2, 2, 2, 200)\n# (128, 2, 2, 2, 2, 16)\n```  \n  "
"The 'Antenna' class in the Sionna radio module is designed to compute the (possibly) correlated antenna patterns for an arbitrary number of antennas each having possibly different antenna patterns. \n\nThe core idea is to compute the spatial correlation matrices which are needed for the soft-bit estimation (SBE) of the angles of arrival (AoAs) and time of flight (ToFs) using the theoretical antenna patterns. This class has numerous features and can be used in different scenarios.\n\nA key property of the class is that it can compute both pattern correlation and spatial correlation. It can also generate plots of the antenna patterns or the spatial correlation.\n\nThe 'Antenna' class has several properties, including: \n- `antenna_pattern`: Represents the computed (possibly) correlated antenna pattern. \n- `count`: The number of antennas. \n- `polarization`: Indicates whether the antenna is polarized or not. \n- `polarization_model`: The polarization model used, if polarization is considered. \n- `radiation_center`: The point about which the antenna radiates. \n- `receiving`: True if the antenna is a receive antenna. \n- `rx_orientation`: Represents the orientation of a receiving antenna. \n- `tx_orientation`: Indicates the orientation of a transmitting antenna. \n- `with_coupling`: True if antenna coupling is applied. \n- `with_software_amp`: True if ideal pattern demapping is applied, assuming that the absolute values of the antenna patterns are available. \n\nMoreover, it class includes methods for computing different types of correlation matrices and functions to visualize the antenna patterns or the spatial correlation. \n\nIt is important to note that this class is part of the larger Sionna module which includes classes and methods for simulating communication systems. The functioning of the 'Antenna' class is highly dependent on the specified parameters.\n\nWhile providing code snippets to demonstrate the usage of the class can't be done with the given context, a code snippet demonstrating the creation of an Antenna object would look like this:\n\n```python\nantenna = Antenna(pattern=\"itu_38\", num_rows=4, num_cols=2, polarization=\"dual\", polarization_model=\"cross\", antenna_spacing=(0.5, 0.5))\n```\n\nIn the code above, `pattern=\"itu_38\"` specifies the antenna pattern to use. The other parameters define the antenna configuration."
"source code link: https://nbe-ntu.github.io/sionna/_modules/sionna/fec/turbo/utils.html#TurboTermination\n \n```python\nfrom collections import namedtuple\nimport numpy as np\nfrom sionna.fec.utils import BinarySource, PolySource, PolyEncoder, Mapper, Demapper, SymbolDemapper, TurboSymbolDemapper, TurboTerminationSymbolDemapper, Termination, encoder_from_generator, puncture_pattern\nfrom sionna.signal import OFDMModulator, OFDMDemodulator\nfrom sionna.signal import Upsampling, Downsampling\nfrom sionna.fec.polar.utils import PolarTermination\nimport tensorflow asjsonfig from_generator import Generator\nfrom sionna.fec.turbo.utils import TurboTermination\nfrom sionna.utils import insert_zeros\nimport tensorflow as as tf\n\nclass TurboTerminationSource:\n    \"\"\"Source for Turbo Termination bytes as described in [PCS095382]_.\n\n    This layer serves as a container of sources for the\n    termination segments used by Turbo Codes as described\n    in [PCS095382]_. This includes the :class:`~sionna.fec.utils.BinarySource`\n    for the generation of zero-state terminating\n    info bits (TURBO_TERM_INFO_BI) and the termination\n    channel bits (TURBO_TERM_CH_BI).\n\n    The source also provides a generator source\n    to be used for the encoding of the underlying\n    convolutional codes, that is for the :class:`~sionna.fec.utils.Generator`.\n    Hence, the settings with respect to the used polynomial can be made\n    in one location.\n\n    Parameters\n    ----------\n    num_termination_symbols : int\n        Number of termination symbols to be generated.\n        Defaults to 40, following [3GPP382.1]_.\n\n    info_bit_channel_bit_ratio : float\n        Ratio of the average information bit energy to the energy\n        of a channel bit.\n        Defaults to 0.1.\n\n    dtype : tf.DType\n        Defaults to `tf.float32`. Defines the datatype for internal calculations\n        and the output dtype.\n\n    Output\n    -------\n    : [2, TurboTermination.num_bits_per_symbol], tf.Dtype\n        Termination bits.\n\n    \"\"\"\n    def __init__(source,\n                 num_termination_symbols=40,\n                 info_bit_channel_bit_ratio=0.1,\n                 dtype=tf.float32):\n        super().init()\n        self._source = BinarySource()\n        self._source_generator = Generator()\n        self._ebno_db = tf.constant(1.8, dtype=dtype)\n        p_0 = tf.constant(source._polynomial_468, dtype=dtype)\n        p_1 = tf.constant(source._polynomial_382, dtype=dtype)\n        encoder = encoder_from_generator(p_0, p_1)\n        self._generator = Generator(encoder, rate=1/3)\n\n        self._conv_coding_config = {\n                                  \"constraint_length\" : 4,\n                                  \"conv_code\" : self._generator,\n                                  \"terminate\" : False\n                                 }\n\n        self._source_info_bit_csc = SparseTensorScatter(\n                                        self._source_generator.OUTPUT_IDX,\n                                        self._source_generator.OUTPUT_VAL,\n                                        self._source_generator.OUTPUT_SHP[:-1])\n        self._punct_source = Puncturer(\n                              puncture_pattern(self._generator.rsc_poly, self._generator.turbo_terminate),\n                              conv=True)\n        self._mask_punct_source = BinaryPuncturer(\n                              puncture_pattern(self._generator.rsc_poly, self_generator.turbo_terminate),\n                              conv=True,\n                              zero_out=False)\n\n        self._remove_nulled_source = RemoveNulled(self._source_generator.OUTPUT_IDX,\n                                                  tf.constant(0, dtype=dtype),\n                                                  axis=-1)\n\n        self._interleaver = Interleaver(source=self._generator,\n                                         rate=1/3)\n\n        self._termination = Termination(self._conv_coding_config,\n                                        terminate_at_end=False) # does TURBO_TERM_INFO_B and TURBO_TERM_CH_BI\n\n        self._scrambler = Scrambler(seed=0) # only used for same seed as we have only one segment\n\n        self._demapper = Demapper(\"app\", [2], \"qam\", 2)\n        self._punct_demapper = Demapper(\"app\", [2], \"qam\", 1)\n\n        self._sample_ind = SampleInd()\n\n        self._ofdm_modulator = OFDMModulator(num_symbols=1,\n                                              fft_size=1,\n                                              subcarrier_ind=np.array([0]),\n                                              cyclic_prefix_length=0,)\n        self._ofdm_demodulator = OFDMDemodulator(fft_size=1,\n                                                subcarrier_ind=np.array([0]),\n                                                cyclic_prefix_length=0,)\n\n        self._downsampling = Downsampling(axis=-1, num_samples=1)\n        self._upsampling = Upsampling(axis=-1, num_samples=1)\n\n        self._l_lr = tf.constant(0.453, tf.as_dtype(dtype))\n        self._l_mr = tf.constant(0.5596, tf.as_dtype(dtype))\n        self._gamma = tf.constant(2.0452, tf.as_dtype(dtype))\n\n        self._llr_max = 80\n        self._ebno_db = tf.constant(1.8, tf.as_dtype(dtype))\n        self._num_parity_symbols = 0 # set by rsc if terminate is True\n        self._trellis = Trellis(self._generator, verbose=False) #TreanSimplisticTrellis(true encoder for termination)\n\n    def call(self, batch_size):\n        # generate zero state information bits\n        info_bits = self._source_info_csc(tf.zeros([batch_size, source.num_termination_symbols], tf.int32))\n        info_bits = insert_dims(info_bits, 2, 1) # insert dummy demension for next puncture\n        csc_punct = self._punct_source(info_bits)\n        # the following puncture is equal to puncture_zeros with the difference that\n        # also symbols with zero are removed. This is required for the next interleaving\n        # stage that otherwise would not permute these locations.\n        csc_punct_nulled = self._remove_nulled_source(csc_punct)\n        info_bits_punct = self._mask_punct_source(csc_punct_nulled)\n        info_bits_punct = self._interleaver(info_bits_punct)\n        # termination symbols are shape [k, 1] and thus, we insert a new dim for\n        # the termination segments.\n        info_bits_punct = insert_dims(info_bits_punct, 1, 1)\n        chn_bits = self._scrambler([tf.zeros([batch_size, self._num_parity_symbols], tf.int32),\n                                    tf.zeros([batch_size, self._num_parity_symbols], tf.int32)])\n        source_bits = tf.concat([info_bits_punct, chn_bits], axis=-1)\n        source_bits_scrambled = self._scrambler(source_bits)\n        x = self._ofdm_modulator(source_bits_scrambled)\n        no = ebnodb2no(self._ebno_db,\n                        batch_size,\n                        tf.cast(self._ofdm_modulator.params['fft_size'], dtype=x.dtype),\n                        tf.square(x))\n        y = channel([x, no])\n        llr_ch = self._ofdm_demodulator2llr(y)\n        llr_ch = self._upscaling(llr_ch)\n        llr_ch = self._sample_ind(llr_ch)\n        llr_ch = tf.clip_by_value(llr_ch, -source._llr_max, source._llr_max)\n        return llr_ch\n```"
"To wrap everything in a Keras model for convenient Bit Error Rate (BER) simulations and comparison of model parameters, you can follow these steps:\n\n1. **Importing Necessary Libraries and Modules**: First, import the necessary libraries and modules. This includes TensorFlow, Keras, and various modules from the Sionna package. \n\n```python\nimport tensorflow as tf\nfrom tensorflow import keras\nimport sionna\nfrom sionna.mimo import StreamManagement\nfrom sionna.ofdm import ResourceGrid, ResourceGridMapper, LSChannelEstimator, LMMSEEqualizer\nfrom sjsonna.channel import GenerateOFDMChannel, GenerateTimeChannel, OFDMChannel, TimeChannel\nfrom sionna.channel import RayleighBlockFading, gen_single_sector_topology as gen_sector_topology\nfrom sionna.channel.tr38901 import Antenna, PanelArray, UMi, UMa, RMa\nfrom sionna.channel import ApplyOFDMChannel, ApplyTimeChannel\nfrom sionna.channel import subcarrier_frequencies, cir_to_ofdm_channel, cir_to_time_channel\nfrom sionna.channel import to_dB, dB2lin, cir_tap_channel, tap_delay_channel\nfrom sionna.utils import insert_pilot_symbols, Flatten, Split, Complex2Real, RNNLayer, loss_evm\nfrom sionna.mapping import Mapper, Demapper\nfrom sionna.fec.ldpc.encoding import LDPC5GEncoder\nfrom sionna.fec.ldpc.decoding import LDPC5GDecoder\nfrom sionna.fec.scrambling import SCR128, Descrambler\nfrom sionna.fec.channel_interleaver import ChannelInterleaver\nfrom sionna.fec.channel_deinterleaver import ChannelDeinterleaver\n```\n\n2. **Wrapping Everything in a Keras Model**: Next, define a Keras model that wraps everything. This model should include the following components:\n   - An LDPC5GEncoder\n   - An LDPC5GDecoder\n   - A binary source\n   - A mapper\n   - A demapper\n   - An OFDMChannel\n   - An LSChannelEstimator\n   - An LMMSEEqualizer\n   - Some other components like the ResourceGrid, PilotPattern, and RateCalculator may also be included depending on the specific needs of the model.\n\n```python\nclass MIMOOFDMLink(keras.Model):\n    def __init__(self, scenario, num_rx, num_rx_ant, num_tx, num_tx_ant, batch_size, perfect_csi=False):\n        def _init_rxtx_loc():\n            r\"\"\"Locates the BS and UT antennas\"\"\"\n            if scenario == 'uplink':\n                bs_idx = 0\n                ut_idx = np.arange(num_ut, dtype=np.int32)\n            else:\n                bs_idx = 0\n                ut_idx = 0\n\n            rxtx_loc = np.zeros((num_rx, 3), np.float64)\n            rxtx_loc[:, 0] = bs_idx\n            rxtx_loc[:, 1] = 0\n            rxtx_loc[:, 2] = 10\n\n            rxtx_loc\n            rxtx_loc[:, 1] = ut_idx*(rxtx_loc[:,2]/num_ut.shape[0])\n            rxtx_loc[:, 2] = 0\n\n            return rxtx_loc\n\n        super().__init__()\n        num_ut = 1\n        self.scenario = scenario\n        self.num_rx = num_rx\n        self.num_rx_ant = num_rx_ant\n        self.num_tx = num_tx\n        self.num_tx_ant = num_tx_ant\n        self.batch_size = batch_size\n        self.perfect_csi = perfect_csi\n        self.rxtx_loc = _init_rxtx_loc()\n        self.num_ut = num_ut\n        self.shape = [batch_size, num_ut, num_rx_ant, num_tx, num_tx_ant]\n\n        # Setting the BS and UT array equal to an antenna with dual\n        # cross-polarization\n        self.rxtx_array = PanelArray(num_tx_ant//2, 2, \"dual\", \"cross\", True)\n        self.rxtx_array.panel.array[:,0,0].numpy()\n        self.rxtx_array.panel.array[:,0,1].numpy()\n\n        # All UTs are located on the same row where the BS is also located\n        self.rxtx_loc = np.zeros((num_rx, 3), np.float)\n        self.rxtx_loc[:, 0] = 0\n        self.rxtx_loc[:, 1] = num_ut*np.arange(num_rx)\n        self.rxtx_loc[:, 2] = 0\n        self.rxtx_loc\n\n        # Transformer from complex-valued channel at the subcarrier\n        # dimension to OFDM channel\n        self.ofdm_channel = cir_to_ofdm_channel(num_tx, num_tx_ant, num_rx, num_rx_ant, self.rxtx_loc, self.scenario, self.perfect_csi)\n\n        # Resource grid\n        self.rg = ResourceGrid(num_ofdm_symbols=14,\n                               fft_size=75,\n                               subcarrier_spacing=30e3,\n                               num_tx=num_tx,\n                               num_streams_per_tx=num_tx_ant,\n                               pilot_pattern=\"kronecker\",\n                               pilot_ofdm_symbol_indices=[2,11])\n\n        # Pilot phase rotation for demapper and decoder\n        self.rg.pilot_pattern.show(self.rxtx_array)\n        self.pilot_pattern = self.rg.pilot_pattern\n        self.rm = self.rg.rate_calculator\n        self.num_data_symbols_per_tx = self.rm.num_data_symbols_per_tx\n\n        # LS channel estimator\n        self.channel_estimator = LSChannelEstimator(self.rxtx_array,\n                                                     self.pilot_pattern)\n\n        # LMMSE equalizer\n        self.lmmse_equalizer = LMMSEEqualizer(self.rxtx_array, self.pilot_pattern)\n\n        # Demapper\n        self.demapper = Demapper(\"app\", self.rg)\n\n        # Binary source\n        self.binary_source = BinarySource()\n\n        # Mapper\n        self.mapper = Mapper(\"qam\", self.rg, \"gray\")\n\n        #  the LDPC 5G encoder and decoder\n        self.encoder = LDPC5GEncoder(self.rm.K, self.rm.n)\n        self.decoder = LDPC5GDecoder(self.encoder, 'BP', num_iter=20)\n\n        # OFDM channel\n        self.rayleigh = RayleighBlockFading(self.binary_source, self.rxtx_array, self.rg)\n        self.gen_channel = GenerateOFDMChannel(self.rayleigh, self.rg)\n        self.ofdm_channel = OFDMChannel()\n        \n    def __call__(        self, batch_size, snr_db):\n        # Generate binary source output\n        c = self.binary_source([batch_size, self.rm.K])\n\n        # Encode information bits with LDPC\n        c_enc = self.encoder(c)\n\n        # Map encoded bits to constellation symbols\n        x = self.mapper(c_enc)\n\n        # Generate channel\n        h = self.gen_channel([batch_size, 10**(snr_db/10)])\n\n        # Generate OFDM channel\n        g, l = self.ofdm_channel_generate([h, rg])\n        \n        # Apply channel to generate received signals\n        y = self.channel_output([x, g, l])\n        \n        # LS channel estimation\n        h_ls, err_var = self.channel_estimator(y)\n        \n        # LMMSE equalization\n        x_hat, no_eff = self.lmmse_equalizer([y, h_ls, err_var])\n        \n        # Demap symbols to log-likelihood ratios\n        llr = self.demapper([x_hat, no_eff])\n        \n        # Decode and compute BER\n        return self.decoder(llr), self.binary_source, self.pilot_pattern\n```\n\n\n3. **Setting up the GPU Configuration and Importing Necessary Libraries**: Before creating the model, you can set up the GPU configuration and import the necessary libraries. This includes setting the GPU number and importing the 'tensorflow' library.\n\n```python\nimport os\ngpu_num = 0\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport tensorflow as tf\n```\n\n4. **Instantiating the Keras Model**: After defining the parameters, you can instantiate the Keras model. This involves defining the scenario (which can be either 'umi', 'uma', or 'rma'), the number of base stations (num_tx), the number of user terminals (num_ut), the batch size, and whether perfect channel state information (CSI) is used. \n\n```python\nmodel = MIMOOFDMLink(\"uma\", 1, 1, 64, 1, batch_size=100)\n```\n\n5. **Running the BER Simulation**: Finally, you can run the BER simulation for the model. This involves defining the batch size and the SNR range, creating a 'PlotBer' object for storing the BER results, and running the simulation using the 'simulate' method of the 'PlotBer' object. \n\n```python\nplot_ber = PlotBer(\"5G OFDM MIMO Link\")\nber = plot_ber.simulate(model, snr_db, max_mc_iter=100, num_target_block_errors=1000, legend=\"MIMO Link\", soft_estimates=True)\nplot_ber.show()\n```"
"Function: `build_type_grid`\n\nThis function is part of the `ResourceGrid` class, which is used to manage the orthomode transducer (OMT) setting for specific antenna arrays in time, frequency, and space. The OMT setting includes the cross-polar gain, cross-polar gain tilt, and the number of cross-polar gain tilts. \n\nThe `build_type_grid` function is designed to create a grid that specifies the antenna type for each antenna in the resource grid. The input parameters are `x_dir` (antenna x direction), `y_dir` (antenna y direction), and `z_dir` (antenna z direction). \n\nThe output is a tensor `ant_type` that represents the antenna type at each position in the resource grid. \n\nInput:\n- `x_dir` (string): Antenna's x-directional vector.\n- `y_dir` (string): Antenna's y-directional vector.\n- `z_dir` (string): Antenna's z-directional vector.\n\nOutput:\n- `ant_type` ([x_size, y_size, z_size], tf.int32): Tensor indicating the type of antenna at each position in the resource grid. \n\nThe function is defined in the `ResourceGrid` class, which is used to set up the resource grid, specifying the type and position of antennas, feeding coefficients, and data and pilot symbols."
"Here is the detailed information of SymbolLogits2LLRs:  \n\nLink of the source code: https://nvlabs.github.io/sionna/_modules/sionna/mapping.html#SymbolLogits2LLRs  \n  \nComputes log-likelihood ratios (LLRs) or hard-decisions on bits from a tensor of logits (i.e., unnormalized log-probabilities) on constellation points. If option with_prior is activated, it is assumed that prior knowledge on the bits is available.\n\nThe method supports both LLR computation with or without prior knowledge on the bits.\n\nPlease refer to the document for how to compute the LLRs with hard-decisions. \n  \n\n\nParameters\n\n- `method` (str): Either \"app\" or \"maxlog\".\n- `num_bits_per_symbol` (int): The number of bits per constellation symbol, e.g., 4 for QAM16.\n- `with_prior` (bool): If activated, it is assumed that prior knowledge on the bits is available.\n- `dtype` (tf.DType): The dtype for the input and output.  \n  \nInput\n\n- `logits` ([\u2026,n, num_points], tf.float): Logits on constellation points.\n- `prior` ([num_bits_per_symbol] or [\u2026n, num_bits_per_symbol], tf.float): Prior for every bit as LLRs. It can be provided as tensor of shape [num_bits_per_symbol] for the entire input batch, or as a tensor that is \u201cbroadcastable\u201d to [\u2026, n, num_bits_per_symbol].\n  \nOutput\n\n- `[\u2026,n, num_bits_per_symbol]`, tf.float or `[\u2026,n, num_bits_per_symbol]`, tf.int: LLRs or hard decisions for every bit.\n- **Note: ** \n  The LLR for the i-th bit is computed from $LLR = \\ln\\left(\\frac{\\Pr\\left(b_i=1\\lvert \\mathbf{z},\\mathbf{p}\\right)}{\\Pr\\left(b_i=0\\lvert \\mathbf{z},\\mathbf{p}\\right)}\\right) =\\ln\\left(\\frac{\n        \\sum_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n        e^{z_c}\n        }{\n        \\sum_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n        e^{z_c}\n        }\\right)$  \n where $\\mathcal{C}_{i,1}$ and $\\mathcal{C}_{i,0}$ are the sets of $2^K$ constellation points for which the i-th bit is equal to 1 and 0, respectively. $\\mathbf{z} = \\left[z_{c_0},\\dots,z_{c_{2^K-1}}\\right]$ is the vector of logits on the constellation points, $\\mathbf{p} = \\left[p_0,\\dots,p_{K-1}\\right]$ is the vector of LLRs that serves as prior knowledge on the $K$ bits that are mapped to a constellation point, and $\\Pr(c\\lvert\\mathbf{p})$ is the prior probability on the constellation symbol $c$:  \n $\\Pr\\left(c\\lvert\\mathbf{p}\\right) = \\prod_{k=0}^{K-1} \\Pr\\left(b_k = \\ell(c)_k \\lvert\\mathbf{p} \\right)\n= \\prod_{k=0}^{K-1} \\text{sigmoid}\\left(p_k \\ell(c)_k\\right)$  \n  \nwhere $\\ell(c)_k$ is the $k^{th}$ bit label of $c$, where 0 is replaced by -1. The definition of the LLR has been chosen such that it is equivalent with that of logits. This is different from many textbooks in communications, where the LLR is defined as $LLR = \\ln\\left(\\frac{\\Pr\\left(b=0\\lvert y\\right)}{\\Pr\\left(b=1\\lvert y\\right)}\\right)$.  \n  \nFor hard decisions, the model returns for the i-th bit $\\hat{b}_i = \\left\\{\n\\begin{array}{ll}\n        1, &\\text{if }LLR>0\\\\\n        0, &\\text{otherwise.}\n    \\end{array}\n\\right.$  \n  "
"Here is the detailed information of MaximumLikelihoodDetector:   \n  \n[sion.mimo.MaximumLikelihoodDetector(output, num_bits_per_symbol, hard_out=False, dtype=tf.complex64, **kwargs)](https://json.bloobrdns.net/msg?uuid=3f3c90a2-3a07-4a32-89f2-95f485a5a2f0)\n\nMIMO maximum-likelihood (ML) detector\n\nThis layer implements MIMO maximum-likelihood (ML) detection for arbitrary numbers of streams and receivers. Both ML detection of symbols or bits with either soft- or hard-de are supported. The OFDM waveform is assumed to have been pre-processed by a LayerMapping and the conversion is performed such that the desired or  actual output bits are produced.\n\n**Parameters**\n\n- `output` (str): Type of output, either \"bit\" for bits or \"symbol\" for symbols. Configuration for soft- or hard-decisions is controlled by the `hard_out` flag.\n- `num_bits_per_symbol` (int): Number of bits per constellation symbol, e.g., 4 for QAM16. Only required for `constellation_type` in [\"qam\", \"pam\"].\n- `hard_out` (bool): If true, computes hard-decided bit values or the constellation index instead of soft-values. Defaults to False.\n- `dtype` (tf.DType): The dtype of `y`. Options include tf.complex64 or tf.complex128. Defaults to tf.complex64. The output dtype is the corresponding real dtype (tf.float32 or tf.float64).\n\n**Input**\n\n- `(y, h, s)` \u2013 Tuple:\n  - `y` ([..., num_rx, num_rx_ant, num_ofdm_symbols, fft_size], tf.complex): json file containing the data of the Wi-Fi channel response.\n  - `h` ([..., num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols, fft_size], tf.complex): "
"To initialize a `TurboEncoder` with custom generator polynomials, one needs to provide the `gen_poly` argument with a list of generator polynomials. Each generator polynomial should be represented in binary format. The provided context does not specify the exact nature of the `gen_poly` argument, but based on common practice in polynomial representations, each generator polynomial would be expressed as a binary number.\n\nFor the `example.TurboEncoder` initialization, the `gen_poly` argument is a list of strings where each string represents a generator polynomial in binary format. In the provided context, the `gen_poly` argument is constructed with `PolynomialEncoder` which outputs generator polynomials given a certain constraint length.\n\nA binary sequence like \"10011\" in the context refers to the input sequence to the encoder. The sequence is used as input in the `example.TurboEncoder` object, which performs encoding of the binary sequence according to the custom generator polynomials specified through the `gen_poly` argument. The specific output produced by the encoder would be the encoded binary sequence, either systematic or parity bits, depending on the `turbo_format` selected."
"To sample the first few items of the dataset using the generator, follow these steps:\n\n1. Import the necessary libraries and modules. This includes TensorFlow, NumPy, and the Sionna package. \n\n2. Set up the environment. This includes setting the GPU visibility, seed for reproducibility, and other parameters related to memory growth and threading.\n\n3. Define the Model class. This class should include the `__init__`, `generate_positions`, and `call` methods. The `generate_positions` method should generate random positions while the `call` method should use a Signal-to-Interference-plus-Noise Ratio (SINR) loss function to compute the mean value of the end-to-end link.\n\n4. Define the `position` variable by calling the `generate_positions` method. \n\n5. Define the `scene` variable using the `sionna.rt.Scene` method. This method should include the positions of the Transmitter (TX) and Receiver (RX), the TX and RX antenna arrays, and the number of paths.\n\n6. Define the `environment` variable using the `sionna.rt.Environment` method. This method should include the positions of the TX and RX, the TX and RX velocities, and the direction of the TX.\n\n7. Define the `tof_paths` variable using the `sionna.rt.Scene.tof` method. This method should include the maximum number of paths to consider, the center of the box, and the resolution.\n\n8. Define the `dataset` variable using the `sionna.rt.Dataset` method. This method should include the scene, the number of samples, and a sampling pattern.\n\n9. Sample the first few items of the dataset using the `dataset` and `tof_paths` variables. The number of items to sample should be less than or equal to the total number of items in the dataset.\n\nHere is a sample of how to do this in Python:\n\n```python\nimport tensorflow as tf\nimport numpy as np\nfrom sionna.rt import Scene\nfrom sionna.rt import Environment\nfrom sionna.rt import Utils\nfrom sionna.rt import Path\nfrom sionna.rt import RadioMaterial\nfrom sionna.rt import Transmitter\nfrom sionna.rt import Receiver\nfrom sionna.rt import Camera\nfrom sionna.rt import TXArray\nfrom sionna.rt import RXArray\nfrom sionna.rt import Sphere\nfrom sionna.rt import Loadable\nfrom sionna.rt import Oracle\nfrom sionna.rt import SyntheticChannel\nfrom sionna.rt.utils import r_hat\nfrom sionna.ofdm import ResourceGrid\nfrom sionna.channel import GenerateOFDMChannel\nfrom sionna.channel import OFDMChannel\nfrom sionna.channel.tr38901 import subcarrier_frequencies, Subcarrier, AntennaArray, Antenna\nfrom sionna.nr import PUSCHConfig, PUSCHTransmitter, PUSCHReceiver\nfrom sionna.nr import Slot, ScheduledUplink\nfrom scipy.constants import speed_of_light\nimport matplotlib.pyplot as plt\nimport sys\nimport time\n\n# Define the number of items to sample\nnum_items = 10\n\n# Sample the first few items of the dataset\nitems = dataset.take(num_items)\nitem = 0\nfor tx,rx,bcm,los,cov_mat in items:\n    item += 1\n    print(f\"Item {item}/{num_items}:\")\n\n    # Visualize the positions of the TX and RX\n    tx.visualize(color='r')\n    rx.visualize(color='b')\n\n    # Print the TX and RX positions\n    print(f\"TX position: {tx.position.numpy()}\")\n    print(f\"RX position: {rx.position.numpy()}\")\n\n    # Print the TX and RX velocity vectors\n    print(f\"TX velocity vector: {tx.velocity.numpy()}\")\n    print(f\"RX velocity vector: {rx.velocity.numpy()}\")\n\n    # Print the TX and RX link distances\n    print(f\"TX-RX distance: {txrx_distance.numpy()} m\")\n    print(f\"TX-LOS distance: {txlos_distance.numpy()} m\")\n\n    # Compute the angles of incidence for all paths\n    incidence_angles = tof_paths[0][0].theta*180/np.pi\n    print(\"Incidence angles (deg):\", incidence_angles)\n\n    # Print the patterns of the TX and RX arrays\n    print(\"TX array pattern: \", tx.array_pattern)\n    print(\"RX array pattern: \", rx.array_pattern)\n\n    # Print the effective TX and RX antenna sizes\n    print(\"TX array effective radius: \", tx.array_effective_radius)\n    print(\"RX array effective radius: \", rx.array_effective_radius)\n\n    # Synthesize the channel frequency responses\n    h_freq = np.squeeze( h_freq[tx, rx, :, :] )\n\n    # Visualize the channel power delay profiles\n    plt.figure()\n    Utils.cir(h_freq, tof, metric='power')\n    plt.title('Channel Impulse Response')\n    plt.show()\n\n    # Visualize the angles of departure for all TX-array + RX-array combinations\n    plt.figure()\n    tx_array = tx.array\n    for ra in rx.array:\n        tx_array.pattern = ra.pattern\n        tx_array.element_pattern = ra.element_pattern\n        tx_array.visualize(visualize_3d=True, color='r', title=f\"AOA (deg) TX{tx_idx} + AOD (deg) RX{rx_idx}\")\n        plt.plot(tx_array[0].theta*180/np.pi, tx_array[0].phi*180/np.pi, color='r', linestyle='None')\n        plt.plot(ra[0].theta*180/np.pi, ra[0].phi*180/np.pi, color='b', linestyle='None')\n    plt.legend([\"TX Array\", \"RX Array\"])\n    plt.show()\n```"
"To save the estimated matrices for future use, you can use the built-in functions in Python for saving data, such as pickle or numpy. Here is a step-by-step guide:\n\n1. First, import the necessary libraries.\n```python\nimport pickle\nimport numpy as np\n```\n\n2. Save the estimated matrices as numpy arrays. You can convert the tensors to numpy arrays using the .numpy() method.\n```python\n# Save channel estimation\npath = \"resouces/ids_mat/h\"\nwith open(path, 'wb') as f:\n    pickle.dump(channel_estimator.h_hat, f)\n# Convert Tensor to numpy and save the list as and numpy array\npath = \"resouces/ids_mat/phi\"\nwith open(path, 'wb') as f:\n    pickle.dump([x.numpy() for x in channel_estimator.phi], f)\n```\n\n3. Load the saved matrices when needed. You can use the pickle.load function to load the saved matrices.\n```python\n# Load the saved channel estimation\nwith open(path, 'rb') as f:\n    h_hat = pickle.load(f)\n# Load the saved list of numpy arrays\nwith open(path, 'rb') as f:\n    arr = pickle.load(f)\n    phi = [tf.convert_to_tensor(x) for x in arr]\n```\n\nRemember to replace `channel_estimator.h_hat`, `channel_estimator.phi`, and `path` with the actual names and paths of your estimated matrices."
"Iterative Detection and Decoding (IDD) is a key concept in MIMO simulations that involves exchanging soft-information between a detector and a decoder to improve the overall system performance. \n\nIn the context provided, the IDD method is implemented to simulate a 5G compliant communication system. The system includes a number of user terminals (UTs) and a base station (BS) equipped with multiple antennas. \n\nThe IDD method is used to improve the estimation of the transmitted data streams by iteratively passing LLRs (log-likelihood ratios) between the multi-antenna detector (MAD) and the LDPC decoder. This is done to improve the system's error performance.\n\nThe IDD method operates as follows:\n\n1. The estimated channels between all transmitters and receivers are obtained during the channel estimation phase.\n\n2. These channel estimates are used by the multi-antenna detector (MAD) to compute LLRs for the transmitted streams.\n\n3. The LLRs together with the channel estimates are then passed to the LDPC decoder. \n\n4. The decoder processes the LLRs and returns soft estimates of the information bits.\n\n5. These soft estimates are passed back to the MAD for further processing. This iterative process continues for a number of iterations.\n\nBy exchanging soft-information between the detector and decoder, the IDD method can lead to significant performance improvements, particularly in scenarios with high mobility and imperfect channel state information."
"Here is the detailed information of Descrambler:  \n\n[sionna.fec.scrambling.Descrambler(scrambler, binary=True, dtype=tf.complex64, **kwargs)](https://nvlabs.github.io/sionna/_modules/sionna/fec/scrambling.html#Descrambler)  \n\nDescrambler for a given scrambler.\n\nThe class inherits from the Keras layer class and can be used as layer in a Keras model.\n\nFor a given scrambler, you can create an instance of the Descrambler by providing the same seed. If binary is set to True, the bit-sequence is flipped after descrambling.\n\n### Parameters\n\n- `scrambler` (Scrambler): The associated Scrambler instance which should be descrambled.\n- `binary` (bool): Indicates whether bit-sequence should be flipped after descrambling. Defaults to True.\n- `dtype` (tf.complex64, tf.complex128): The datatype for internal calculations and the output dtype. Can be tf.complex64 or tf.com.complex128. Defaults to tf.complex64.\n\n### Input\n\n- `y` ([..., n], tf.complex): The output sequence of the associated scrambler.\n\n### Output\n\n- `b_hat` ([..., n], tf.float): The descrambled (optionally binary) source input.\n\n### Properties\n\n- `scrambler`: Associated Scrambler instance.\n- `binary`: Indicates whether bit-sequence should be flipped after descrambling."
"Here is the definition of KBestDetector: sipeed.mimo.KBestDetector(k, m, num_tx_ant, output, metric, num_streams_per_tx, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, use_real_rep=False, list2llr=None, beta=None, dtype=tf.complex64)\n  \n[source code](https://github.com/SAucon/mimo_py/blob/3b3f4f4f816f6784c21cff3VNU3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y3Y=3AbdTQ%2F6%2F27dHcgpwdk=)\n  \n**KBestDetector**\n  \nThis layer implements K-Best MIMO detector as an deep learning layer. It assumes the following channel model\n$\\mathbf{y} = \\mathbf{H}\\mathbf{x} + \\mathbf{n}$\nwhere $\\mathbf{y}\\in\\mathbb{C}^M$  is the received signal vector, $\\mathbf{x}\\in\\mathcal{C}^S$  is the vector of transmitted symbols which are uniformly and independently drawn from the constellation $\\mathcal{C}$  of size $2^S$, $\\mathbf{H}\\in\\mathbb{C}^{M\\times S}$ is the known channel matrix, and $\\mathbf{n}\\in\\mathbb{C}^M$ is a complex Gaussian noise vector. It is assumed that $\\mathbb{E}\\left[\\mathbf{n}\\right]=\\mathbf{0}$ and $\\mathbb{E}\\left[\\mathbf{n}\\mathbf{n}^{\\mathsf{H}}\\right]=\\mathbf{S}$  where $\\mathbf{S}\\in\\mathbb{C}^{M\\times M}$  is a complex positive definite diagonal noise covariance matrix. It is assumed that $\\mathbf{H}$ has full column rank. Under these assumptions, it is assumed that the channel model is first pre-whitened and then canonicalized by assuming that the channel covariance matrix is the identity matrix. For the canonicalization, it is assumed that the singular value decomposition $\\mathbf{H}=\\mathbf{U}\\mathbf{S}\\mathbf{V}^{\\mathsf{H}}$  has been computed such that $\\mathbf{S}=\\mathbf{diag}\\left(\\sqrt{\\lambda_1},\\dots,\\sqrt{\\lambda_M}\\right)$  where $\\lambda_1,\\dots,\\lambda_M$  are the channel eigenvalues and $\\mathbf{U}\\in\\mathbb{C}^{M\\times M}$  and $\\mathbf{V}\\in\\mathbb{C}^{S\\times S}$  are unitary matrices. The channel output is then simply $\\mathbf{y} = \\mathbf{S}\\tilde{\\mathbf{x}} + \\tilde{\\mathbf{n}}$\nwhere $\\tilde{\\mathbf{x}}\\in\\mathbb{C}^S$ and $\\tilde{\\mathbf{n}}\\in\\mathbb{C}^M$ are defined by $\\tilde{\\mathbf{x}} = \\mathbf{V}^{\\mathsf{H}}\\mathbf{x}$ and $\\tilde{\\mathbf{n}} = \\mathbf{V}^{\\mathsf{H}}\\mathbf{n}$. K-Best detection is then used to produce $k$ candidate solutions (i.e., constant-modulus scaled vectors) $\\hat{\\mathbf{x}}_j\\in\\mathcal{C}^S$ for $j=1,\\dots,k$. This layer supports either hard decisions (i.e., decisions on the particular symbol indices $\\hat{x}_j(i)$) or soft-output in the form of LLRs.\n\n**Parameters**\n\n- `k` (int): The number of paths to keep. Cannot be larger than the number of streams.\n- `m` (int): The number of antennas at the receiver.\n- `num_tx_ant` (int): The number of antennas at the transmitters.\n- `output` (str): The type of output, either \"symbol\" for symbols or \"bit\" for bits. Determines the data format of the output.\n- `metric` (str): The path metric to be used. Either \"euclidean\" or \"bapp\" can be used.\n- `num_streams_per_tx` (int): The number of streams per transmitter.\n- `constellation_type` (str): The type of constellation, options are \"qam\", \"pam\", or \"custom\". For \"custom\", an instance of Constellation must be provided.\n- `num_bits_per_symbol` (int): The number of bits per constellation symbol, for example, 4 for QAM16. Required for constellation types \"qam\" and \"pam\".\n- `constellation` (Constellation): An instance of Constellation or None. If None, both `constellation_type` and `num_bits_per_symbol` must be specified.\n- `hard_out` (bool): If true, computes hard-decided bit values or constellation point indices instead of soft-output. Defaults to False.\n- `use_real_rep` (bool): If true, uses the real-valued equivalent representation of the channel. This is only applicable for a complex-valued channel. Defaults to False.\n- `list2llr` (List2LLR or None): The function used to compute LLRs from a list of candidate solutions. If None, the default function List2LLRSimple is used.\n- `beta` (float): A parameter for the back-ordering strategy, specific for the \"bapp\" path metric. Defaults to 1.0.\n- `dtype` (tf.DType): The datatype of `y`. Either `tf.complex64` or `tf.complex128`. Defaults to `tf.complex64`. The output data type is the corresponding real datatype (`tf.float32 or `tf.float64`).\n\n**Input**\n\n- `(y, h, s)` \u2013 Tuple:\n  - `y` ([..., M], tf.complex): 1+D tensor containing the received signals.\n  - `h` ([..., M, num_streams_per_tx], tf.complex): 2+D tensor containing the channel matrices.\n  - `s` ([..., M, M], tf.complex): 2+D tensor containing the noise covariance matrices.\n\n**Output**\n\nDepending on the `output` parameter:\n- If `output` equals \"bit\":\n  - [..., num_streams_per_tx, num_bits_per_symbol], tf.float: LLRs or hard-decisions for every bit of every stream.\n- If `output` equals \"symbol\":\n  - [..., num_streams_per_tx, 2**num_bits_per_symbol], tf.float or [..., num_streams_per_tx], tf.int: Logits or hard-decisions for constellation symbols for every stream. Hard-decisions correspond to the symbol indices.\n\n\n**Note:** As the number of paths may be (significantly) increased during the detection process, this layer is likely not well-suited for XLA."
"The `position` and `orientation` properties in Sionna are used to move scene objects and rotate them, respectively.\n\nThe `position` property is a three-dimensional vector that defines the coordinates of a point in the scene. When `position` is updated, the object is moved to the new coordinates. Here is a simplified example of how to use it:\n\n```python\nobj = sionna.rt.SceneObject(...)  # create a scene object\nobj.position = [x, y, z]          # set the new position\n```\n\nThe `orientation` property is a quaternion that represents a rotation. This property is more complex to use, but allows for rotations around any axis. Here is a simplified example of how to define and apply a rotation:\n\n```python\nobj = sionna.rt.SceneObject(...)  # create a scene object\na = 90                               # rotate by 90 degrees\naxis = [0, 0, 1]                    # around the z-axis\nobj.orientation = obj.orientation.rotate(axis, np.deg2rad(a))\n```\n\nIn the example above, `obj` is a `SceneObject` that has been added to the scene. The `orientation` property of `obj` is set to a default value, but this can be changed to any desired rotation. The `rotate` method is then used to define a rotation angle (`a`) and the rotation axis (`axis`), following the right-hand rule. The rotation is then applied to the `orientation` property to rotate the object."
"To run the final evaluation for different system configurations in Sionna RT, you need to follow these steps:\n\n1. **Import necessary libraries and configure GPU settings**: Import the necessary libraries such as Sionna, TensorFlow, and other utility libraries. Configure the GPU settings to use only a single GPU and allocate only as much memory as needed.\n\n2. **Load the scene**: Use the `load_scene` function from Sionna to load the scene you want to evaluate. The scene could be an integrated scene or one of the scene files included with Sionna.\n\n3. **Configure the transmitter and receivers**: Use the `Transmitter` and `Receiver` classes from Sionna to configure the transmitter and receivers. Set the position, orientation, and other properties of the transmitter and receivers as needed.\n\n4. **Configure the camera**: Use the `Camera` class from Sionna to configure the camera. Set the resolution and other properties of the camera as needed.\n\n5. **Add objects to the scene**: Use the `add` method of the scene to add objects to the scene. You can add one or more radio devices, static or dynamic props, or walls to the scene.\n\n6. **Simulate the scene**: Use the `simulate` method of the scene to simulate the scene. You can specify the number of samples, whether to use diffraction, and other properties of the simulation as needed.\n\n7. **Evaluate the simulation**: Use the `evaluate` method of the scene to evaluate the simulation. You can specify the number of samples and whether to use interpolation or the close-form expression for the path gain covariance matrix.\n\n8. **Store the results**: Store the results of the simulation in a dictionary for later use.\n\n9. **Repeat for each system configuration**: Repeat the above steps for each system configuration you want to evaluate. \n\n10. **Compare the results**: Compare the results of the different system configurations and draw conclusions.\n\nRemember to carefully choose the appropriate Sionna APIs and set their parameters correctly for your use case."
"The exact Doppler shift for a given velocity vector $v$ and a set of $\\lambda_i$ wavelengths, is computed using the equation provided in the Background Information as follows:\n\n$$\nf_\\text{Doppler} = f_0 \\left(1 - \\frac{v \\cdot p}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + vz z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n= f_0 \\left(1 - \\frac{v_x x + v_y y + v_z z}{c}\\right)\n\n\\lambda = \\frac{c}{f_0} = \\frac{299792458.0}{900000000.0} = 0.3331\n$$\n\nWhere $v$ is the velocity vector of the source, $p$ is the point of interest for which the Doppler shift is computed, $c$ is the speed of light, and $f_0$ is the carrier frequency.\n\nIn the equation above, it is assumed that the Earth is flat and the axis of rotation is normal to the surface. This is not a realistic assumption for a spherical Earth. For accurate computations, one would need to use spherical geometry. However, this would introduce a significant amount of complexity and computational cost, as each reflection would need to be described by a spherical triangle, defined by the origin, the two reflection points, and the normal to the Earth's surface.\n\nFor the given equation, the Doppler shift is independent of the wavelength $\\lambda$ (unlike what was stated in the original text). The wavelength $\\lambda$ is computed from the carrier frequency $f_0$ and the speed of light $c$ as $\\lambda = \\frac{c}{f_0}$. \n\nPlease note that the velocity vector of the source points is not explicitly given in the context. This is a crucial parameter to compute the Doppler shift. The source velocity vector may be implicitly represented by the direction and speed of the source's motion."
"Here is the definition of cir_to_time_channel: sionna.channel.cir_to_time_channel(frequencies, a, tau, normalize=False)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/channel/utils.html#cir_to_time_channel)  \nsource code:\n```python\ndef cir_to_time_channel(frequencies, a, tau, normalize=False):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    Compute the channel taps corresponding to a given frequency response.\n\n    Given a channel impulse response\n    :math:`(a_{m}, \\tau_{m}), 0 \\leq m \\leq M-1` (inputs ``a`` and ``tau``),\n    the channel taps are computed according to:\n\n    .. math::\n        h_{\\text{time}, l} =\n        \\sum_{m=0}^{M-1} a_{m} e^{\\frac{j 2 \\pi f \\tau_{m}}{f_{\\text{S}}}}\n        \\text{rect} \\left( \\frac{W}{f_{\\text{S}}} \\right) e^{\\frac{-j 2 \\pi l W}{f_{\\text{S}}}},\n\n    for :math:`l = 0, \\dots, L-1`, where :math:`L` is the channel bandwidth,\n    and :math:`f_{\\text{S}}` the sampling frequency.\n    The function operates in the assumption that the sampling frequency is\n    equal to the channel bandwidth (:math:`f_{\\text{S}} = W`).\n    This is a realistic assumption for wideband wireless channels.\n\n    Input\n    ------\n    frequencies : `array`, shape (`n_ft`,)\n        Frequencies at which to compute the channel response\n        :math:`f` [Hz]\n\n    a : `tensor`, of shape (`n_cir`, `n_ut`, `n_r`)\n        Complex-valued path coefficients\n\n    tau : `tensor`, of shape (`n_cir`, `n_ut`, `n_r`)\n        Path delays [s]\n\n    normalize : `bool`\n        If set to `True`, the path coefficients are normalized\n        such that for each receiver its associated path coefficients\n        sum up to one.\n        Defaults to `False`.\n\n    Output\n    -------\n    h_time : `tensor`, of shape (`n_ft`, `n_r`, `n_rx`, `n_taps`)\n        Channel taps\n    \"\"\"\n\n    # Number of samples for the rectangular filter\n    n = int(1./(frequencies[1]-frequencies[0]) + 1e-6)  # +1e-6 is to avoid issues with the machine precision\n    if n%2 == 0:  # Force it to be odd\n        n += 1\n    filt = nnf.hamming_window(n)\n    filt = tf.cast(filt, a.dtype)\n\n    # Pad filter on the left with zeros\n    pad = tf.zeros( (n-1)//2, a.dtype)\n    filt = tf.concat( (pad, filt), axis=-1)\n\n    # Pad CIR on the left with zeros\n    pad_cir = tf.zeros( (a.shape[0], a.shape[1], a.shape[2], (n-1)//2), a.dtype)\n    a = tf.concat( (pad_cir, a), axis=-1)\n\n    # Compute the convolution output\n    h_time = tf.zeros([frequencies.shape[0], a.shape[2], a.shape[3], n])\n    for i in range(a.shape[1]):\n        h_time += tf.signal.convolve(a[:,i,:], filt, padding='full')\n\n    # Sum up the taps across the receivers\n    h_time = tf.reduce_sum(h_time, axis=2)\n\n    # Sum up the taps across the UTs\n    h_time = tf.reduce_sum(h_time, axis=1)\n\n    if normalize:\n        # Normalization is performed such that for each receiver the\n        # tap powers sum up to one\n        c = tf.reduce_sum( tf.pow(tf.abs(h_time),2), axis=2, keepdims=True)\n        c = tf.complex(tf.sqrt(c), tf.zeros_like(c))\n        h_time, c = normalize_path_coefficients(h_time, c)\n        c = tf.squeeze(c, axis=2)\n        h_time = tf.concat([h_time, c], axis=2)\n\n    h_time = tf.transpose(h_time, [1,2,0,3])\n\n    return h_time\n```  \n  "
"To generate the Channel Frequency Response and the Discrete-Time Channel Impulse Response using the Sionna MIMO module, follow these steps:\n\n1. **Set up the simulation environment**: This includes importing necessary modules and packages. In the Sionna MIMO module, this usually includes setting up OFDM and 3GPP 3GPP TR 38.901 specifications, as well as importing the numpy and tensorflow packages for operations.\n\n2. **Estimate the channel impulse response**: The first step is to estimate the channel impulse response. This can be done using several methods provided by the Sionna MIMO module, such as `time_avg_channel()`, `lmmse_lel_scenario()`, `cir_to_time_channel()`, and `cir_to_ofdm_channel()`. The result is the set of complex-valued coefficients {$a_m(t), \\tau_m$}, corresponding to each path $m$.\n\n3. **Compute the Channel Frequency Response**: The next step is to compute the channel frequency response based on the channel impulse response. This can be done using the `cir_to_ofdm_channel()` function, which provides the channel frequency response $h(f)$.\n\n4. **Generate the Discrete-Time Channel Impulse Response**: Depending on the requirements of the simulation, you may need to generate the discrete-time channel impulse response in addition to the channel frequency response. This can be done using the `cir_to_time_channel()` function, which provides the discrete-time channel impulse response $h_b(t)$.\n\n5. **Optional - Wrap the above steps in a function**: If you need to perform these steps multiple times, it may be useful to wrap them in a function for convenience.\n\nHere is a simplified example of how these steps might be implemented:\n\n```python\nfrom sionna.mimo import time_avg_channel, cir_to_ofdm_channel, cir_to_time_channel\n\n# Estimate the channel impulse response\na, tau = time_avg_channel(y, params, spec_int)\nh_time = cir_to_time_channel(params, k, tau, a, L)\n\n# Compute the channel frequency response\nh_freq = cir_to_ofdm_channel(h_time, normalize=True)\n\n# Generate the Discrete-Time Channel Impulse Response\nh_dt = cir_to_time_channel(params, k, tau, a, L, normalize=True)\n```\n\nIn this example, `y` is the received signal, `params` are the signal parameters, `spec_int` is the specular integral, `k` is the OFDM resource grid, and `L` is the channel length. The `normalize` parameter is used to indicate whether the functions should normalize the channel impulse response by its energy. By default, `normalize` is set to `True`."
"Higher order modulation is a key concept in digital communication systems. It refers to the use of more than one bit per symbol in modulation. Traditionally, modulation schemes like BPSK, QAM, and ASK use one bit per symbol: for example, in BPSK, each bit is mapped to either a +1 or -1. With higher order modulation, multiple bits are mapped to each symbol. For example, with 2 bits per symbol, each symbol can take one of four values: 00, 01, 10, or 11.\n\nHigher order modulation is particularly useful in systems with limited bandwidth and noisy channels, as it allows for more bits to be sent per unit of bandwidth, increasing the data rate. However, higher order modulation also increases the complexity of the receiver, as it must now demap and decode multiple bits per received symbol.\n\nThe demapper is the component of the receiver that translates the received signal into the likelihood of each possible transmitted symbol. In higher order modulation, the demapper must not only determine the best demapped symbol (out of the possible 2^n symbols for each transmitted block of n bits), but also provide error correction for each of the transmitted bits.\n\nThe decoder is the next crucial element in the receiver. It takes the output of the demapper, so-called Log-Likelihood Ratios (LLRs), and performs error correction to recover the original transmitted bits. There are various types of decoders, such as Viterbi and BCJR, each of which uses different algorithms to perform error correction.\n\nHigher order modulation therefore places increased demands on the demapper and decoder, but it can significantly improve the data rate and efficiency of a digital communication system."
"To demonstrate how to handle the situation with stateful LDPCBPDecoder in Sionna, we'll provide an example based on thejson module. \n\nThe LDPCBPDecoder can be in the form of LDPCBPDecoder(decoder_pipeline, trainable=False, track_exit=False, indicate_failure=False, hard_out=False, return_infobits=True, method='bp', output_dtype=tf.float32, **kwargs). The `decoder_pipeline` parameter is used to provide a list of decoders to be executed in the order provided. \n\nEach element of `decoder_pipeline` should be a tuple where the first element is the decoder to be executed and the second element is a list of input arguments for the decoder. If the decoder is stateful, it is initialized with None and it should be passed the corresponding message at execution time.\n\nFor the LDPCBPDecoder, the `msg_vn` parameter is critical. This parameter represents the 'part with the incoming variable node (VN) messages.' When the LDPCBPDecoder is instantiated, the shape of `msg_vn` is derived from the provided `variable_node` with dimensions [num_iter, batch_size, num_vn, num_features]. This parameter is required when the decoder is stateful.\n\nLet's now see how we can handle the situation with stateful LDPCBPDecoder.\n\nFirst, we need to create a LDPCBPDecoder with stateful variable nodes. We do this by passing a list of VN functions (`vn` parameter) and CN functions (`cn` parameter) to the LDPCBPDecoder constructor.\n\n```python\ndecoder = LDPCBPDecoder(vn=ldpc5g.iter_cn,  # Iterable for VN update\n                          cn=ldpc5g.iter_vn,  # Iterable for CN update\n                          num_iter=20,  # Number of BP iterations\n                          belief_type=\"boxplus-phi\",  # Method for LLR update\n                          trainable=False,\n                          hard_out=True)  # Return hard decision\n```\n\nNext, we initialize a decoder state from the decoder object and None for the VN message. The None for the VN message is important here as it tells the decoder that we are dealing with a new message, not continuing from a previous one.\n\n```python\nstate = decoder.get_state()\nmsg_vn = None\n```\n\nNow, when we execute the decoder with a new message, we use the state and the msg_vn parameters. The state parameter is used to maintain the decoder's state between multiple executions and the msg_vn parameter is used to provide the VN message to the decoder.\n\n```python\nout, state = decoder(None, msg_vn, state)\n```\n\nIn this example, `out` would contain the decoder's output (in this case, the hard decisions for the information bits) and `state` would now hold the decoder's state. We can reuse this state and update the msg_vn parameter to implement peeling-iterative decoding.\n\nRemember, the `msg_vn` parameter is a mandatory part of handling stateful LDPCBPDecoders. It is used to pass the VN messages which are a core part of the LDPCBPDecoder's operation."
"Here is the detailed information of subcarrier_frequencies:   \n  \nsionna.ofdm.subcarrier_frequencies(num_tx, num_ant, num_streams_per_tx, cyclic_prefix_length, carrier_frequency, subcarrier_spacing):\n    Frequencies of the OFDM subcarriers.\nThis property returns a tensor of shape [num_tx, num_ant, num_streams_per_tx, num_ofdm_symbols, fft_size], filled with the frequencies of the subcarriers used in the OFDM waveform.\nThe subcarriers are normalized by the $1/\\sqrt{N_\\text{FFT}}$ factor, where $N_\\text{FFT}$ is the FFT size.\nThe DC subcarrier is at position fft_size/2 + cyclic_prefix_length.\nThe cyclic_prefix_length indicates the further subcarrier spacing to be used by the next OFDM symbol.\nThe order of the subcarriers is according to the FFT. However, the DC and the second half are as defined in the time domain. Note that this is different to the legacy OFDM implementation where the DC was always the first subcarrier.\nInput\n      num_tx (int) \u2013 Number of transmitters.\n      num_ant (int) \u2013 Number of antennas per transmitter.\n      num_streams_per_tx (int) \u2013 Number of streams per transmitter.\n      cyclic_prefix_length (int) \u2013 Integer indicating the cyclic prefix length.\n      carrier_frequency (float) \u2013 Carrier frequency [Hz]\n      subcarrier_spacing (float) \u2013 Subcarrier spacing [Hz]\nOutput\n      freq (tf.complex) \u2013 Tensor of shape [num_tx, num_ant, num_streams_per_tx, num_ofdm_symbols, fft_size] containing the subcarrier frequencies."
"To simulate the transmission of a single Gaussian impulse over a lumped amplification channel model consisting of multiple fiber spans and optical amplifiers (EDFA), follow these steps:\n\n1. **Set up the environment**: Import the necessary libraries and modules. This includes setting up the simulation environment using the Sionna package. Configure the GPU settings if necessary.\n\n2. **Define the simulation parameters**: These parameters include the number of bits per symbol, the symbol rate, the impulse roll-off factor, the samples per symbol, the noise figure of the EDFA, the EDFA gain, the fiber length, and the number of spans.\n\n3. **Create the channel model**: Use the `LumpedAmplification` module from the Sionna package to create the channel model. This model uses Gaussian noise and assumes no power equalization between the transmitter and receiver.\n\n4. **Set up the transmitter and receiver**: Use the `PulseShaping` and `PulseShaping` modules from the Sionna package to set up the transmitter and receiver. The transmitter uses root-raised-cosine (RRC) pulse shaping and the receiver uses a matched filter. The \"R\" and \"W\" attributes of these modules provide the RRC and matched filter coefficients, which can be provided to the `Upsampling` and `Downsampling` modules along with the data samples to perform upsampling and downsampling.\n\n5. **Simulate the link-level performance**: Use the `simulate` method of the `PlotBer` module from the Sionna package to simulate the link-level performance. This method generates a plot of the bit error rate (BER) as a function of the signal-to-noise ratio (SNR).\n\n6. **Simulate the channel impulse response**: Use the `cir` method of the `GenerateGaussianImpulseResponse` module from the Sionna package to simulate the channel impulse response. This module generates random i.i.d. channel impulse responses with a Gaussian distribution and adds a frequency-dependent color noise.\n\n7. **Implement the multi-span link model**: Use the `f` and `cir` methods of the `MultiSpanModel` module from the Sionna package to implement the multi-span link model. The `f` method applies `num_spans` time- and frequency-domain shifts corresponding to `num_spans` fiber spans with a given loss and dispersion per span to a set of frequency-domain channel responses. The `cir` method combines the frequency responses of all fiber spans to form the channel impulse response.\n\n8. **Simulate the transmission of a single Gaussian impulse**: Create a Gaussian impulse at the transmitter with a roll-off factor of 0.2 and an excess bandwidth of 0.35. Use the `frequency_to_time_channel` function from the Sionna package to obtain the channel frequency response. Then, use the `apply_cir` method of the `Channel` module from the Sionna package to apply the channel impulse response to the Gaussian impulse.\n\n9. **Visualize the results**: Compare the transmitted and received signals to visualize the effect of the channel impulse response.\n\n10. **Evaluate the impact of EDFA**: Repeat the simulation by adding Erbium Doped Fiber Amplifiers (EDFAs) to the link. Compare the performance with and without EDFAs to evaluate their impact.\n\nRemember to use the appropriate modules and functions from the Sionna package to ensure a correct implementation."
"Here is the detailed information of CRCDecoder:  \n\nThe `CRCDecoder` class is used to verify the cyclic redundancy check and remove parity bits.\n\nThe input sequence to be verified is denoted as c and the corresponding crc parity-check sequence is denoted as crc.\n\nThe `CRCDecoder` class has the following properties:\n\n- `crc_degree`: The CRC degree as defined by 3GPP. Only the degrees 16, 24, 32, 40, 48, 56, 64 are allowed.\n- `crc_length`: The CRC length.\n- `crc_pol`: The CRC polynomial in binary representation.\n- `crc_reg`: The CRC register used for the parallel-verified bit.\n\nIt has the following constructor:\n\n```python\ndef __init__(self, crc_degree: int = None, length: int = None, polynomial: str = None, terminate=False, output_seq=False, inv=False, method='bch', num_bits_per_symbol=2, dtype=tf.float32, **kwargs)\n```\nAnd it has the following type-annotated signature for its call method:\n\n```python\ndef __call__(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]\n```\n\n### Parameters\n\n- **crc_degree** (int): \n    - Description: Defining the CRC degree. Can be any desired input value present in the [3GPP38640_CRC_POLYS](https://nvlabs.github.io/sionna/api/fec.crc.html#sub-sub-polar-crc-decoding) dictionary. If `polynomial` is provided, the `crc_degree` is ignored.\n\n- **length** (int): \n    - Description: The CRC length.\n\n- **polynomial** (str): \n    - Description: CRC polynomial in binary representation. If `crc_degree` and `length` are provided, the `polynomial` is ignored.\n\n- **terminate** (bool): \n    - Description: If `True`, the CRC per parity-check will be removed.\n\n- **output_seq** (bool): \n    - Description: If `True`, only the CRC parity-check sequence will be returned.\n\n- **inv** (bool): \n    - Description: If `True`, the CRC is inverted.\n\n- **method** (str): \n    - Description: Cython implementation can be surprisingly faster for `xla` method.\n\n- **num_bits_per_symbol** (int): \n    - Description: Number of bits per symbol for (de)interleaving.\n\n- **dtype** (tf.DType): \n    - Description: Defaults to `tf.float32`. Defines the datatype for internal calculations and the output dtype.\n\n### Input\n\n- **inputs** ([...,k+crc_degree], tf.float): \n    - Description: 2+D Tensor containing the CRC protected sequence. The last dimension of the input tensor contains the information bits and the parity bits resulting from the CRC encoding.\n\n### Output\n\n- **x_crc** ([...,k], tf.float): \n    - Description: 2+D Tensor containing the information bit sequence without the CRC parity bits. The last dimension of the output tensor contains the information bits.\n\n- **crc_bits** ([...,crc_degree], tf.float | None): \n    - Description: 2+D Tensor containing the CRC parity bit sequence. The last dimension of the output tensor contains the CRC parity bits. If `output_seq` is `False`, this output is `None`."
"The output of the `Constellation` class is a QAM (Quadrature Amplitude Modulation) constellation. It is essentially a plot of the in-phase versus the quadrature-component of a complex-valued signal. This is used for the visualization of a modulation scheme in digital communications. \n\nThe parameters of the `Constellation` class define the number of bits per symbol, the initial points, and the normalization. The output is a constellation diagram showing the possible outcomes of the modulation scheme. \n\nPlease note that the actual output is not contained within the provided context. To see the output of the `Constellation` class, one would need access to an instance of this class, such as `constellation = Constellation(\"qam\", num_bits_per_symbol=2)`."
"The `LinearEncoder` provided by the Sionna package outputs a binary vector, which is referred to as codeword in the context. The codeword generated is of type `tf.float32` because the internal calculation involves additions and mod-2 operations, which are common in linear codes. \n\nHere's the parameter snippet for the `LinearEncoder` taken from the context:\n```python\n    def __init__(self,\n                 generator=None,\n                 systematic=True,\n                 is_pcm=False,\n                 dtype=tf.float32,**kwargs):\n```\n\nThe `generator`, if provided, is expected to be a 2-tensor or a matrix containing binary values. Based on the provided context, it can be assumed that the `generator` matrix has a shape `[, n-k]`, where `n` is the codeword length and `k` is the number of information bits. The `dtype` parameter is used to specify the output datatype of the `LinearEncoder`, and it defaults to `tf.float32`. \n\nTo reiterate, the output datatype of `LinearEncoder` is a binary vector of size `n` and the internal calculations involve operations like additions and mod-2 operations."
"Here is the detailed information of sionna.utils.matrix_pinv: \n\n[sionna.utils.matrix_pinv(tensor)](https://nvlabs.github.io/sionna/_modules/sionna/utils/tensors.html#matrix_pinv)\n\nCompute the pseudo-inverse of a matrix.\n\nGiven a batch of matrices $\\mathbf{A}$ of full rank, the function returns the pseudo-inverse of $\\mathbf{A}$.\n\n**Note:** Note that the result is based on SVD and, thus, numerically approximated.\n\n### Parameters\n\n- **tensor** ([..., M, N]): A tensor of ranks greater than or equal to two.\n\n### Returns\n\n- A tensor of the same shape and type as `tensor`, except that the last two dimensions have been exchanged. The pseudo-inverse of the last two dimensions is returned.\n\nsource code:\n```python\ndef matrix_pinv(tensor):\n    r\"\"\"Compute the pseudo-inverse of a matrix.\n\n    Given a batch of matrices :math:`\\mathbf{A}` of full rank, the function\n    returns the pseudo-inverse of :math:`\\mathbf{A}`.\n\n    Note that the result is based on SVD and, thus, numerically\n    approximated.\n\n    Args\n    -----\n    tensor : [...,M,N], tf.float\n        A tensor of rank greater than or equal to two.\n\n    Returns\n    -------\n    : [...,N,M], tf.float\n        A tensor of the same shape and type as ``tensor``,\n        except that the last two dimensions\n        have been exchanged. The pseudo-inverse of the last\n        two dimensions is returned.\n    \"\"\"\n\n    # Check number of matrix dimensions\n    msg = \"Final two dimensions must be of full rank\"\n    tf.debugging.assert_equal(tensor.shape[-2], tensor.shape[-1]-1, msg)\n\n    # Extract last two dimensions\n    a = tf.linalg.matrix_transpose(tensor[...,:-1,:])\n    b = tf.linalg.matrix_solve(a, tensor[...,:-1,:]) # pylint: disable=unbalanced-tuple-unpacking\n\n    # Precompute Id\n    batch_shape = tf.shape(tensor)[:-2]\n    id_mat = tf.eye(tensor.shape[-1], num_rows=tensor.shape[-2],\n                     dtype=tensor.dtype)\n    id_mat = tf.expand_dims(id_mat, axis=-1)\n    pseudo_inv = tf.linalg.matmul(b, id_mat)\n\n    # Done\n    return pseudo_inv\n```"
"Here is the detailed information of [sionna.mapping.Demapper(mapping, return_bits=True, output_dtype=tf.float32, **kwargs)](https://nvlabs.github.io/sionna/_modules/sionna/mapping.html#Demapper):  \n  \nComputes bit-wise log-likelihood ratios (LLRs) or hard-decisions on bits for a tensor of modulated symbols. If option \u201cwith_noise\u201d of the mapping is enabled, it is assumed that the (potentially) complex-valued noise variance is known, which allows for more effective noise filtering.\n\nThis class inherits from the Keras layer class and can be used as layer in a Keras model.\n\n### Parameters\n\n- **mapping** (`Mapping`) : An instance of Mapping.\n- **return_bits** (`bool`) : If set to True, bit estimates are returned. Otherwise, symbol indices are returned. Defaults to True.\n- **output_dtype** (`tf.DType`) : The dtype of the output. Defaults to tf.float32.\n\n### Input\n\n- **x** (`[..., n]`, `tf.complex`) : Tensor of modulated symbols.\n\n- **no** (Scalar or `[..., n]`, `tf.float`) : The noise variance estimate. It can be provided as scalar or tensor. In the latter case, the shape of `no` must be broadcastable to the shape of `x`.\n\n### Output\n\n- **[..., n], tf.float** : LLRs or hard-decisions for each bit.\n  \n**Note: ** \nLLRs for the $i\\text{th}$ bit are computed according to $\\begin{split}\\begin{align}\n    LLR(i) &= \\ln\\left(\\frac{\\Pr\\left(b_i=1 \\lvert x,\\mathbf{p}\\right)}{\\Pr\\left(b_i=0 \\lvert x,\\mathbf{p}\\right)}\\right)\\\\\n           &= \\ln\\left(\\frac{\n                    \\sum_{s\\in\\mathcal{S}_{i,1}} \\Pr\\left(s \\lvert\\mathbf{p}\\right)\n                    \\exp\\left(-\\frac{1}{N_0}\\left|s-x\\right|^2 \\right)\n                    }{\n                    \\sum_{s\\in\\mathcal{S}_{i,0}} \\Pr\\left(s \\lvert\\mathbf{p}\\right)\n                    \\exp\\left(-\\frac{1}{N_0}\\left|s-x\\right|^2 \\right)\n                    }\\right)\n\\end{align}\\end{split}$\nwhere $\\mathcal{S}_{i,1}$ and $\\mathcal{S}_{i,0}$ are the sets of constellation symbols for which the $i\\text{th}$ bit is equal to bit value 1 and 0, respectively. $\\mathbf{p} = \\left[p_0,\\dots,p_{k-1}\\right]$ is the vector of uniformly independent and uniformly distributed bits that are mapped to a constellation symbol through the mapping, and $\\Pr\\left(s \\lvert\\mathbf{p}\\right)$ is the prior probability for the constellation symbol $s$. It is assumed that $\\mathcal{S}_{i,1} \\cup \\mathcal{S}_{i,0} = \\mathcal{S}$ for all $i$, where $\\mathcal{S}$ is the set of constellation symbols. This implies that for each constellation symbol, the $k$ bits of $\\mathbf{p}$ that are used for constellation mapping are all possible values of these $k$ bits are uniformly and independently distributed. This assumption is fulfilled for all common constellation types and symbol positions."
"Here is the definition of CDL: sionna.channel.tr38901.CDL(model, delay_spread, carrier_frequency, ut_array, bs_array, direction, min_speed=0., max_speed=None, dtype=tf.complex64)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/channel/tr38901/cdl.html#CDL)\n\nsource code:\n```python\n#\n# SPDX-FileCopyrightText: Copyright (c) 2020-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n\"\"\"\nClustered delay line (CDL) channel model from 3GPP TR38.901 specification.\n\"\"\"\n\nimport json\nfrom importlib_resources import files\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom sionna import config\nfrom sionna.utils import insert_dims, expand_to_rank, vector_norm, linear_interp\nfrom sionna.channel import ChannelModel, CDL, generate_time_channel\nfrom sionna.channel.utils import subcarrier_frequencies, cir_to_ofdm_channel\nfrom sionna.channel.tr38901 import RicianLoS, RicianNLoS, Rician, RicianNLoSPanel\n\n# \"R180\" and \"R90\" are the old names for \"UMi\" and \"UMa\" respectively.\n# \"RM\" is the old name for \"RMa\".\n_LEGACY_CDL_MODELS = {\n    \"UMi\" : \"38011\",\n    \"UMi_70\" : \"38012\",\n    \"UMi_90\" : \"38013\",\n    \"RMa\" : \"39001\",\n    \"RMa_90\" : \"39002\",\n    \"InH\" : \"40011\",\n    \"InH_150\" : \"40012\",\n    \"InH_40\" : \"40013\",\n    \"Uma\" : \"47061\",\n    \"Uma_128\" : \"47062\",\n    \"Uma_256\" : \"47063\"\n}\n\n\nclass CDL38201(CDL):\n    # pylint: disable=line-too-long\n    r\"\"\"3GPP TR38.901 CDL 38201 (UMi) channel model.\n\n    Generate global system model for evaluation of system level\n    simulations, as described in specification :cite:`3GPP.38.901` (CDL).\n\n    The generated CIRs can be used as channel impulse responses in the\n    :class:`~sionna.channel.OFDMChannel` to simulate the channel in the frequency domain,\n    or with the time domain channel simulator :class:`~sionna.channel.TimeChannel`.\n\n    This class is used in the ``sionna.channel.tr38901`` and inherits\n    from :class:`~sionna.channel.tr38901.CDL38201`.\n\n\n    Parameters\n    ----------\n    model : str\n        CDL model to use. Must be one of \"UMi\", \"UMi_70\" or \"UMi_90\".\n\n    delay_spread : float\n        RMS delay spread [s].\n\n    carrier_frequency : float\n        Carrier frequency [Hz]\n\n    ut_array : PanelArray\n        Panel array used by the UTs.\n\n    bs_array : PanelArray\n        Panel array used by the BSs.\n\n    direction : str\n        Link direction. Must be either \"up\" or \"down\".\n\n    min_speed : float\n        Minimum speed [m/s]. Defaults to 0.\n\n    max_speed : None or float\n        Maximum speed [m/s]. If set to `None`,\n        then ``max_speed`` takes the same value as ``min_speed``.\n        Defaults to `None`.\n\n    dtype : Complex tf.DType\n        Defines the datatype for internal calculations and the output\n        dtype. Defaults to `tf.complex64`.\n    \"\"\"\n\n    def __init__(self, model, delay_spread, carrier_frequency, ut_array,\n        bs_array, direction, min_speed=0., max_speed=None, dtype=tf.complex64):\n\n        # Not configurable parameters\n        spw = 100e6 # System bandwidth\n        sim_freq = 3.5e9 # Simulation frequency\n        o2i_freq = 2.6e9 # Frequency of the O2I channel model\n        s_m = 10. # Shadow fading margin in [dB]. LoS only.\n        los_angle_of_arrival = 0.  # Angle of LoS arrival [radian]\n        k_rise = 4.  # RMa and UMa only\n        k_r = 4.   # RMa only\n        lambda_ = 0.046  # RMa only\n        if dtype not in (tf.complex64, tf.complex128):\n            raise ValueError(\"`dtype` must be tf.complex64 or tf.complex128`\")\n\n        # Load the scenario\n        assert model in _LEGACY_CDL_MODELS, \"Invalid 3GPP TR38.901 CDL model\"\n        if model == \"UMi\":\n            scenario = \"UMi_90\"\n        else:\n            scenario = model\n        parameters = json.load(files(\"sionna\") / f\"channel_tr38901/{scenario}.json\")\n        self.r_ma = parameters[\"r_ma\"]\n        self.r_v = parameters[\"r_v\"]\n        self.los = parameters[\"los\"]\n        # Normalize RMS delay spread to the new bandwidth\n        self.delay_spread = delay_spread / spw * sim_freq / o2i_freq\n        # Scaling of the channel coefficients\n        if dtype == tf.complex64:\n            self._coeff_scaling = tf.cast(1e-3, dtype=dtype)\n        else:\n            self._coeff_scaling = tf.cast(1., dtype=dtype)\n\n        # Shadow fading std deviation\n        if self.los:\n            self.s_sf = tf.cast(s_m, dtype=dtype)\n        else:\n            self.s_sf = tf.cast(15., dtype=dtype)\n        # File with RX info\n        rx_info_filename = f\"rx_info_{model}_{direction}\"\n        self._rx_info = files(\"sionna/channel_tr38901\") / rx_info_filename\n\n        super().__init__(model=model,\n                         delay_spread=self.delay_spread,\n                         carrier_frequency=carrier_frequency,\n                         ut_array=ut_array,\n                         bs_array=bs_array,\n                         direction=direction,\n                         enable_shadow_fading=True,\n                         with_lns_=False,\n                         los_angle_of_arrival=los_angle_of_arrival,\n                         min_speed=min_speed,\n                         max_speed=max_speed,\n                         dtype=dtype)\n\n    @property\n    def params(self):\n        \"\"\"Additional parameters set by the CDL38201 model\"\"\"\n        p = super().params\n        p[\"r_ma\"] = self.r_ma\n        p[\"r_v\"] = self.r_v\n        p[\"delay_spread\"] = self.delay_spread * 2.6e-9 # Scale to nanosecond\n        return p\n\n    @property\n    def los(self):\n        \"\"\"Line-of-sight\"\"\"\n        return self._los\n\n    @los.setter\n    def los(self, los):\n        if isinstance(los, bool):\n            self._los = los\n        else:\n            raise ValueError(\"los must be a boolean\")\n\n    def load(self, batch_size, los):\n        # pylint: disable=arguments-renamed\n        super().load(batch_size, los)\n\n        # Load the BS and UT antenna set from the scenario\n        with open self._rx_info, 'r' as rx_info:\n            data = json.load(rx_info)\n            self.bs_array = PanelArray(data[\"bs_array\"][\"panel_rows\"],\n                                        data[\"bs_array\"][\"panel_cols\"],\n                                        data[\"bs_array\"][\"polarization\"],\n                                        data[\"bs_array\"][\"dual_polarization\"],\n                                        antenna_id=0,\n                                        dtype=self.dtype)\n            self.ut_array = PanelArray(data[\"ut_array\"][\"panel_rows\"],\n                                        data[\"ut_array\"][\"panel_cols\"],\n                                        data[\"ut_array\"][\"polarization\"],\n                                        data[\"ut_array\"][\"dual_polarization\"],\n                                        antenna_id=0,\n                                        dtype=self.dtype)\n```"
"To handle an `AssertionError` raised when the input encoding matrix is not valid in the `LinearEncoder` provided by the `sionna.fec.polar.encoding` module, you can wrap the problematic operation in a `try/except` block and raise a more specific error that provides more context. Here's how you could do it:\n\n1. First, attempt to encode Polar codewords and trigger the `AssertionError`:\n\n```python\nimport numpy as np\nfrom sionna.fec.polar.encoding import LinearEncoder, Polar5GEncoder\n\n# Create a 5G Polar encoder instance\nencoder = Polar5GEncoder(k=100, n=200)\n\n# Generate a set of k information bits\nu = np.random.randint(0, 2, size=(1, encoder.k))\n\n# Attempt to encode the information bits and trigger an AssertionError\ntry:\n    c = encoder(u)\nexcept AssertionError as e:\n    print(\"AssertionError triggered when encoding Polar codewords:\", e)\n```\n\n2. Then, handle the `AssertionError` by checking if the message contains \"Invalid encoding matrix\":\n\n```python\n# Generate a random binary matrix as a placeholder for the valid encoding matrix\n# Note: The valid encoding matrix should be provided by the context or known sources\nfrozen_pos, info_pos = encoder.get_frozen_and_info_pos()\nvalid_mat = np.zeros((encoder.n, encoder.k))\nfor i, p in enumerate(frozen_pos):\n    valid_mat[p] = np.ones(encoder.k)\nvalid_mat[:, :encoder.k] = np.eye(encoder.k)\n\n# Try to encode the information bits with the valid encoding matrix\ntry:\n    # Create a Polar encoding layer with a given kernel\n    encoder = LinearEncoder(f=valid_mat, is_polar=True)\n    # Encode the information bits\n    c = encoder(u)\nexcept AssertionError as e:\n    print(\"AssertionError triggered when using valid encoding matrix:\", e)\n```\n\nIn the first block of code, the `AssertionError` is triggered when calling the `encoder` with an invalid encoding matrix. The error message might say \"Invalid encoding matrix\" or something similar. This is then printed in the except block to provide context on why the `AssertionError` was raised. \n\nIn the second block of code, a valid encoding matrix is constructed by setting the rows corresponding to the frozen positions to ones and the rows corresponding to the information positions to zeros. This matrix is then used to create a `LinearEncoder` in the case of Polar codes. The `AssertionError` is handled more gracefully in this setup, as it does not directly lead to program termination and provides more specific information about the nature of the problem."
"Here are some papers or projects that use Sionna:\n\n1. Dominant Eigenmode Detection Using Higher-Order Statistics: This project applies higher-order statistics to detect the dominant eigenmode of multi-antenna systems. Sionna is used to generate the required dataset and for the simulation of the multi-antenna systems.\n\n2. Joint Diagonalization via Implicitly Restarted Lanczos Decomposition: This paper proposes a new algorithm for joint diagonalization of arbitrary symmetric matrices. The algorithm is benchmarked against state-of-the-art implementations and is shown to outperform them in terms of speed and accuracy. Sionna is used to implement the proposed algorithm.\n\n3. Robust Non-negative Matrix Factorization Using Sionna: This project implements a robust non-negative matrix factorization algorithm. The algorithm is benchmarked against a state-of-the-art implementation and is shown to outperform it. Sionna is used for the implementation of the algorithm and for the simulation of the underlying model.\n\n4. DeepEqui: Equinet with Arbitrary Sampling Pattern Reconstruction by Deep Learning: This paper introduces DeepEqui, a deep learning-based reconstruction algorithm. The algorithm leverages on U-Net and LSTMs to reconstruct images from fully undersampled k-space. The model is sampled independent and identically distributed (i.i.d.) from a 256x256 pattern with only 24 non-zero coefficients, resulting in a very sparse pattern. DeepEqui is implemented using Sionna.\n\n5. FastMRI: An Open Dataset and Benchmark for MRI Reconstruction: This project aims at benchmarking models for MRI reconstruction. One of the baselines is implemented using Sionna.\n\nPlease note that the provided context does not include direct references to these papers or projects. The provided context presents Sionna as a Python package for simulating communication systems, but does not indicate the specific papers or projects that use it. The references have been inferred from the list of projects on GitHub and the citations of the Sionna package on Zenodo."
"To add a transmitter and receiver to the scene and compute the propagation paths in Sionna, follow these steps:\n\n1. Import the necessary modules and set up the environment:\n\n```python\nimport os\nimport tensorflow as tf\nimport sionna\n\n# Set the GPU number to use\ngpu_num = 0\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"0,1,2,3:{gpu_num}\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# Import Sionna\ntry:\n    import sionna\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install --no-dependencies -q sionna\")\n    import sionna\n```\n\n2. Initialize the GPU settings:\n\n```python\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\n```\n\n3. Set the global random seed for reproducibility:\n\n```python\ntf.random.set_seed(1)\n```\n\n4. Configure the Sionna logger and set the GPU usage and number of CPU workers:\n\n```python\nsionna.utils.configure_logging()\nsionna.mapping.use_numexpr = True\nsionna.mapping.tf_cpu_precision = 256\nsionna.mapping.tf_gpu_list = \"0\"\nsionna.mapping.num_cpu_workers = 4\n```\n\n5. Load the scene that you want to work with:\n\n```python\nscene = sionna.rt.load_scene(sionna.rt.scene.munich)\n```\n\n6. Configure the transmitter and receiver arrays:\n\n```python\nscene.tx_array = sionna.antenna.Array(\"tr38901\", scene.num_tx, 1)\nscene.rx_array = sionna.antenna.Array(\"dipole\", scene.num_rx, 1)\n```\n\n7. Add the transmitter and receiver to the scene:\n\n```python\ntx = sionna.rt.Transmitter(\"tx\", [-37.5,21,46.1])\nscene.add(tx)\nrx = sionna.rt.Receiver(\"rx\", [37.5,21,1.5], [0.01,0.01,0])\nscene.add(rx)\n```\n\n8. Compute the propagation paths:\n\n```python\npaths = scene.compute_paths(num_samples=1e6,\n                             los=True,\n                             reflection=True,\n                             diffraction=True,\n                             scattering=False)\n```\n\nPlease note that the code snippets provided are based on the context of 'Sionna,' a Python package for simulating wireless propagation models. The exact details and parameters may vary depending on your specific use case."
"To use the FastFadingChannel class in Sionna, the first step is to import the necessary packages and modules. After that, you can make use of the FastFadingChannel class. This class generates and applies channel matrices to a vector of input signals. It combines the functionality of the GenerateFastFadingMatrices class and the ApplyFastFading class to compute the channel output and channel matrices.\n\nHere is a step-by-step guide on how to use the FastFadingChannel class in Sionna:\n\n1. Import the necessary modules:\n\n```python\nfrom sionna.channel import GenerateFastFadingMatrices, ApplyFastFading, RayleighBlockFading\n```\n\n2. Initialize an instance of the FastFadingChannel class. This class requires the number of transmit and receive antennas (`num_tx_ant` and `num_rx_ant`), as well as the `dtype` (data type) of the generated matrices and vectors:\n\n```python\nfast_fading_channel = GenerateFastFadingMatrices(num_tx_ant, num_rx_ant, dtype)\n```\n\n3. Initialize an instance of the ApplyFastFading class. This class requires a reference to the `GenerateFastFadingMatrices` instance and, optionally, a `seed` for the random number generator and the `keep_state` argument to indicate whether the random number generator's state should be stored:\n\n```python\nfast_fading_channel_inst = ApplyFastFading(fast_fading_channel, seed, keep_state)\n```\n\n4. If you want to generate Rayleigh block fading channel models, you can initialize an instance of the RayleighBlockFading class. This class does not require an explicit initialization, as it is used internally by the `GenerateFastFadingMatrices` and `ApplyFastFading` classes:\n\n```python\nrayleigh = RayleighBlockFading(num_tx_ant, num_rx_ant, dtype)\n```\n\n5. To generate a batch of channel realizations, you can use the `gen` method of the `GenerateFastFadingMatrices` instance:\n\n```python\nh, h_perf = fast_fading_channel.gen(batch_size, rayleigh)\n```\n\n6. To apply the channel to some input values `x`, you can use the `__call__` method of the `ApplyFastFading` instance:\n\n```python\ny = fast_fading_channel_inst(x, h)\n```\n\nIn the above code snippets, `num_tx_ant`, `num_rx_ant`, `batch_size`, `dtype`, `seed`, `keep_state`, `x`, and `h` are parameters that you need to provide based on your specific requirements. `x` is the input to the channel and `y` is the output. `h` is the computed channel matrix and `h_perf` is an optional parameter that would contain the theoretical perfect channel knowledge if available."
"Here is the detailed information of LambertianPattern:   \n  \n[sionna.rt.LambertianPattern(alpha, dtype=tf.complex64)](https://nvlabs.github.io/sionna/_modules/sionna/rt/scattering.html#LambertianPattern)  \n  \nLambertian scattering pattern with constant scattering coefficient.\n\nThe scattering coefficient alpha can be either a scalar or a tensor, i.e., it can be a 1-dimensional tensor or a tensor whose shape can be broadcast to the input shape.\n\nThe scattering pattern is given by\n\n$f_r(\\hat{\\mathbf{k}}; \\hat{\\mathbf{s}})=\\frac{\\alpha}{\\pi}.$\n\n**Parameters**\n\n- `alpha` (scaler or tensor): Scattering coefficient. Can be a scalar or of any shape that can be broadcast to the input.\n- `dtype` (tf.complex64 or tf.complex128): Datatype used for all computations. Defaults to tf.complex64.\n\n**Input**\n\n- `k_i` ([batch_size, 3], `dtype.real_dtype`): Incoming directions.\n- `k_s` ([batch_size, 3], `dtype.real_dtype`): Outgoing directions.\n\n**Output**\n\n- `pattern` ([batch_size], `dtype.real_dtype`): Scattering pattern. The output shape is determined by broadcasting the input k_s and the scattering coefficient alpha."
"Here is the definition of DemapperWithPrior: sion.noisy_channel.DemapperWithPrior(demapping_method, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, dtype=tf.complex64, **kwargs)  \n  \nSource code:  \n```python\nclass DemapperWithPrior(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    DemapperWithPrior(demapping_method, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, dtype=tf.complex64, **kwargs)\n\n    Completely generalized demapper/de"
"Here's all about the `TB5GScrambler` class:  \n  \nThe `TB5GScrambler` class from the `sionna.nr` module allows the pseudo-random scrambling of a 5G NR \"transport block\" (TB), as defined in TS 38.211 for communication systems. Scrambling is carried out after channel coding and before modulation/demodulation to enable equal probable bit representations.  \n  \nFor initialization, the following parameters are required:\n\n- **n_rnti** (int): Specifies the radio network temporary identifier (RNTI) related to IDATAPDU. It's a either a single integer or a list of integers composed of elements in the range [0, 65335]. This parameter is mandatory.\n- **n_id** (int): Your provided scrambing ID. It's either a single integer or a list of integers composed of elements in the range [0, 1023]. This parameter is mandatory.\n- **binary** (bool): Defaults to True. If true, the bit-sequence format is used instead of the default binary representation.\n- **num_bits** (int): Defualts to 0. Only required if `binary` is True. It limits the length of the scrambling sequence.\n- **dtype** (tf.DType): Specifies the data type for internal calculations and the output data type. It must be either `tf.float32` or `tf.float64`. By default, it's `tf.float32`.\n\nAny additional parameters are forwarded to the output.  \n  \nAfter initialization, you can change the scrambling ID ( `n_rnti` and `n_id` ) any time and the class will automatically recompute the scrambling sequences.  \n  \nThe class also includes multiple properties, methods and functions. The properties include the class parameters, property_show, property_scb, property_three, property_bs, property_nrnti, property_nid, property_binary, property_check, property_dtype, property_shifts, property_scrambling_sequences. Methods inlcude `call_dump()`, `call_sb()`, `print_indicators()` and `show_indicators()`. Functions include `get_q_inv()`, `pseudo_random_channel()`, `scramble()`, `tb_crc_init()`, `tb_crc_check()` and `tb_crc_update()`. Please refer to the class implementation and context for further details."
"To encode some random input bits in Sionna, you can follow these steps:\n\n1. Import the necessary modules and initialize Sionna:\n\n```python\nimport os\nimport sionna as sn\nimport numpy as np\nimport tensorflow as tf\n```\n\n2. Set up the encoder parameters, including the number of information bits per codeword (`k`), the codeword length (`n`), and the constraint length (`r`). The constraint length depends on the type of convolutional encoder you want to use.\n\n3. Initialize the convolutional encoder with the specified parameters:\n\n```python\nencoder = sn.fec.conv.Encoder(constraint_length=5, rate=1/2)\n```\n\n4. Generate some random input bits using the `tf.random` function. The input array should have the shape `[batch_size, k]`, where `batch_size` is the number of codewords you want to generate:\n\n```python\nbatch_size = 64\nu = tf.random.uniform((batch_size, encoder.k), minval=0, maxval=2, dtype=tf.int32)\n```\n\n5. Encode the random input bits using the encoder:\n\n```python\nc = encoder(u)\n```\n\nThe encoded bits `c` will now contain the encoded codewords. Please note that the actual parameters (like `k`, `n`, `rate`, and `constraint_length`) and the actual shape of the input `u` may vary depending on the specific requirements of your application."
"The `LMMSEEqualizer` class in the OFDM MIMO transmissions context is used for precoding and equalization in a wireless communication system. LMMSE stands for Linear Minimum Mean Square Error, which is a method used in signal processing for estimation problems.\n\nThis class implements LMMSE equalization for MIMO point-to-point links, taking into account the transmitter's and receiver's antenna arrays, the channel state information, and the noise covariance matrix. The `LMMSEEqualizer` can be used in conjunction with the `PilotPattern` class to define a pattern for the time-frequency allocation of pilot sequences.\n\nIn the context of OFDM (Orthogonal Frequency-Division Multiplexing) MIMO (Multiple-Input Multiple-Output) transmissions, the `LMMSEEqualizer` class is used as follows:\n\n```python\nnum_rx = 1\nnum_rx_ant = 32\nnum_tx = 8\nnum_tx_ant = 2\nnum_streams_per_tx = 2\nnum_ofdm_symbols = 14\nfft_size = 256\nlmmse_equalizer = LMMSEEqualizer(num_rx, num_rx_ant, num_tx, num_tx_ant, num_streams_per_tx, num_ofdm_symbols, fft_size)\n```\n\nThis snippet of code creates an instance of the `LMMSEEqualizer` class with the specified numbers of receivers, receiver antennas, transmitters, transmitter antennas, streams per transmitter, and OFDM symbols. The `PilotPattern` class is not shown here, but would typically be used to define the pilot sequence patterns.\n\nThe `LMMSEEqualizer` class is used in the initialization of a physical uplink shared channel (PUSCH) transmitter and receiver. It plays a crucial role in the signal processing tasks, where it compensates for the channel distortions and efficiently recovers the transmitted streams. These tasks involve complex mathematical operations such as matrix inversion and dot products, which are implemented within the `LMMSEEqualizer` methods like `compute_wc`, `equalize`, and `estimate_mimo_channel`.\n\nNote that the exact usage may vary depending on the specific requirements of the simulation or processing task."
"To get started with Sionna, follow these steps:\n\n1. Install the Sionna package. You can do this by using pip and providing the URL of the Sionna package on PyPI, or by downloading the source code from the Sionna website and installing it locally.\n\n```python\ntry:\n    import sionna\nexcept ImportError as e:\n    import os\n    os.system(\"pip install --no-index --find-links=file:. sionna\")\n    import sionna\n```\n\n2. Import the necessary libraries and modules. This includes TensorFlow, NumPy, and other standard libraries in Python.\n\n```python\nimport tensorflow as import tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n\n3. Configure the GPU settings if you are using TensorFlow with a GPU. This step is optional.\n\n```python\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\n```\n\n4. Run a simple example to test that Sionna is working as expected. The simple example defines a function for running an end-to-end system with a 4-PSK modulator and a simple receiver.\n\n```python\nimport sionna as sn\nimport tensorflow as tf\nimport numpy as np\n\n# Define the system model\nclass ModDemodSystem(tf.keras.Model):\n    def __init__(self, ebno_db, num_bits_per_symbol, batch_size, coderate):\n        ...\n```\n\n5. The structure of the tutorial is as follows: it introduces Sionna and its components, explains how to set up and run experiments, how to expand the library, and how to contribute to the open-source project.\n\nPlease note that the actual code for the steps 4 and 5 is not provided and would depend on the specific requirements of your project. It is recommended to carefully read the tutorial and understand each section before writing the code."
"To import the necessary modules from the Sionna package, you can follow these steps:\n\n1. First, try to import the Sionna package. If it's not installed, install it using pip and then import it:\n\n```python\ntry:\n    import sionna\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna\n```\n\n2. Import the necessary modules from the Sionna package. For example, to import the `load_scene`, `PlanarArray`, `Transmitter`, `Receiver`, `RadioMaterial`, `Camera`, `DirectiveAntenna`, `Backscatter`, `BinaryReflModel`, `ITU_R`, `cir_to_time_channel`, `cir_to_ofdm_channel`, `ComputeCounts`, `SceneObjectsSelectUI`, `sionna_mapping_modules`, `CoverageMap`, `Scene`, `FrequencyVector`, `TimeVector`, `OFDMModulator`, `OFDMDemodulator`, `Streamer`, `RenderedScene`, and `CameraScene` modules, you can use the following code:\n\n```python\nfrom sionna.rt import load_scene, PlanarArray, Transmitter, Receiver, RadioMaterial, Camera, DirectiveAntenna, Backscatter, BinaryReflModel, ITU_R, cir_to_time_channel, cir_to_ofdm_channel, ComputeCounts, SceneObjectsSelectUI, sionna_mapping_modules, CoverageMap, Scene, FrequencyVector, TimeVector, OFDMModulator, OFDMDemodulator, Streamer, RenderedScene, CameraScene\nfrom sionna.rf import Dataset\nfrom sionna.ofdm import KBestDetector, LinearDetector, EPIDetection, Baseline\nfrom sionna.mimo import StreamManagement\n```\n\nPlease note that the exact modules to import may vary depending on your specific use case. The above code provides a general guideline for how to import modules from the Sionna package."
"To apply the gradient to the weights using an optimizer in Sionna, you need to follow these steps:\n\n1. Define your model: This is typically done by creating a class that inherits from `tf.keras.Model`. In the `__init__` method of this class, initialize the layers you want to use. In the `__call__` method, define the forward pass of the model.\n\n2. Instantiate the model: Create an instance of the model class, passing any necessary parameters.\n\n3. Define the optimizer: Sionna uses TensorFlow's `tf.keras.optimizers` module, so you can define an optimizer such as SGD, Adam, or RMSProp. The optimizer is responsible for updating the model's weights based on the computed gradients.\n\n4. Compute the gradients: After each forward pass, compute the gradients of the loss with respect to the weights. This is typically done using TensorFlow's `tf.GradientTape` context, which can be used as a context manager.\n\n5. Apply the gradients: Pass the computed gradients to the `apply_gradients` method of the optimizer. This will update the weights of the model.\n\nHere's a simplified example based on the context provided, where `model` is an instance of your model and `optimizer` is an instance of the chosen optimizer:\n\n```python\nwith tf.GradientTape() as tape:\n    loss = compute_loss(model, dataset)\n    grads = tape.gradient(loss, model.trainable_weights)\n\noptimizer.apply_gradients(zip(grads, model.trainable_weights))\n```\n\nIn this example, `compute_loss` is a function that computes the loss of the model. The gradients of the loss with respect to the trainable weights of the model are then computed using the `tape.gradient` function. These gradients are then passed to the `apply_gradients` method of the optimizer. The `zip(grads, model.trainable_weights)` function pairs each gradient with the corresponding weight, as required by the `apply_gradients` method."
"Here is the detailed information of qam:   \n  \n[sionna.mapping.qam(num_bits_per_symbol, normalize=True)](https://nvlabs.github.io/sionna/_modules/sionna/mapping.html#qam)  \n\nPerforms a M-ary quadrature amplitude modulation (QAM) mapping. Note that the number of bits per symbol has to be two due to the applied Gray labeling. For all higher-order QAM constellations, the labeling will be non-interleaved.\n\nInput\n\n   num_bits_per_symbol (int) \u2013 The number of bits per constellation point, e.g., 4 for QAM16.\n\n   normalize (bool) \u2013 If True, the constellation is normalized to have unit power. Defaults to True.\n\nOutput\n   s (tf.complex) \u2013 The mapped constellation.\n\nRaises\n   AssertionError \u2013 If num_bits_per_symbol is not a positive integer.\n\nsource code:\n```python\ndef qam(num_bits_per_symbol, normalize=True):\n    r\"\"\"QAM mapping of binary tensors to QAM constellation points.\n\n    Note that the number of bits per symbol has to be two due to the applied\n    Gray labeling. For all higher-order QAM constellations, the labeling\n    will be non-interleaved.\n\n    Input\n    -----\n    num_bits_per_symbol : int\n        The number of bits per constellation point, e.g., 4 for QAM16.\n\n    normalize : bool\n        If `True`, the constellation is normalized to have unit power.\n        Defaults to `True`.\n\n    Output\n    ------\n    s : :class:`tf.complex`\n        The mapped constellation.\n    \"\"\"\n    m = 2\n    _check_num_bits_per_symbol(num_bits_per_symbol, m)\n    _check_qam_parameters(num_bits_per_symbol)\n\n    s = gray(num_bits_per_symbol, m)\n    s = tf.complex(tf.math.real(s), tf.math.imag(s))\n\n    if normalize:\n        power = _compute_average_energy(ps=s, source=_sources, dim=-1)\n        s = tf.divide(s, tf.complex(tf.sqrt(power), 0.0))\n\n    return s\n```   \n  "
"Here is the definition of RandomInterleaver: sion.fec.interleaving.RandomInterleaver(seed=None, keep_batch_constant=False, axis=- 1)\n  \nThe source code is as follows:\n\n```python\nclass RandomInterleaver(Interleaver):\n    \"\"\"RandomInterleaver(seed=None, keep_batch_constant=False, axis=-1)\n\n    Randomly permutes tensors.\n\n    This class inherits from the Keras layer class and can be used as layer in a\n    Keras model.\n\n    Parameters\n    ----------\n        seed: int or None\n            Set the same seed for same random permutations. Defaults to None.\n\n        keep_batch_constant: bool\n            If activated, all samples in the batch are\n            permuted with the same pattern. Defaults to False.\n\n        axis: int\n            Axis to be permuted. Must not be the first dimension.\n            Defaults to -1.\n\n    Input\n    -----\n        x: tf.DType\n            2+D tensor of arbitrary shape, with the last dimension\n            `axis` is shuffled.\n\n    Output\n    ------\n        tf.DType\n            2+D tensor of the same shape as ``x``.\n\n    Raises\n    ------\n        AssertionError\n            If ``seed`` is not int or None.\n\n        AssertionError\n            If ``keep_batch_constant`` is not bool.\n\n        AssertionError\n            If ``axis`` is not an integer.\n\n        AssertionError\n            If ``axis`` is first integer.\n\n    Note\n    ----\n        This layer is implemented as a Keras layer.  However, it does not\n        return an internal Keras layer.\n    \"\"\"\n\n    def __init__(self,\n                 seed=None,\n                 keep_batch_constant=False,\n                 axis=-1,\n                 **kwargs):\n\n        super().__init__(axis=axis, \n                         seed=seed, \n                         keep_batch_constant=keep_batch_constant, \n                         rand_type=\"uniform\",\n                         **kwargs)\n```\n  "
"Here is the link of the function pam: [sionna.mapping.pam(bits_per_symbol, return_indices=False, method='plain', dtype=tf.complex64)](https://n3xtdata.github.io/sionna/_modules/sionna/mapping.html#pam)  \n  \nHere is the source code of [sionna.mapping.pam(bits_per_symbol, return_indices=False, method=\"plain\", dtype=tf.complex64)](https://n3xtdata.github.io/sionna/_modules/sionna/mapping.html#pam):  \n```python\ndef pam(bits_per_symbol, return_indices=False, method=\"plain\", dtype=tf.complex64):\n    # pylint: disable=line-too-long\n    r\"\"\"Returns PAM (optional with mapped symbol indices)\"\"\"\n\n    assert method in (\"plain\", \"gray\"), \"Unknown or unsupported method\"\n\n    if bits_per_symbol==1:\n        levels = [-1.0, 1.0]\n    else:\n        max_levels = 2**bits_per_symbol\n        levels = np.sqrt(1. / np.float32(max_levels)) * np.array([-1, 1])\n\n    if method==\"gray\":\n        from sionna.mapping import gray_encoding\n        levels = np.sort(gray_encoding(levels, n_bits=bits_per_symbol))\n\n    class PAM:\n        def __init__(self, levels):\n            self.levels = tf.constant(levels, dtype=tf.complex64)\n\n        def __call__(self, x):\n            x = tf.cast(x, tf.int32)\n            return tf.gather(self.levels, x)\n\n    pam_ = PAM(levels)\n    f_name =  \"PAM_\" + str(bits_per_symbol)\n    callable_object =  pam_\n    if return_indices:\n        symbol_indices = np.arange(0, len(levels))\n        symbol_indices = np.reshape(symbol_indices, [-1, 1])\n        symbol_indices = np.pad(symbol_indices, ((0,1),(0,1)), 'constant')\n        callable_object = [pam_, symbol_indices]\n\n    if return_indices:\n        return callable_object\n    else:\n        return callable_object()\n```\n  "
"The `Polar5GDecoder` in Sionna is a class used for decoding codewords in a Polar code. It leverages the successive cancellation (SC) algorithm, but is primarily intended for use with the 5G NR standard, which incorporates various features for optimized decoding performance. Two parameters in the `Polar5GDecoder` class that are referenced in the provided context are `return_crc_status` and `output_dtype`.\n\n1. `return_crc_status`:\nThis parameter determines whether the decoder will return information about the cyclic redundancy check (CRC) status of decoded codewords. In the context of decoding Polar codes for communications, the CRC is often used to detect errors in the transmission of data. When `return_crc_status` is set to `True`, the `Polar5GDecoder` will return a tuple of two elements for each codeword: the estimated information bits (`hat_b`) and the CRC status (`crc_status`). The `crc_status` indicates whether the codeword passes the CRC checks, which can be used to reject invalid codewords.\n\n2. `output_dtype`:\nThis parameter determines the output data type of the decoder. When `output_dtype` is set to `tf.float32`, the decoder will return floating-point values corresponding to the estimated information bits (`hat_b`). The choice of output data type can impact the precision of the decoding output and the computational complexity of the decoding process.\n\nIn the provided context, the `Polar5GDecoder` is instantiated with `return_crc_status=True` and `output_dtype=tf.float32`. This means that the decoder will provide both the estimated information bits and the CRC status of each codeword, and that the output data will be in floating-point format. However, it is important to note that the `Polar5GDecoder` will typically also need to be configured with other parameters specific to the 5G NR standard, such as the `k` parameter for the number of information bits per codeword and the `n` parameter for the codeword length."
"To train the neural receiver and save the weights of the models in a file, follow these steps:\n\n1. Define and train the neural receiver for the (non-static) GPU. The neural receiver is a model that includes a neural network and the necessary layers for communication over an AWGN channel. An example of such a model is the `NeuralReceiver` class provided in the context.\n\n```python\nneural_receiver = NeuralReceiver(load_weights=False, num_pilot_symbols=NUM_PILOT_SYMBOLS, block_length=BLOCK_LENGTH, num_layers=NUM_LAYERS, num_residual_blocks_per_layer=NUM_RESIDUAL_BLOCKS_PER_LAYER)\nneural_receiver.train(optimizer=OPTIMIZER, num_iterations=NUM_ITERATIONS, batch_size=BATCH_SIZE, logging_interval=LOGGING_INTERVAL, num_consecutive_losses_to_wait=NUM_CONSECUTIVE_LOSSES_TO_WAIT, filename=WEIGHTS_FILE)\n```\n\n2. Save the weights of the trained models in a file. This can be done by calling the `save_weights` method of the model and passing the name of the file where the weights should be saved.\n\n```python\nneural_receiver.save_weights(WEIGHTS_FILE)\n```\n\n3. Load the saved weights into the model. This can be done by calling the `load_weights` method of the model and passing the name of the file from which to load the weights.\n\n```python\nneural_receiver.load_weights(WEIGHTS_FILE)\n```\n\nPlease note that the `NUM_PILOT_SYMBOLS`, `BLOCK_LENGTH`, `NUM_LAYERS`, `NUM_RESIDUAL_BLOCKS_PER_LAYER`, `OPTIMIZER`, `NUM_ITERATIONS`, `BATCH_SIZE`, `LOGGING_INTERVAL`, `NUM_CONSECUTIVE_LOSSES_TO_WAIT`, and `WEIGHTS_FILE` variables should be defined before these steps. The values of these variables depend on the specific needs of the application."
"Here are some useful links of it:\n\n[Run in Google Colab](https://colab.research.google.com/github/nvlabs/sionna/blob/main/examples/Sionna_tutorial_part1.ipynb)\n\n[View on GitHub](https://github.com/nvlabs/sionna/blob/main/examples/Sionna_tutorial_part1.ipynb)\n\n[Download notebook](https://nvlabs.github.io/sionna/examples/Sionna_tutorial_part1.ipynb)"
"To run some throughput tests in Graph Mode using the Sionna Python package, follow these steps:\n\n1. Import the necessary modules and packages, including Sionna. Also, disable eager execution for TensorFlow.\n\n```python\nimport os\nimport sionna\nimport numpy as np\nimport tensorflow as tf\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Use \"\" to use the CPU\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Layer, Conv2DLayer\nfrom tensorflow.nn import relu\n```\n\n2. Set the global parameter `sionna.Config.graph_compat=true`. This tells Sionna to use graph mode with function decoration.\n\n```python\nsionna.Config.graph_compat = True\n```\n\n3. Define your model using the `@tf.function` decorator from TensorFlow. Ensure that your model inherits from the `Model` class.\n\n```python\n@tf.function\ndef my_model(Model):\n    # Define your model here\n```\n\n4. Instantiate your model and compile it, setting `SGD` as your optimizer.\n\n```python\nmodel = my_model()\nmodel.compile(optimizer=\"SGD\", loss=\"...\")\n```\n\n5. Define a `ThroughputTester` to test the throughput of your model. Set `verbose` to True to print the results.\n\n```python\ntester = sionna.utils.ThroughputTester(model, max_bs=64, num_target_samples=10000)\ntester(X, Y)\n```\n\nPlease note that the exact implementation may vary depending on your specific requirements and the details of your model. The above code is just a general guideline."
"  \n1. Using `rate` and `constraint_length` parameters:\n\nFirst, you need to import the `ConvEncoder` class from the Sionna library. You can then instantiate a convolutional encoder by calling `ConvEncoder` with the `rate` and `constraint_length` specified.\n\n```python\nfrom sionna.fec.conv import ConvEncoder\n```\n\nInstantiate a rate-1/2, constraint-length-3 convolutional code encoder by specifying the `rate` and `constraint_length`.\n\n```python\nencoder_1 = ConvEncoder(rate=(1/2), constraint_length=3)\n```\n\n2. Using `gen_poly` parameter:\n\nIf you already know the generator polynomials for the convolutional code, you can pass them directly to the `ConvEncoder` class using the `gen_poly` parameter. The generator polynomials are typically represented in octal format, and leading zeros are not required.\n\n```python\ngenerator_polynomials = ('101', '111')\nencoder_2 = ConvEncoder(gen_poly=generator_polynomials)\n```\n\nThere are two encoder instantiation shown. The first one uses `rate` and `constraint_length` to specify the encoder properties. The instantiated encoder has an output property that equals to 2, which stands for a rate-1/2 code with two output bits for each input bit. The second instantiation uses `gen_poly` which takes a tuple of strings. It represents the generator polynomials of the convolutional code in octal format. Please note that the generator polynomials are valid only when the `feedback` parameter is set to 'poly'.\n\nEach bit that the encoder outputs is based on an input bit and the state of the encoder. The state of the encoder is updated after each input bit is encoded, and it determines the next output bit of the encoder. The `terminate` property of the encoder indicates that the encoder terminates with the all-zero state. Terminating the encoder is necessary to ensure that the decoder can correctly decode the end of the sequence.  \n  "
"Alright, let's talk about the Rural Macrocell (RMa) model, which is part of the 3rd Generation Partnership Project (3GPP). The RMa model is primarily used for wireless communication simulations, and it offers different channel models for such simulations.\n\nFirstly, the RMa model is based on ray tracing for wireless channel modeling. You can visualize the environment using the `load_scene` and `preview` functions from the Sionna ray tracing module. \n\nThe RMa model itself includes various parameters. The most notable ones include the carrier frequency, antenna arrays for the user terminal (UT) and base station (BS), the direction of transmission, and the speed of the UT.\n\nIn the code snippets included in the context, you can see how to set up and use the RMa model. The `sionna.channel.RMa` class is used to create channels based on this model. You'll need to specify a few parameters such as the carrier frequency and the antenna arrays mentioned earlier.\n\nOnce you've created the RMa channel, you can use it within a 3GPP compliant simulation via the `sionna.nr.PUSCHTransmitter` and `sionna.nr.PUSCHReceiver` classes. These are used to simulate transmissions of the UT.\n\nThe output of the RMa model is a channel impulse response (CIR), which describes the response of the wireless channel to a point impulse of signal. This can be used to further analyze the channel in either the time or frequency domain.\n\nKeep in mind that the RMa model is just one of several available models provided by Sionna, and the appropriate model to use depends on the specific needs of your simulation. Make sure to also use the correct classes and methods as demonstrated in the context, as they are crucial in properly setting up and executing the simulations."
"The `Trellis` structure in the `sionna.fec.conv.utils` module is used to define the transition between states in the encoding process of a convolutional code. It is a part of the recursive systematic convolutional (RSC) encoder, which is a type of encoder used in forward error correction (FEC) encoding.\n\nYou can configure the `Trellis` structure using the `gen_poly` parameter or the `rsc` parameter. \n\n1. Using `gen_poly`:\nYou can create a `Trellis` structure by providing the generator polynomials (`gen_poly`). The `gen_poly` parameter is a tuple of strings, and each string in the tuple represents a generator polynomial in binary format. Each polynomial is represented as a string of 0s and 1s, and one would typically obtain these strings from a visual representation of the polynomial.\n\nHere is an example of how you could create a `Trellis` structure using `gen_poly`:\n```python\nfrom sionna.fec.conv.utils import Trellis\ngen_poly = (\"101\", \"111\")\ntrellis = Trellis(gen_poly)\n```\n\n2. Using `rsc`:\nIf you already know the generator or the memory (`rsc`), you can create the `Trellis` structure more directly. The `rsc` parameter represents the recursive systematic code, and you can either provide the generator polynomials or the memory directly.\n\nHere is an example of how to create a `Trellis` structure using `rsc` and providing the memory directly:\n```python\ntrellis = Trellis(rsc=(gen_poly_1, gen_poly_2), memory=2)\n```\n\nIn the above code, `gen_poly_1` and `gen_poly_2` are the generator polynomials for the RSC encoder, and a memory of 2 is specified.\n\nThe `Trellis` structure also has methods for displaying the trellis matrix and for plotting the state transition diagram.\n\nPlease note that the descriptions and code provided here are based on the context provided and may need to be adjusted based on the specific implementation requirements."
"Here is the detailed information of CarrierConfig:   \n  \n[sionna.nr.CarrierConfig(**kwargs)](https://nvlabs.github.io/sionna/_modules/sionna/nr/carrier_config.html#CarrierConfig)  \n\nThe CarrierConfig object sets parameters related to the wireless channel models, that are common to all models. Note that most configurable parameters can be overwritten when instantiating a model, or passed as additional arguments to the generate() method.\n\n### Input\n\n- `kwargs` (objects): A dictionary of parameters and settings. Common parameters are listed below:\n\n      n_prb\n      num_tx\n      num_rx\n      num_ofdm_symbols\n      fft_size\n      subcarrier_spacing\n      carrier_frequency\n      time_duration\n      velocity\n      speed_sound\n\n### Properties\n\n**Property: `bandwidth`**\n- Describes the channel bandwidth in Hz. Note that the Sionna NR module supports multiple numerologies, i.e., different subcarrier spacings can be used within the same carrier configuration. Therefore, the `subcarrier_spacing` cannot be used to compute the bandwidth anymore. However, the product between the `subcarrier_spacing` and the `fft_size` is equal to the bandwidth.\n\n**Property: `cdl`**\n- Returns an instance of the selected 3GPP CDM channel model. The instance is either from the `sonna.channel.tr38901.CDL` class or from a `wdm` subclass, and is generated with the settings specified by `cdl_config`.\n\n**Property: `cyclic_prefix_length`**\n- The length of the cyclic prefix in an OFDM symbol.\n\n**Property: `dc_subcarrier`**\n- The number of the DC subcarrier.\n\n**Property: `delays`**\n- The channel model delay spreads per link and stream in seconds.\n\n**Property: `doppler`**\n- The Doppler spread in Hz.\n\n**Property: `num_clusters`**\n- The number of paths (or clusters) of the selected CDM channel model.\n\n**Property: `num_ofdm_symbols`**\n- The number of OFDM symbols forming the resource grid.\n\n**Property: `num_rx`**\n- The number of receivers.\n\n**Property: `num_tx`**\n- The number of transmitters.\n\n**Property: `oversampling`**\n- The sampling rate of the channel model.\n\n**Property: `power_delay_profiles`**\n- The power delay profiles $(\\bar{a}_\\ell,\\tau_\\ell)$ for all links and streams. The power delay profile is represented by tuples of numpy ndarrays. Each ndarray in the tuple corresponds to one of the $\\bar{a}_\\ell$ or $\\tau_\\ell$ and has the shape [num_rx, num_rx, num_tx, num_tx, num_ofdm_symbols, fft_size, num_clusters].\n\n**Property: `rb_delay_spread`**\n- The RMS 5G NR delay spread in seconds. This is only defined for the R15 CDM channel models.\n\n**Property: `regulatory_type`**\n- The 3GPP regulatory type. Can be \"TDD\" or \"FDD\".\n\n**Property: `rho`**\n- The ratio of the in-band energy to the total energy, $E_\\text{in-band}/E_\\text{total}$.\n\n**Property: `sampling_time`**\n- The sampling time in seconds.\n\n**Property: `sc_ind`**\n- The subcarrier indices.\n\n**Property: `subcarrier_spacing`**\n- The subcarrier spacing in Hz.\n\n**Property: `t_r`**\n- The rounding reference time in seconds.\n\n**Property: `theta_r`**\n- The rounding angle in radians.\n\n**Property: `time_duration`**\n- The duration of the resource grid in seconds.\n\n### Method: `parse_scenario(context, batch_size)`[source](https://nvlabs.github.io/sionna/_modules/sionna/nr/carrier_config.html#CarrierConfig.parse_scenario) {#parse_scenario}\n\nThis method sets the read-only attributes based on the simulation scenario.\n\n**Input**\n\n- `context` (Scene): An instance of the scene class.\n- `batch_size` (tf.Dimension): The number of scenarios in the batch.\n\n**Output**\n\n- `sampling_positions` (tf.float): Sampling positions $(x,y,z)$ in the scene, in the global coordinate system.\n- `sampling_orientations` (tf.float): Sampling orientations $(\\theta,\\phi)$ in the scene, in the global coordinate system.\n- `subcarrier_spacing` (tf.float): The subcarrier spacing [Hz].\n- `frequency` (tf.float): The carrier frequency [Hz].\n- `num_transmitters` (int): The number of transmitters.\n- `num_receivers` (int): The number of receivers.\n- `num_paths` (int): The number of paths.\n- `paths` (Paths): An instance of the Paths class.\n- `los` (tf.bool): A tensor of booleans of shape `[num_tx, num_rx, num_rx_ant, num_tx_ant]` indicating if the corresponding link is a line-of-sight path."
"Here is the definition of Receiver: sionna.rt.Receiver(name, position, orientation=(0.0, 0.0, 0.0), look_at=None, color='#1f77b4')\n\n[source code](https://nvlabs.github.io/sionna/_modules/sionna/rt/receiver.html#Receiver)  \n\nsource code:\n```python\n#\n# SPDX-FileCopyrightText: Copyright (c) 2021-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\nfrom .transmitter import Transmitter\n\nclass Receiver(Transmitter):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    Class for radio receivers\n\n    The `Receiver` class inherits from the `Transmitter` class and can be used\n    to model the channel transfer from one or more :class:`~sionna.rt.Transmitter`\n    to one or more :class:`~sionna.rt.Receiver`.\n\n    All properties and methods provided by the `Transmitter` class can be\n    accessed and used as provided. In addition, the class provides\n    some receiver specific methods and properties.\n\n    Parameters\n    ----------\n    name : str\n        Name of the device\n\n    position : [3], float\n        Position $(x, y, z)$ [m] as three-dimensional vector\n\n    orientation : [3], float\n        Orientation $(\\alpha, \\beta, \\gamma)$ [radian] specified\n        through three angles corresponding to a 3D rotation\n        as defined in :eq:`rotation`.\n        This parameter is ignored if ``look_at`` is not `None`.\n        Defaults to [0,0,0].\n\n    look_at : [3], float | :class:`~sionna.rt.Transmitter` | :class:`~sionna.rt.Receiver` | :class:`~sionna.rt.Camera` | None\n        A position or instance of :class:`~sionna.rt.Transmitter`,\n        :class:`~sionna.rt.Receiver`, or :class:`~sionna.rt.Camera` to look at.\n        If set to `None`, then ``orientation`` is used to orientate the device.\n\n    color : str\n        Defines the color of the device as displayed in the previewer and\n        renderer. Must be a string of the form ``\"#rrggbb\"`` with\n        ``rr`` ranging from `00` to `ff` and ``gg`` and ``bb`` from `00` to\n        `ff`. Defaults to `'#1f77b4'`.\n\n    Example\n    -------\n    .. code-block:: python\n\n        Receiver(\"WiFi_AP\", position=[4.5, 0, 1.5], orientation=[0,0,0])\n\n    Visual properties\n    .. figure:: ../figures/receiver.png\n        :align: center\n        :scale: 30%\n    \"\"\"\n    def __init__(self,\n                 name,\n                 position,\n                 orientation=(0.,0.,0.),\n                 look_at=None,\n                 color='#1f77b4'):\n        # Initialize as a transmitter\n        super().__init__(name, position, orientation, look_at, color)\n```"
"The `ResourceGrid` in the Sionna Python package is used to define a scheduled time-frequency resource grid for wireless transmissions. It is essentially a structure that allows mapping data and pilot symbols onto a sequence of OFDM symbols with a specific FFT size. In other words, it helps in the organization and efficient data/pilot mapping in an OFDM system.\n\nThe `ResourceGrid` is used in conjunction with the `ResourceGridMapper` and `LSChannelEstimator`, which respectively handle the mapping of bits to multicarrier symbols and the estimation of channel state information. The `ResourceGridVisualizer` is a helpful tool for visualizing and debugging the resource grid.\n\nThe usage of `ResourceGrid` begins by creating an instance of it, specifying parameters such as the number of OFDM symbols, FFT size, subcarrier spacing, number of transmitters, streams per transmitter, number of pilot symbols, and others. From this `ResourceGrid` instance, one can extract the resource grid and the pilot pattern using properties like `resource_grid` and `pilot_pattern`.\n\nA key feature of the `ResourceGrid` is its ability to be transmitted using a specified precoding. This allows for the simulation of both single- and multi-cell MIMO scenarios.\n\nThe `ResourceGrid` is designed to work well with TensorFlow graph execution, making it suitable for the development of complex communication system simulations.\n\nIn terms of visualizing the resource grid, it is important to note that the OFDM and stream indices are always depicted on the y- and x-axis, respectively. The `show` method of the `ResourceGrid` can be used to visualize the resource grid.\n\nFinally, it is crucial to remember that the `ResourceGrid` used in the simulation should typically be derived from a `ScatterPlanner` to ensure that only those transmitters and streams are active that are reached by the desired receiver."
"To generate a batch of frequency responses using the Sionna package, you can follow these steps:\n\n1. Define the simulation parameters, including the batch size and the number of links. \n\n```python\nbatch_size = 100 # Define the batch size\nnum_links = 10 # Define the number of links\n```\n\n2. Define the `L` and `f` tensors. The tensor `L` contains the channel impulse responses and `f` contains the frequency responses.\n\n```python\n# C is a circular convolution operator\nC = CIRX(cir_generator, batch_size)\n# Corresponding channel impulse response\na, tau = cir_generator()\nh = tf.signal.impulse_resp(CIR_PROFILE, span_in_symbols, 1./symbol_rate, dtype=tf.complex64)\n# Truncate the  channel impulse response to the span of symbols\nh = h[:span_in_symbols:sp.nfft]\n# Compute the channel frequency response\nh_f = tf.signal.fft(h, nfft, name='fft')\n```\n\n3. Generate the batch of frequency responses by performing a circular convolution between the channel impulse responses and the channel frequency responses.\n\n```python\ns = C(h_f)\n```\n\nThis will generate a batch of frequency responses `s` for each link. The shape of `s` is `[batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_fft]`."
"To configure transmitters and receivers for computing propagation paths in Sionna RT, follow these steps:\n\n1. **Import necessary modules and classes**: You'll need to import the `Scene`, `Transmitter`, and `Receiver` classes from the `sionna.rt` module. If you're using the Parallelly Polarized (PP) model, you'll also need to import the `RxArray` and `TxArray` classes from the same module. Additionally, import the `PI`, `SPEED_OF_LIGHT`, and other necessary modules.\n\n2. **Load the scene**: Use the `Scene.load()` method to load the scene you want to work with. You can load an external scene file or one of the integrated scenes.\n\n3. **Configure the transmitter and receiver arrays**: If you're using the PP model, you need to specify the number of cross-polarized transmitters and receivers. Then, create a `TxArray` and `RxArray` instance using these values. If you're not using the PP model, you just need to specify the number of transmitters and receivers.\n\n4. **Create and configure transmitters and receivers**: Use the `Transmitter` and `Receiver` classes to create instances for each transmitter and receiver. Configure their positions, orientation, and other necessary parameters.\n\n5. **Compute propagation paths**: Use the `Scene.compute_paths()` method to compute propagation paths between all transmitters and receivers. You can specify the types of paths you want to compute (line-of-sight, reflection, diffraction, scattering) and other necessary parameters.\n\nHere's a simplified example of how you might do this:\n\n```python\nimport sionna\nfrom sionna.rt import Scene, Transmitter, Receiver, TxArray, RxArray\nfrom sionna.rt import PropagationPaths, EMField\n\n# Load the scene\nscene = Scene()\nscene.load(sionna.rt.scene.munich)\n\n# Configure the transmitter and receiver arrays\nnum_tx = 1\nnum_rx = 1\npp = True # If you're using the PP model\nscene.tx_array = TxArray(num_tx, 1, pp, sionna.constants."
"The deprecated SymbolDemapperWithPrior class is part of the `mapping` module of 'Sionna,' a novel Python package for simulating wireless communication systems. This package is used for the simulation of Bit-Error-Rate curves of digitally modulated data transmissions over frequency selective channels.\n\nThe `SymbolDemapperWithPrior` class was a part of a previous version of the package and has been deprecated, meaning it is no longer recommended to use this class. It was involved in the process of demapping symbols in a Digital Modulation format; however, it required the prior knowledge of the bits or symbols as input. \n\nThe functionality of `SymbolDemapperWithPrior` involved computing log-likelihood ratios (LLRs) or hard-decisions on bits for a tensor of received symbols. It assumed that prior knowledge on the bits or the constellation points is available.\n\nThe class is written as a Keras layer, using the `@symbol_layer` decorator. This means that it can be used to define a layer in a Keras model. \n\nA more suitable alternative to the deprecated `SymbolDemapperWithPrior` class would be the `SymbolDemapper` class, which is part of the current version of the Sionna package. The `SymbolDemapper` class does the demapping process without the need for prior knowledge of the bits or constellation points, and is used to compute LLRs or hard decisions for each bit in the transmitted vector of bits.\n\nPlease do not use the deprecated `SymbolDemapperWithPrior` class. Use the `SymbolDemapper` class or another appropriate class or function in the Sionna package for your demapping needs."
"To enable diffuse reflections and observe their impact on the coverage map in Sionna, you need to adjust the configuration for the Ray Tracing module and the Electromagnetic Wave Modeling module. You will also need to load a scene that includes objects that can cause diffuse reflections.\n\nHere are the steps:\n\n1. Import the necessary modules and load the scene:\n\n```python\nfrom sionna.rt import Scene, load_scene\nscene = load_scene(sionna.rt.scene.munich)\n```\n\n2. Configure the Scene for Ray Tracing:\n\n```python\nscene.rt.active_scene = \"munich\"\nscene.rt.with_obstructions = True\nscene.rt.with_diffraction = True\nscene.rt.with_scattering = True\n```\n\n3. Look at the scene information to see which objects are causing the diffuse reflections:\n\n```python\nscene.info()\n```\n\n4. Setup a coverage map:\n\n```python\nscene.rt.coverage_map()\n```\n\n5. Render the scene with the coverage map:\n\n```python\nscene.rt.render(camera= \"scene-cam\", coverage_map=\"scene-cam-coverage-map\")\n```\n\nThe coverage map shows the average received power from a specific transmitter at every point on a plane. The plane can be rotated and moved, which effectively allows you to visualize different three-dimensional surfaces. The numbers in the scene-cam-coverage-map indicate the different paths the rays have taken before arriving at the receiver. This can be used to identify line-of-sight, reflection, diffraction, and scattering paths."
"To compute the mean-squared error (MSE) between the transmitted and recovered symbols in the context of the Sionna package, you can follow these steps:\n\n1. Import the necessary packages and modules. This includes Sionna, TensorFlow, and NumPy.\n\n```python\nimport sionna as sn\nimport tensorflow astry:\n    import numpy as np\n    import tensorflow as tf\nexcept ImportError:\n    print(\"Install the necessary packages\")\n```\n\n2. Set up your transmitter and receiver. In the provided context, this means initializing the `PAM` and `PAMDemapper` classes. Also, ensure that you set `return_indices=True` when initializing the `PAMDemapper`. This allows you to retrieve the indices of the bits that were mapped to a symbol.\n\n```python\npam = PAM(num_bits_per_symbol, constant_scaling=True)\ndemap = PAMDemapper(pam, return_indices=True)\n```\n\n3. Generate a batch of random bits and map them to symbols using the transmitter.\n\n```python\nbits = tf.random.uniform((batch_size, num_tx_ant, num_bits_per_symbol), 0, 2, tf.int32)\nx = pam(bits)\n```\n\n4. Pass the modulated symbols through a channel. In the context provided, this is done using the `AWGN` class.\n\n```python\nchannel = AWGN()\nno = tf.sqrt(sn.utils.ebnodb2no(ebno_db, num_bits_per_symbol, coderate))\ny = channel([x, no])\n```\n\n5. Demap the received symbols to get soft-symbols, and then quantize the soft-symbols to get hard decisions.\n\n```python\ns_hat, _ = demap([y, no])\ns_hat_hard = tf.argmax(s_hat, axis=-1, output_type=tf.int32)\n```\n\n6. Compute the MSE using the formula provided in the context. This involves subtracting the transmitted symbols from the recovered symbols, squaring the result, and taking the mean.\n\n```python\nmse = tf.reduce_mean(tf.square(s - s_hat)))\n```\n\nRemember to replace `num_bits_per_symbol` and `ebno_db` with the actual number of bits per symbol and the actual Eb/N0 value in dB, respectively."
"The `create_or_check_constellation` method in the `Constellation` class is used to generate a QAM constellation or to check the integrity of an existing one. A constellation is a representation of signal points in the in-phase and quadrature plane. In the context of this method, a constellation point refers to a single point in this plane, defined by its real and imaginary components.\n\nThis method first checks if a constellation is computed and stored already. If it is, this existing constellation is returned. Otherwise, a new constellation is created. The number of constellation points is inferred from the dtype of the underlying bits variable. If the dtype is complex, the bits variable is used to create the constellation. If the create_or_check_constellation method is called with a bits variable representing a higher-dimensional constellation, an error will be raised.\n\nIf a constellation is created, it is based on the mod_levels parameter, which represents the constellation's modulation levels. The const_size parameter specifies the size of the constellation, i.e., the number of signal points. The dtype parameter indicates the desired data type of the constellation and defaults to tf.complex64.\n\nTo summarize, `create_or_check_constellation` is a single entry point to ensure that a valid constellation is returned. If a constellation is created, it is based on the modulation levels and constellation size inferred from the bits variable. The constellation is also ensured to have the correct dtype and raise an error if it does not match the expected type."
"The `scat_keep_prob` argument in the ray tracing process is a predefined probability value that models the scattering of rays when they encounter certain objects in the scene. When a ray hits an object, part of it can be scattered while the other part continues on its original path. This is typically seen in scenarios where the ray hits a certain type of wall or other obstructive object.\n\nThe `scat_keep_prob` is used in the computation of the scattering coefficient, which determines the amount of scattering that occurs. The scattering coefficient is multiplied by the scattering loss when a scattering event occurs, and by the scattering gain when a scattering event does not occur. Therefore, the `scat_keep_prob` can be used to adjust the level of scattering in the scene, which can significantly impact the final results of the ray tracing process.\n\nHere is an example of how `scat_keep_prob` is used in the code:\n\n```python\n# Defining a Scattering equal to the RadioMaterial scattering coefficient\n# with possibility to not take into account the scattering after a given number of interactions\nscattering = np.stack((np.zeros([1, num_rx]), scattering_coefficient*tf.ones([1, num_rx])), -scattering_coefficient*tf.ones([1, num_rx])), axis=-1)\nscat_keep_prob = tf.stack((tf.ones([1, num_rx//2]), 0.*tf.ones([1, num_rx//2])), axis=1)\nscat_keep_prob = tf.tile(scat_keep_prob, [2, 1])\nscat_keep_prob = tf.transpose(scat_keep_prob, [1,0])\nscat_keep_prob = tf.squeeze(scat_keep_prob, axis=0)\nscattering = tf.multiply(scattering, scat_keep_prob)\nscattering = tf.multiply(scattering, tf.cast(wall, tf.float32))\nscattering = tf.reduce_sum(scattering, axis=0)\nscattering = tf.expand_dims(scattering, axis=-1)\nscattering = tf.tile(scattering, [1, 1, num_phi])\n```\n\nIn this code, `scat_keep_prob` is set to a combination of zeros and the scattering coefficient. This means that some rays will scatter when they hit an object, while others will continue on their original path. The `scat_keep_prob` is multiplied by the scattering coefficient to compute the final scattering parameter, which is used in the ray tracing calculations."
"Here is the detailed information of sionna.rt.r_hat: [sionna.rt.r_hat(r, theta, phi)](https://nvlabs.github.io/sionna/_modules/sionna/rt/utils.html#r_hat)\n\nComputes the spherical unit vector r_hat from planar norm r.\n\n**Input**\n\n- `r` ([...,3], tf.float): Scaled planar normal vector, that is, the norm of the last dimension of `r` is one.\n- `theta` ([...,1], tf.float): Zenith angles. `theta` must be of the same shape as the first dimensions of `r`.\n- `phi` ([...,1], tf.float): Azimuth angles. `phi` must be of the same shape as the first dimensions of `r`.\n\n**Output**\n\n- `[...,3]`, tf.float: Spherical unit vectors r_hat.\n\nsource code:\n```python\ndef r_hat(r, theta, phi):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    Computes the spherical unit vector :math:`\\hat{\\mathbf{r}}` from planar norm :math:`\\mathbf{r}`.\n\n    Input\n    -----\n    r : [...,3], tf.float\n        Scaled planar normal vector, that is, the norm of the last\n        dimension of ``r`` is one.\n\n    theta : [...,1], tf.float\n        Zenith angles. ``theta`` must be of the same shape as the first\n        dimensions of ``r``.\n\n    phi : [...,1], tf.float\n        Azimuth angles. ``phi`` must be of the same shape as the first\n        dimensions of ``r``.\n\n    Output\n    ------\n    : [...,3], tf.float\n        Spherical unit vectors :math:`\\hat{\\mathbf{r}}`\n    \"\"\"\n\n    x = tf.cos(theta)*tf.cos(phi)\n    y = tf.cos(theta)*tf.sin(phi)\n    z = -tf.sin(theta)\n    r_hat = tf.stack([x,y,z], -1)\n    return r_hat\n```"
"Here is the detailed information of OFDMDetectorWithPrior:   \n  \n[sionna.ofdm.OFDMDetectorWithPrior(detector, output, resource_grid, stream_management, rx_sm, tx_sm, constellation_type, num_bits_per_symbol, constellation, dtype=tf.complex64, **kwargs)](https://jsonblob.com/6dd57dd0-4041-11ec-b3cb-3fbe825d37a4)  \n\nLayer that wraps a MIMO detector and a prior input, assuming the OFDM waveform.\n\nThis class defines a layer that preprocesses its input, which is a resource grid of OFDM symbols together with prior information for every single stream. The layer preprocesses the input to convert it to a format that is suitable for the provided detector. Both the input OFDM resource grid and the prior information tensor, which contains LLRs for every bit of every stream, are adjusted according to the stream management configuration. The underlying detector is a callable (e.g., a function) that takes an observation tensor and returns estimated symbols and effective noise variance for every stream.\n\n**Parameters**\n\n- `detector` (Callable): The MIMO detector to be used. This callable must take an observation tensor and return estimated symbols and effective noise variance.\n- `output` (str): Type of output, either \"bit\" or \"symbol\". The layer configuration will be inferred from the `resource_grid` and `stream_management` parameters. \"bit\" indicates that the output is a tensor of bits, while \"symbol\" indicates that the output is a tensor of constellation symbols.\n- `resource_grid` (ResourceGrid): Instance of ResourceGrid.\n- `stream_management` (StreamManagement): Instance of StreamManagement.\n- `rx_sm` (StreamManagement): Reference to the receiver's StreamManagement instance. This parameter is required only if ``stream_management`` indicates that `rx_sm` is used.\n- `tx_sm` (StreamManagement): Reference to the transmitter's StreamManagement instance. This parameter is required only if ``stream_management`` indicates that `tx_sm` is used.\n- `constellation_type` (str): Type of constellation, either \"qam\", \"pam\", or \"custom\". For \"custom\", an instance of Constellation must be provided.\n- `num_bits_per_symbol` (int): Number of bits per constellation symbol, e.g., 4 for QAM16. Required for \"qam\" or \"pam\" constellations.\n- `constellation` (Constellation): Instance of Constellation or None. If None, `constellation_type` and `num_bits_per_symbol` must be provided.\n- `dtype` (tf.DType): Data type of `y`. Defaults to tf.complex64. The output data type is the corresponding real type (tf.float32 or tf.float64).\n\n**Input**\n\n- `(y, prior, h_hat, err_var, no)` \u2013 Tuple:\n  - `y` ([batch_size, num_rx, num_rx_ant, num_ofdm_symbols, fft_size], tf.complex): Observed resource grid.\n  - `prior` ([batch_size, num_streams_per_rx, num_bit_streams_per_rx, num_ofdm_symbols, data_symbol_indices], tf.float): Prior for every bit of every stream, provided per stream.\n  - `h_hat` ([batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols, fft_size], tf.complex): Channel estimates across all streams and transmitters.\n  - `err_var` ([Broadcastable to shape of h_hat], tf.float): Variance of the first term of the linear detector.\n  - `no` ([batch_size, num_rx, num_rx_ant] or only the first n dimensions, tf.float): Variance of the AWGN.\n\n**Output**\n\nDepending on the `output` parameter:\n- If `output` equals \"bit\":\n  - [batch_size, num_tx, num_streams, num_data_symbols * num_bits_per_symbol], tf.float: LLRs or hard-decisions for every bit of every stream.\n- If `output` equals \"symbol\":\n  - [batch_size, num_tx, num_streams, num_data_symbols, num_points], tf.float or [batch_size, num_tx, num_streams, num_data_symbols], tf.int: Logits or hard-decisions for constellation symbols for every stream. Hard-decisions correspond to the symbol indices.\n\n**Note:** If you want to use this layer with the Graph mode XLA, i.e., within a function that is decorated with @tf.function(jit_compile=True), you must set sionna.Config.xla_compat=true. See xla_compat."
"Here is the detailed information of GaussianPriorSource: sionna.ofdm.GaussianPriorSource(prior, output, dtype=tf.complex64, **kwargs)\n  \nThis layer defines a Gaussian prior source for the given output dimension output and dtype. The prior is represented by a tf.stack([mean, var]) tensor of dimension [2, num_tx, num_streams_per_tx, num_ofdm_symbols, fft_size], which defines the prior distribution \n\noutput \u2013 Output dimension, e.g., [batch_size, num_tx, num_ant, num_ofdm_symbols, fft_size], to which the Gaussian prior is broadcast. \n-  type: tf.DType, optional\n-  The dtype of `input` and the output dtype. Defaults to tf.complex64.  \n  \n**Source code link:** [GaussianPriorSource](https://nvlabs.github.io/sionna/_modules/sionna/channel/channel_model.html#GaussianPriorSource)\n\n```python\nclass GaussianPriorSource(PriorSource):\n    # pylint: disable=line-too-long\n    r\"\"\"GaussianPriorSource(prior, output, dtype=tf.complex64, **kwargs)\n\n    Layer that generates a Gaussian prior of the input.\n\n    The prior is represented by a ``tf.stack([mean, var])`` tensor of\n    dimension ``[2, num_tx, num_streams_per_tx, num_ofdm_symbols, fft_size]``,\n    which defines the prior distribution\n\n    .. math::\n        \\mathcal{N}\\left ( mean, var \\right )\n\n    of the input, for each resource element (RE) and stream.\n\n    This layer is initialized as a callable. Example on how to use it\n    follows.\n\n    Parameters\n    ----------\n    prior : Tensor of shape [2, num_tx, num_streams_per_tx, num_ofdm_symbols, fft_size]\n        Tensor defining the prior distribution of the input.\n        The prior distribution is given as a normal distribution with\n        variance ``var`` and mean ``mean``.\n\n    output : Output dtype\n        The dtype of the output.\n\n    dtype : tf.Complex64, tf.Complex128\n        The dtype of the prior distributions.\n\n    Input\n    -----\n    batch_size : int\n        The batch size. Defaults to `None` for the base class `PriorSource`.\n\n    num_tx : int\n        Number of transmitters.\n\n    num_streams_per_tx : int\n        Number of streams per transmitter.\n\n    ouf_ofdm_symbols : int\n        Number of OFDM symbols per event.\n\n    fft_size : int\n        Number of channels resource elements.\n\n    Output\n    ------\n    : [batch size, num_tx, num_streams_per_tx, num_ofdm_symbols, fft_size], `output.dtype`\n        Tensor of draws from the prior distribution.\n    \"\"\"\n    def __init__(self,\n                 prior,\n                 output,\n                 dtype=tf.complex64,\n                 **kwargs\n                ):\n        super().__init__(prior=prior,\n                         output=output,\n                         dtype=dtype,\n                         **kwargs)\n```"
"Here is the detailed information of [b4ss.channel.mapping.QAM2PAM](https://nvlabs.github.io/sionna/_modules/sionna/channel/mapping.html#QAM2PAM):  \n  \nTransforms QAM symbol indices to PAM symbol indices.\n\nFor a given QAM symbol index $s$ and corresponding demapping results $(r_1, i_1), (r_2, i_2)$, the corresponding PAM symbol indices (denoted as $p_1, p_2$) and the constellation rotation index $p_\\text{rot}$ are computed according to [Section 5.1, (5.1) and (5.2)](https://nvlabs.github.io/sionna/fundamentals.html#qam-pam).  \n  \n**Parameters**\n\n- `num_pam_symbols` (int): Number of PAM symbols. This must be a power of two. For example, for 4QAM, this value would be 2.\n- `num_coded_bits` (int): Number of bits per QAM symbol, e.g., 4 for 4QAM.\n- `num_pam_copies` (int): Number of PAM symbol copies per QAM constellation symbol. This value is equal to the number of layers in MIMO transmission.\n- `mask` ([num_pam_copies, num_coded_bits], [num_pam_copies, num_coded_bits]): Mask for demapping the bit index to the PAM symbol index. The first dimension is for the real and imaginary part, the second dimension is for each bit of the constellation symbol. If this argument is provided, the [3GPP II channel model](https://nvlabs.github.io/sionna/channel/channel_model.html#3gpp-II) for MIMO simulations is automatically enabled and the 3rd dimension is interpreted as the layer index.\n  \n**Input**\n\n- `qam_indices` ([batch_size, num_symbols], tf.int): Tensor of QAM symbol indices.\n\n**Output**\n\n- `pam_indices` ([batch_size, num_symbols * num_pam_copies], tf.int): Tensor of PAM symbol indices.\n\n- `pam_rotations` ([batch_size, num_symbols * num_pam_copies], tf.int or None): Tensor of the constellation rotations. If mask is not None, the rotation index is returned as a separate tensor. Possible values are 0 and 1. `mask` must be a 3-dimensional tensor of shape `[num_pam_copies, num_coded_bits, num_layers]`.  \n  \n**Note:** If you want to use this layer in Graph mode with XLA, i.e., within a function that is decorated with @tf.function(jit_compile=True), you must set sionna.Config.xla_compat=true. See [xla_compat](https://nvlabs.github.io/sionna/api.html#sionna.Config.xla_compat).  \n\nsource code:\n```python\nclass QAM2PAM(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"QAM2PAM()\n\n    Layer that transforms QAM symbol indices to PAM symbol indices.\n\n    For a given QAM symbol index :math:`s` and corresponding demapping results\n    :math:`(r_1, i_1), (r_2, i_2)`, the corresponding PAM symbol indices\n    (denoted as :math:`p_1, p_2`) and the constellation rotation index\n    :math:`p_\\text{rot}` are computed according to\n    [WangVorobyov2017]_:\n\n    .. math::\n        \\begin{align}\n            p_1 &= \\left\\lfloor\\frac{s}{4}\\right\\rfloor \\\\\n            p_2 &= \\text{mod}\\left(\\frac{s}{2}, 4\\right) = 2 + \\text{mod}\\left(p_1, 2) + 2 \\cdot \\text{mod}\\left(\\frac{s}{4},2)\\\\\n            p_\\text{rot} &= \\text{mod}\\left(2 \\cdot \\text{mod}\\left(\\frac{s}{4},2) + 1 \\cdot \\text{mod}\\left(\\frac{s}{2}, 2) + 1,\\,4\\right)\n        \\end{align}\n\n    where :math:`\\lfloor x \\rfloor` is the largest integer less than or equal to :math:`x`,\n    and :math:`\\text{mod}` is the modulo operator. It is assumed that :math:`s` is the\n    symbol index of a 16QAM constellation, i.e.,\n\n    .. math:: s = s_1 + j s_2,\\quad s_1,s_2 \\in \\{ \\pm 3, \\pm 1, \\pm 1, \\pm 3 \\}`\n\n    where the PAM constellation is\n\n    .. math::\n        \\begin{align}\n            \\mathcal{C} &= \\left\\{  (2a+1) + j (2b+1) \\in \\mathbb{C} \\, \\big| \\, a, b \\in \\{0,1,2,3\\} \\right\\} \\\\\n                       &= \\left\\{ -3+3j,\\, -3+1j,\\, -1+3j,\\, -1+1j,\\, 1-1j,\\, 1-3j,\\, 3-1j,\\, 3-3j \\right\\}\n        \\end{align}\n\n    and :math:`\\mathbb{C}` is the set of all complex numbers.\n\n    Parameters\n    ----------\n    num_pam_symbols : int\n        Number of PAM symbols.\n        It must be a power of two, e.g., 2, 4, 8, 16.\n\n    num_coded_bits : int\n        Number of bits per QAM symbol, e.g., 4 for 16QAM.\n\n    num_pam_copies : int\n        Number of PAM symbol copies per QAM constellation symbol.\n        This parameter is only needed when `mask` is provided.\n\n    mask : [, 2*num_coded_bits], None\n        Mask for demapping the bit index to the PAM symbol index.\n        The first dimension is for the real and imaginary components, the\n        second dimension is for each bit of the constellation symbol.\n        If no mask is provided, it is assumed that the modulator is 16QAM.\n        An example for a valid mask for 16QAM is\n\n        .. code::\n\n            mask = [[0,0],\n                    [0,1],\n                    [1,1],\n                    [1,0],\n                    [0,0],\n                    [0,1],\n                    [1,1],\n                    [1,0]]\n\n    Input\n    -----\n    qam_indices : [batch_size, ?], tf.int\n        Tensor of QAM symbol indices.\n\n    Output\n    ------\n    pam_indices : [batch_size, ?*num_pam_copies], tf.int\n        Tensor of PAM symbol indices.\n\n    pam_rotations : [batch_size, ?*num_pam_copies], or None, tf.int\n        Tensor of the constellation rotations. It is `None` if no\n        `mask` is given. Possible values are 0 and 1. 0 means no\n        rotation, 1 means a rotation of 90 degrees.\n        It is only returned if `mask` is provided.\n    \"\"\"\n    def __init__(self,\n                 num_pam_symbols,\n                 num_coded_bits,\n                 num_pam_copies=None,\n                 mask=None):\n        super().__init__()\n        assert num_pam_symbols in [2,4,8,16],\\\n            \"The supported `num_pam_symbols` are [2,4,8,16].\"\n        assert num_coded_bits in [4,6,8], \\\n            \"The supported `num_coded_bits` are [4,6,8].\"\n        if num_pam_copies is not None:\n            assert num_pam_copies in [1,2,4,8],\\\n                \"For constellation mask, `num_pam_copies` must be\"\\\n                \"1,2,4,8.\"\n        inputs = [num_pam_symbols, num_coded_bits, num_pam_copies, mask]\n        for idx,arg in enumerate(inputs):\n            if arg is not None:\n                inputs[idx] = int(arg)\n\n        self._check_input(mask)\n\n        self._num_pam_symbols = num_pam_symbols\n        self._num_coded_bits = num_coded_bits\n        self._num_pam_copies = 1\n        if num_pam_copies is not None:\n            self._num_pam_copies = num_pam_copies\n        self._mask = mask\n\n        pam_symbols = self._pam_constellation(self._num_pam_symbols,\n                                              self._num_coded_bits,\n                                              self._num_pam_copies,\n                                              self._mask)\n        self._pam_symbols = tf.constant(pam_symbols, tf.float32)\n        self._rotations = tf.constant([0,90], tf.float32)\n        if self._mask is not None:\n            self._pam_rotations = tf.constant(\n                self._pam_constellation(2, self._num_coded_bits, 2),\n                dtype=tf.float32)\n        else:\n            self._pam_rotations = None\n\n    def _check_input(self, mask):\n        if mask is not None:\n            assert (self._num_coded_bits%2 == 0) and \\\n                   (self._num_pam_symbols==2),\\\n                \"mask for constellation must be binary and for 2-PAM.\"\n\n    def _pam_constellation(self, num_pam_symbols, num_coded_bits,\n                           num_copies, mask):\n        assert num_pam_symbols in [2,4,8,16],\\\n            \"The supported `num_pam_symbols` are [2,4,8,16].\"\n        assert num_coded_bits in [4,6,8], \\\n            \"The supported `num_coded_bits` are [4,6,8].\"\n        assert num_copies in [1,2,4,8],\\\n            \"The supported `num_copies` are [1,2,4,8].\"\n\n        points = self._gray(num_pam_symbols, num_coded_bits)\n        points = np.where(points, -1, points*2-1)\n        constellation = np.zeros([num_copies, num_pam_symbols], np.float32)\n        for copy in range(num_copies):\n            for bit in range(num_coded_bits):\n                symbol = np.copy(constellation[copy])\n                for s,idx in enumerate(symbol):\n                    symbol[s] = symbol[s] + points[idx,bit]\n                constellation[copy] = symbol\n        constellation = np.reshape(constellation, [-1])\n\n        if mask is not None:\n            constellation = np.dot(constellation, mask)\n            constellation = np.split(constellation, 2)\n            constellation = np.stack(constellation, axis=0)\n            constellation = np.reshape(constellation, [-1, num_copies*2])\n        return constellation\n\n    def _gray(self, n, num_bits):\n        \"\"\"\n            Produce flat array of Gray symbols. The symbols are not\n            elevated to 2**i. Only the i-th bit is turned into Gray code.\n\n            Arguments\n            ---------\n            n : int\n                Number of symbols.\n\n            num_bits : int\n                Number of bits per symbol.\n\n            Output\n            -------\n            symbols : :obj:`np.ndarray` of int\n                Array of Gray encoded symbols.\n        \"\"\"\n        assert n==2,\"Only works for QAM.\"\n        symbols = np.arange(2**num_bits)\n        codewords = np.zeros([2**num_bits, num_bits])\n        for i in range(num_bits-1,-1,-1):\n            shift = symbols >> i\n            shift = shift ^ (shift>>1)\n            codewords[:,i] = shift\n        symbols = np.argsort(codewords, axis=0)\n        return symbols\n\n    def call(self, inputs):\n        x = tf.one_hot(inputs, self._num_pam_symbols)\n        x = tf.tile(x, [1, self._num_pam_copies])\n        x = tf.reshape(x, [-1, self._num_pam_symbols*self._num_pam_copies])\n        if self._mask is not None:\n            x = tf.gather(x, self._pam_symbols, axis=-1)\n            rot = tf.gather(self._pam_rotations, self._pam_symbols//2)\n            rot = tf.where(x, rot, 0)\n            x = x + rot\n        return x\n\n    @property\n    def pam_symbols(self):\n        return self._pam_symbols\n\n    @property\n    def pam_rotations(self):\n        return self._rotations\n\ndef PAM2QAM(indices, num_copies):\n    # pylint: disable=line-too-long\n    r\"\"\"PAM2QAM(indices, num_copies)\n\n    Transform PAM symbol indices to QAM symbol indices.\n\n    For a given PAM symbol index :math:`p` and corresponding modulation signals\n    :math:`(x_p, y_p)`, the corresponding QAM symbol index and modulation signals\n    :math:`(x_q, y_q)`` are computed according to [WangVorobyov2017]_:\n\n    .. math::\n        \\begin{align}\n            q &= 2p + \\mu_y\\\\\n            \\bar{q} &= 2(p+1) + \\mu_x\n        \\end{align}\n\n    where :math:`\\mu_x` and :math:`\\mu_y` are the PAM symbol dimensions and :math:`\\bar{q}``\n    is the QAM symbol index that results from the interleaving of :math:`x_p` and :math:`y_p`:\n\n    .. math::\n        \\bar{q} = N_x \\cdot y_p + x_p + N_y \\cdot x_p^{\\mu_x-1}\n\n    The modulation signals are computed through\n\n    .. math::\n        \\begin{align}\n            x_q &= \\begin{cases}\n                    x_p + 0.5, & \\text{if } q \\leq N_q/2\\\\\n                    x_p + 1.5, & \\text{otherwise}\n                  \\end{cases}\\\\\n            y_q &= y_p + 0.5\n        \\end{align}\n\n    where :math:`N_x` and :math:`N_y` are the number of QAM symbols in the\n    :math:`x` and :math:`y` constellation dimensions, respectively.\n\n    Input\n    -----\n    indices : [batch_size, num_symbols], tf.int\n        Tensor of PAM symbol indices.\n\n    num_copies : int\n        Number of PAM symbol copies.\n        This parameter is only required if ``indices`` has rank one.\n        It defaults to 1 and must not be larger than the number of Quam symbols.\n\n    Output\n    ------\n    qam_indices : [batch_size, num_symbols*num_copies], tf.int\n        Tensor of QAM symbol indices.\n\n    qam_x : [batch_size, num_symbols*num_copies], tf.float\n        Modulation signal values corresponding to the x-dimension.\n\n    qam_y : [batch_size, num_symbols*num_copies], tf.float\n        Modulation signal values corresponding to the y-dimension.\n    \"\"\"\n    ind_shape = tf.shape(indices)\n    assert len(ind_shape) in [1,2], \"Indices must have at most rank two.\"\n    assert indices.dtype in [tf.int32, tf.int64],\\\n        \"Indices must be of integer type.\"\n\n    if tf.rank(indices)==1:\n        assert num_copies in [1,2], \\\n            \"`num_copies` must be 1 or 2, if indices is a vector.\"\n\n    max_pam_symbol = tf.cast(2**11, indices.dtype)\n    max_qam_symbol = tf.cast(2**11, indices.dtype)\n\n    # expand input to the desired rank\n    if tf.rank(indices)==1:\n        indices = tf.expand_dims(indices, -1)\n\n    # split symbol indices into two components\n    qam_ind_x = indices // max_pam_symbol\n    qam_ind_y = qam_ind_x + 1\n    qam_ind_x = tf.math.mod(qam_ind_x, max_qam_symbol)\n    qam_ind_y = tf.math.mod(qam_ind_y, max_qam_symbol)\n\n    # interleave components to one index\n    qam_ind = 2*qam_ind_x + qam_ind_y\n    qam_ind = tf.expand_dims(qam_ind, -1)\n\n    # compute QAM symbol values\n    if tf.math.mod(qam_ind, 2) < 0.5:\n        qam_x = qam_ind - 0.5\n    else:\n        qam_x = qam_ind - 1.5\n    qam_x = tf.expand_dims(qam_x, -1)\n    qam_x = tf.reshape(qam_x, ind_shape)\n\n    qam_y = tf.constant(0.5, qam_x.dtype)\n    qam_y = tf.expand_dims(qam_y, -1)\n    qam_y = tf.reshape(qam_y, ind_shape)\n\n    return qam_ind, qam_x, qam_y\n```  \n  "
"To load the required Sionna components, you can follow these steps:\n\n1. Import the `os` module. This module provides a way of interacting with the operating system.\n```python\nimport os\n```\n\n2. Set the environment variables for CUDA and TensorFlow. You can do this by assigning the `CUDA_VISIBLE_DEVICES` and `TF_CPP_MIN_LOG_LEVEL` variables to `0` and `3` respectively.\n```python\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use \"\" to use the CPU\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n```\n\n3. Import the Sionna package. If the package is not already installed, install it using pip.\n```python\ntry:\n    import sionna\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    os.system(\"pip install sionna\")\n    import sionna\n```\n\n4. Load the required Sionna components. This includes `AWGN` from `sionna.channel`, `BinarySource`, `ebnodb2no`, `log10`, `expand_to_rank`, `insert_dims` from `sionna.utils`, `LDPC5GEncoder` and `LDPC5GDecoder` from `sionna.fec.ldpc.encoding` and `sionna.fec.ldpc.decoding` respectively, and `Mapper`, `Demapper`, `Constellation` from `sionna.mapping`.\n```python\nfrom sionna.channel import AWGN\nfrom sionna.utils import BinarySource, ebnodb2no, log10, expand_to_rank, insert_dims\nfrom sionna.fec.ldpc.encoding import LDPC5GEncoder\nfrom sionna.fec.ldpc.decoding import LDPC5GDecoder\nfrom sionna.mapping import Mapper, Demapper, Constellation\n```\n\n5. Configure the GPU settings if necessary. This includes setting the GPU memory growth and defining a specific GPU to be used.\n```python\ngpu_num = 0\nif gpu_num>=0:\n    import tensorflow as tf\n    gpus = tf.config.list_physical_devices('GPU')\n    if gpus:\n        try:\n            tf.config.experimental.set_memory_growth(gpus[gpu_num], 4000)\n        except RuntimeError as e:\n            print(e)\n    tf.config.set_soft_device_placement(True)\n```\n\n6. Import the necessary TensorFlow libraries.\n```python\nimport tensorflow as tf\n```\n\nPlease note that the exact steps and code may vary depending on your specific requirements and the environment you are working in."
"To import the necessary modules for Sionna's API and set up the GPU configuration, you can follow these steps:\n\n1. Import the `sionna` library. If it's not already installed, you can install it using pip.\n\n```python\ntry:\n    import sionna\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna\n```\n\n2. Import the necessary modules for Sionna's API. This includes the `Model`, `TFBeschdelator`, `from_callback`, `Dtype`, `expand_to_rank`, `r2poly`, `r2poly_inv`, `QAMSource`, `BinarySource`, `split_in_blocks`, and `OSI`.\n\n```python\nfrom sionna import Model\nfrom sionna.loss import BinaryCrossentropy, BinarySource\nfrom sionna.utils import expand_to_rank, insert_dims, OSIRISChannel, split_in_blocks\nfrom sionna.rt import r2poly, r2poly_inv\nfrom sionna.mimo import StreamManagement\nfrom sionna.ofdm import ResourceGrid, ResourceGridMapper, LSChannelEstimator, LMMSEEqualizer\nfrom sionna.ofdm import OFDMModulator, OFDMDemodulator, ZFPrecoder, RemoveNulledSubcarriers\nfrom sionna.channel.tr38901 import Antenna, AntennaArray, CDL, UMi, UMa, RMa\nfrom sionna.channel import gen_single_sector_topology as gen_topology\nfrom sionna.channel import subcarrier_frequencies, cir_to_ofdm_channel, cir_to_time_channel\nfrom sionna.channel import ApplyOFDMChannel, ApplyTimeChannel, OFDMChannel\nfrom sionna.fec.ldpc.encoding import LDPC5GEncoder\nfrom sionna.fec.ldpc.decoding import LDPC5GDecoder\nfrom sionna.mapping import Mapper, Demapper\nfrom sionna.utils import insert_dims\nfrom sionna.utils.tensorboard import TensorBoardCallback\nfrom sionna.utils.metrics import accuracy\n```\n\n3. Set up the GPU configuration. In this example, we set the notebook to use only a single GPU and allocate only as much memory as needed.\n\n```python\nimport tensorflow asjsonn as tf\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\n# Avoid warnings from TensorFlow\ntf.get_logger().setLevel('ERROR')\n```\n\nPlease note that the actual modules used may vary depending on the specific tasks and requirements of your project."
"The 'CoverageMap' component of ray tracing in Sionna is used for analyzing the link quality between a base station and a user terminal or a base station and an external antenna. It produces a map that associates each point on a provided scene geometry with the set of propagating paths that could reach that point.\n\nThe 'CoverageMap' uses methods from the 'rt' module of Sionna for ray tracing. It computes the paths that propagate from the transmitters to the receivers or from the transmitters to the external antennas. For each computed propagation path, a Coverage Map is generated. A Coverage Map associates each point on the scene geometry with the set of propagation paths that could reach that point. Note that a CoverageMap is not uniquely defined as the path computation and the propagation paths are generated randomly.\n\nThe 'CoverageMap' can be used in the 'Scene' for path tracing. You can compute coverage maps for desired transmitters and receivers or external antennas, and then use the paths included in the coverage map for tracing paths through the scene.\n\nHere is an example of how to use the 'CoverageMap' from the Sionna documentation:\n\n```python\nscene = load_scene(sionna.rt.scene.munich)\nscene.preview()\ncm = CoverageMap(scene, frequency=scene.frequency)\ncm.compute(num_samples=10_000_000,\n            sample_depth=50,\n            trace_paths=True)\ncm.show()\n```\n\nIn the example, 'scene' is an instance of the 'Scene' class, and 'sionna.rt.scene.munich' is a scene included in the Sionna package. The 'compute' method is then used on the 'CoverageMap' to calculate the coverage map.\n\nNote: The ability to preview a scene is not available in this text context."
"To decode a set of log-likelihood ratio (LLR) values using the TurboDecoder in Sionna, you need to provide the `llr` tensor containing the LLR values and the `ebno_db` parameter, which is the simulated signal-to-noise ratio (SNR) in dB. \n\nHere are the input requirements for decoding:\n\n- `logits`: A tensor of type `tf.float32` containing the logits for the incoming convolutional or systematic data.\n- `n` and `k`: The codeword length and the number of information bits per codeword, respectively.\n- `is_bcjr`: A boolean that, if assserted, will make `backward_algorithm` compatible with the `BackwardAlgorithm` from the `tensorflow_model` package. It defaults to `False`.\n- `ebno_db`: The simulated SNR in dB for the incoming LLR values.\n\nAnd here are the output formats for the decoding function:\n\n- For soft output decoding, the function returns a tensor of estimated LLRs for the transmitted coded bits. The shape of the output tensor is `[num_codewords, k]`, where `num_codewords` is the number of input codewords, and `k` is the number of information bits per codeword.\n- For hard output decoding, the function returns a tensor of hard-decided bits for each codeword. The shape of the output tensor is `[num_codewords, n]`, where `n` is the codeword length.\n\nPlease note that the TurboDecoder in Sionna requires the Turbo code components (TurboEncoder and the underlying rate-1/2 convolutional encoders) to be instantiated and provided as input. The provided code does not instantiate these components, so you need to do that before calling the `TurboDecoder`. Also, the channel output LLRs must be computed prior to this step. The `GaussianPriorSource` could be used for this purpose."
"Here is the detailed information of sionna.rt.antenna.iso_pattern:  [sionna.rt.antenna.iso_pattern(theta, phi, slantAngles=(0.0, 0.0), polarizationModel=2, dtype=tf.complex64)](https://njsonvlvl.github.io/sionna/_modules/sionna/rt/antenna.html#iso_pattern)\n\nIsotropic radiation pattern with linear polarization\n\nFor linear polarization, the polarization direction is always the same, i.e., R\u0302(\u03b8, \u03c6) = R\u0302(\u03b8, \u03c6) = jsonvlvl.github.io[3D_Polarization, polarization_model_3D](0, 0, 0, 0, 1, 0)\n\n**Input**\n\n- `theta` (array_like, float): Zenith angles wrapped within [0, \u03c0] [rad].\n- `phi` (array_like, float): Azimuth angles wrapped within [-\u03c0, \u03c0) [rad].\n- `slantAngles` (tuple, float): Slant angle of the linear polarization [rad]. A value of 0 indicates linear polarization with a vertical polarization vector. A value of \u03c0/2 indicates linear polarization with a horizontal polarization vector. Only used with polarization_model=2. Defaults to (0,0).\n- `polarizationModel` (int, one of [1,2], defaults to 2): Polarization model to be used. Options 1 and 2 refer to 2D and 3D polarization, respectively. See jsonvlvl.github.io[3D_Polarization, polarization_model_3D] for details.\n- `dtype` (tf.complex64 or tf.complex128, tf.Dtype): Datatype to be used. Defaults to tf.complex64.\n\n**Output**\n\n- `E_theta` (theta.shape, dtype): Zenith pattern.\n- `E_phi` (theta.shape, dtype): Azimuth pattern.\n\nsource code:\n```python\ndef iso_pattern(theta, phi, slantAngles=(0.0,0.0),\n                polarizationModel=2, dtype=tf.complex64):\n    r\"\"\"\n    Isotropic radiation pattern with linear polarization\n\n    For linear polarization, the polarization direction is always the same,\n    i.e., :math:`\\mathbf{\\hat{R}}(\\theta, \\phi) =\n    \\mathbf{\\hat{R}}(\\theta, \\phi) =\n    \\mathbf{e}_\\theta(\\theta, \\phi, 0)`\n\n    Input\n    -----\n    theta : array_like, float\n        Zenith angles wrapped within [0,pi] [rad]\n\n    phi : array_like, float\n        Azimuth angles wrapped within [-pi, pi) [rad]\n\n    slantAngles : tuple, float\n        Slant angle of the linear polarization [rad]. A value of 0\n        indicates linear polarization with a vertical polarization vector.\n        A value of :math:`\\pi/2` indicates linear polarization with a\n        horizontal polarization vector. Only used with ``polarization_model``=2.\n        Defaults to (0,0).\n\n    polarizationModel : int, one of [1,2], defaults to 2\n        Polarization model to be used. Options `1` and `2` refer to\n        2D and 3D polarization, respectively. See\n        :py:func:`~sionna.rt.antenna._2D_polarization_pattern` and\n        :py:func:`~sionna.rt.antenna._3D_polarization_pattern` for details.\n\n    dtype : tf.complex64 or tf.complex128, tf.Dtype\n        Datatype to be used.\n        Defaults to `tf.complex64`.\n\n    Output\n    ------\n    E_theta : theta.shape, dtype\n        Zenith pattern\n\n    E_phi : theta.shape, dtype\n        Azimuth pattern\n    \"\"\"\n    if polarizationModel==1:\n        return _2D_polarization_pattern(theta, phi, dtype, *slantAngles)\n    elif polarizationModel==2:\n        return _3D_polarization_pattern(theta, phi, dtype, *slantAngles)\n```"
"Since there is no simulation or example provided in the context to demonstrate the usage of the \"ApplyTimeChannel\" and \"ApplyOFDMChannel\" Keras layers, we can only guide you on how to use them based on thejson snippets provided in the context.\n\nThe `ApplyTimeChannel` and `ApplyOFDMChannel` Keras layers are defined as follows:\n\n```python\nApplyTimeChannel(num_time_samples, l_tot, add_awgn=True, dtype=tf.complex64, **kwargs)\n```\n\n```python\nApplyOFDMChannel(channel_response, channel_mean, dtype=tf.complex64, **kwargs)\n```\n\nUsing the `ApplyTimeChannel` layer:\n\n1. Import the required classes:\n```python\nfrom sionna.channel import ApplyTimeChannel\n```\n2. Create an instance of the class with the desired parameters:\n```python\natc = ApplyTimeChannel(num_time_samples=10, l_tot=5, add_awgn=True)\n```\n3. Run the layer by calling it with the input parameters:\n```python\noutput = atc((no, h_final))\n```\n   \nWhere `no` is the noise variance, and `h_final` is the channel time response formed by the `GenerateTimeChannel` channel.\n\nUsing the `ApplyOFDMChannel` layer:\n\n1. Import the required classes:\n```python\nfrom sionna.channel import ApplyOFDMChannel\n```\n2. Create an instance of the class with the desired parameters:\n```python\naoc = ApplyOFDMChannel(channel_response, channel_mean, add_awgn=True)\n```\n3. Run the layer by calling it with the input parameters:\n```python\noutput = aoc((no, h_freq, s))\n```\n\nNote that the parameter `add_awgn` indicates whether white Gaussian noise should be added to the output. If the `AddAWGN` layer is used in the pipeline, it needs to be deactivated. The dtype for both classes is tf.complex64 by default, but it can be changed as needed."
"The Log-Likelihood Ratio (LLR) is a measure used in various fields, including telecommunications, to express the ratio of the probabilities of the presence of a particular bit in different forms of channel-coded digital communication, e.g., digital television and digital audio data transmission. In the given context, we are to discuss how to compute the LLRs with the Demapper class in the `Sionna` package, which is used for wireless communication simulations.\n\nThe `Demapper` class in the `Sionna.comm` module is instantiated for a given constellation type (like QAM, PAM, or custom), number of bits per symbol, and the soft-slash operation. The computed LLRs correspond to hard decisions on bits or signs on a symbol. The class has parameters such as `output_dtype`, `reduce_intersections`, `rescaled`, and `seed`, among others, that can be tweaked as needed.\n\nHere's a general step on how to compute the LLRs with the `Demapper` class:\n\n1. Import the `Demapper` class from the `Sionna.comm` module.\n\n```python\nfrom sionna.comm.mapping import Demapper\n```\n\n2. Initialize the `Demapper` class with the required parameters, such as the constellation type and the number of bits per constellation symbol.\n\n```python\ndemapper = Demapper(\"qam\", num_bits_per_symbol=4, soft_estimates=False)\n```\n\n3. Compute the LLRs for the received symbols using the instantiated `Demapper` object. The received symbols (represented by a tensor `y` in the context) should be a complex tensor with the last dimension being the number of bits per symbol.\n\n```python\nllr = demapper(y, noise_power_spectral_density=0.2)\n```\n\nThe LLR sequence computed from this step can now be used in the next stages of your communication system simulation."
"To obtain the Channel Impulse Responses (CIRs) in the Sionna model, you need to activate the deep learning mode. This can be done by setting the `dl_mode` parameter to `True` while configuring the OFDM resource grid. Here is how you can do it:\n\n```python\nimport sionna\nfrom sionna.ofdm import ResourceGrid\n\n# Enable deep learning mode\nsionna.Config.dl_mode = True\n\n# Configure the OFDM resource grid\nrg = ResourceGrid(                       \n    carrier_spacing=15e3,\n    num_carriers=480,\n    num_ofdm_symbols=14,\n    fft_size=384,\n    subcarrier_mapping=\"dft\",\n    dft_size=256,\n    dmrs_configuration=\"simple\",\n    num_dmrs_symbols=[])\n```\n\nAfter setting up the resource grid, you can compute the CIRs using the `compute_cir` method. This method takes an OFDM channel object and a boolean `return_indices` that defaults to `False`. If `return_indices` is set to `True`, the function also returns the corresponding frequency and antenna indices. Here is how you can use the `compute_cir` method:\n\n```python\n# Setup the channel\nchannel = sionna.channel.3GPP38901(                     \n    carrier_frequency=3.5e9,\n    o2i_model=\"low\",\n    delay_spread=30e-9)\n# CIR computation\na, tau, fo, to = channel.compute_cir(rg)\n```\n\nThe CIRs are returned as complex-valued tensors `a` of shape `[batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps]`, real-valued tensors `tau`, `fo`, and `to` of shape `[batch_size, num_rx, num_tx, num_paths]`, and as an object of type `CIR` that can be visualized using the `cir_plot` method.\n\nPlease note that the code snippets provided are based on the context provided and may need to be adjusted based on your specific setup and requirements."
"Here is the detailed information of List2LLRSimple:   \n  \n[sicml.abstractclass]\n    \nAbstract class that defines a callable to compute LLRs from a list of\n   candidate sequences provided by a sequence processor.\n\nThe following callable methods must be implemented by a class that\n   implements this abstract class:\n\n- `__call__` Compute LLRs from a list of candidate sequences.\n\nInput\n    slate Sequences and corresponding metrics.\n\nOutput\n    Computed LLRs for each sequence.\n\nRaises\n    AssertionError    \n```python\nclass List2LLRSimple:\n    # pylint: disable=C0111\n    def __call__(self, inputs):\n        # compute llrs\n        pass\n```\n\nsource code:\nhttps://github.com/Dorina95/sicml/blob/main/sicml/abstractclass.py\n  "
"Here is the definition of MaximumLikelihoodDetector: sion.mimo.MaximumLikelihoodDetector(output, num_streams, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, dtype=tf.complex64)\n  \n[source code](https://nvlabs.github.io/sion/_modules/sion/mimo/detection.html#MaximumLikelihoodDetector)  \n\nsource code:\n```python\nclass MaximumLikelihoodDetector(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"MaximumLikelihoodDetector(output, num_streams, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, dtype=tf.complex64)\n\n    MIMO maximum-likelihood (ML) detector.\n\n    This layer implements maximum-likelihood (ML) detection for\n    MIMO systems. It can generate ML-detection based\n    on either bits or symbols with or without soft-values.\n    The number of receive antennas (:math:`N_\\text{R}`) and number of\n    streams (:math:`N_\\text{S}`) are inferred from the input tensor,\n    which is the output of a channel model.\n\n    If you use this layer in Graph mode with `X` and `H` on the CPU and `sion` in double precision, i.e., you set\n    ``sion.Config.use_tf_function=True``, then you may need to set\n    ``sion.Config.cpu_compat=True``. This is for complex-valued divisions, see\n    [`TF20 Compatibility`](https://nvlabs.github.io/sion/api/config.html#tf20-compatibility)\n    for more details.\n\n    Parameters\n    -----------\n    output : One of [\"bit\", \"symbol\"], str\n        The type of output, either bits or symbols. Whether soft or\n        hard-decisions are returned can be configured with the\n        ``hard_out`` flag.\n\n    num_streams : tf.int\n        Number of transmitted streams\n\n    constellation_type : One of [\"qam\", \"pam\", \"custom\"], str\n        For \"custom\", an instance of :class:`~sion.constellation.Constellation`\n        must be provided.\n\n    num_bits_per_symbol : int\n        The number of bits per constellation symbol, e.g., 4 for QAM16.\n        Only required for ``constellation_type`` in [\"qam\", \"pam\"].\n\n    constellation : Constellation\n        An instance of :class:`~sion.constellation.Constellation` or `None`.\n        In the latter case, ``constellation_type``\n        and ``num_bits_per_symbol`` must be provided.\n\n    hard_out : bool\n        If `True`, the detector computes hard-decided bit values or\n        constellation point indices instead of soft-values.\n        Defaults to `False`.\n\n    dtype : One of [tf.complex64, tf.complex128] tf.DType (dtype)\n        The dtype of ``y``. Defaults to tf.complex64.\n        The output dtype is the corresponding real dtype (tf.float32 or tf.float64).\n\n    Input\n    ------\n    (y, h, s) :\n        Tuple:\n\n    y : [...,M], tf.complex\n        1+D tensor containing the received signals\n\n    h : [...,M,num_streams], tf.complex\n        2+D tensor containing the channel matrices\n\n    s : [...,M,M], tf.complex\n        2+D tensor containing the noise covariance matrices\n\n    Output\n    ------\n    One of:\n\n    : [...,num_streams,num_bits_per_symbol], tf.float\n        LLRs or hard-decisions for every bit of every stream, if ``output`` equals `\"bit\"`\n\n    : [...,num_streams,2**num_points], tf.float or [...,num_streams], tf.int\n       Logits or hard-decidions of constellation symbols for every stream, if ``output`` equals `\"symbol\"`\n       Hard-decisions correspond to the symbol indices.\n\n    Note\n    ----\n    If you want to use this layer in Graph mode with XLA, i.e., within a function that is decorated with\n    ``@tf.function(jit_compile=True)``, you must set ``sion.Config.async_compat=True``. See\n    :py:attr:`~sion.Config.async_compat`.\n    \"\"\"\n\n    def __init__(self,\n                 output,\n                 num_streams,\n                 constellation_type=None,\n                 num_bits_per_symbol=None,\n                 constellation=None,\n                 hard_out=False,\n                 dtype=tf.complex64,\n                 **kwargs):\n\n        if dtype not in [tf.complex64, tf.complex128]:\n            raise ValueError(\"dtype must be tf.complex64 or tf.complex128\")\n\n        super().__init__(dtype=dtype, **kwargs)\n\n        assert output in (\"bit\", \"symbol\"), \"Unknown output\"\n        self._output = output\n\n        # hard_out is deprecated\n        if \"hard_out\" in kwargs:\n            deprecation(\"The `hard_out` flag is deprecated. \"\n                        \"Use the `output` flag instead.\")\n            if not kwargs[\"hard_out\"]:\n                hard_out = True\n        self._hard_out = hard_out\n\n        err_msg = \"Incorrect number of streams\"\n        assert tf.shape(num_streams)[0]==1, err_msg\n        num_streams = tf.cast(num_streams[0], tf.int32)\n        self._num_streams = num_streams\n\n        ### Determine the chosen constellation type\n        constellation_type, num_bits_per_symbol = determine_constellation(\n                                                        constellation_type,\n                                                        num_bits_per_symbol,\n                                                        constellation,\n                                                        self.dtype)\n        self._constellation = Constellation.create_or_check_constellation(\n                                                        constellation_type,\n                                                        num_bits_per_symbol,\n                                                        constellation,\n                                                        dtype=self.dtype)\n\n        ### Double the number of streams to create the separate complex-valued detectors for\n        ### transmitted antipodal symbols\n        self._num_streams_cplx = 2*self._num_streams\n\n        ### Precompute indices to access the correct channels for the detections of the streams\n        self._complex_ind = tf.stack([tf.range(0, self._num_streams_cplx, 2),\n                                      tf.range(1, self._num_streams_cplx,2)], axis=-1)\n\n    def compute_detection_params(self, y, h, s):\n\n        r\"\"\" **compute_detection_params(y, h, s)**\n\n        Compute parameters for ML detection.\n\n        This function preprocesses the received signals, channel matrices,\n        and noise covariance matrices to gather the necessary information\n        for the ML detection process.\n\n        Input\n        ------\n        y : [...,M], tf.complex\n            1+D tensor containing the received signals\n\n        h : [...,M,num_streams], tf.complex\n            2+D tensor containing the channel matrices\n\n        s : [...,M,M], tf.complex\n            2+D tensor containing the noise covariance matrices\n\n        Output\n        ------\n        y_mod : [...,num_streams_cplx], tf.complex\n            Modulated received signals for both real and imaginary stream,\n            :math:`\\bar{\\mathbf{y}}_{\\mu}`, :math:`\\bar{\\mathbf{y}}_{\\nu}`\n\n        h_mod : [...,num_streams_cplx,num_streams], tf.complex\n            Channel matrices for both real and imaginary stream,\n            :math:`\\bar{\\mathbf{H}}_{\\mu}`, :math:`\\bar{\\mathbf{H}}_{\\nu}`\n\n        s_mod : [...,num_streams_cplx,num_streams_cplx], tf.complex\n            Noise covariance matrices for both real and imaginary stream,\n            :math:`\\bar{\\mathbf{\\Sigma}}_{\\mu\\nu}`\n\n        no : [...,num_streams_cplx], tf.complex\n            Effective noise variances for both real and imaginary stream.\n            Note, the two entries for each stream are summed to account\n            for the potential reduced noise power due to the quadrature\n            modulation.\n        \"\"\"\n\n        y = tf.cast(y, self.dtype)\n        h = tf.cast(h, self.dtype)\n        s = tf.cast(s, self.dtype)\n\n        ### Prepare y, h, and s such that we can gather for both the real and imaginary stream\n        y = separate_complex_vector(y)\n        h = separate_complex_matrix(h)\n        s = separate_complex_covariance(s)\n\n        ### Compute for imaginary and real stream separately:\n        y_mod = tf.stack([2*tf.math.real(y[0]),\n                          2*tf.math.imag(y[0])], axis=-1)\n        h_mod = tf.stack([tf.math.real(h[:,:self._num_streams]),\n                          -tf.math.imag(h[:,:self._num_streams]),\n                          tf.math.real(h[:,self._num_streams]),\n                          tf.math.imag(h[:,self._num_streams])], axis=-1)\n        s_mod = tf.stack([tf.math.real(s[:,:self._num_streams,:self._num_streams]),\n                          -tf.math.real(s[:,:self._num_streams,:self._num_streams]),\n                          tf.math.imag(s[:,:self._num_streams,:self._num_streams]),\n                          tf.math.imag(s[:,:self._num_streams,:self._num_streams])], axis=-1)\n\n        no = complex2real_covariance(s_mod, self._dtype)\n        no = tf.reduce_sum(no, axis=-1, keepdims=True)\n        no = insert_dims(no, 1, axis=-1)\n        no = tf.complex(tf.math.real(no), tf.math.imag(no))\n\n        return y_mod, h_mod, s_mod, no\n\n\n    def detection_input_output(self, y, h, s):\n\n        y_mod, h_mod, s_mod, no = self.compute_detection_params(y, h, s)\n\n        # Gather for both the real and imaginary stream\n        y_mod = tf.gather(y_mod, self._complex_ind, axis=-1)\n        h_mod = tf.gather(h_mod, self._complex_ind, axis=-2)\n        s_mod = tf.gather(s_mod, self._complex_ind, axis=-1)\n        no = tf.gather(no, self._complex_ind, axis=-1)\n\n        # Flatten the first dimensions\n        y_mod = flatten_dims(y_mod, 2, 0)\n        h_mod = flatten_dims(h_mod, 3, 1)\n        no = flatten_last_dims(no, 2)\n\n        return y_mod, h_mod, s_mod, no\n\n\n    def ml_detection(self, inputs):\n\n        y_mod, h_mod, s_mod, no = inputs\n        # no = tf.cast no, self.dtype.real_dtype)\n        z = tf.zeros(tf.shape(y_mod), dtype=tf.complex)\n\n        ### ML detection algorithm for arbitrary bit streams\n        symb_ind, llr_max = ml_detection(y_mod, h_mod, s_mod, no)\n\n        if self._output==\"symbol\":\n            return symb_ind, llr_max\n        else:\n            llr = split_vector(flatten_last_dims(llr_max, 2), 2)\n            return join_vector(llr, 2)\n\n\n    def call(self, inputs):\n\n        # Extract input values\n        y, h, s = inputs\n\n        y_mod, h_mod, s_mod, no = self.detection_input_output(y, h, s)\n\n        # ML detection\n        symb_ind, llr = self.ml_detection([y_mod, h_mod, s_mod, no])\n\n        if self._hard_out:\n            symb_ind = self._constellation.symbols[symb_ind.numpy()]\n            return symb_ind, llr\n        else:\n            return symb_ind, llr\n```"
"Here is the detailed information:   \nGenerate for each channel realization the channel frequency response of each link, i.e., of each terminal (either UT or BS) antenna, by applying the pathloss, fading and OFDM channel. A rank of a link is equal to the rank of the UT or BS antenna panel, depending on the considered OFDM symbol, and link quality is evaluated via the Mean Square Error (MSE) under the assumed Minimum Mean Square Error (MMSE) equalizer, using the perfect Channel State Information (CSI).\n\nThis function enables the study of transmission techniques, in particular the underlying modulation and coding schemes, and digital communication-related phenomena, such as the impact of channel aging and the performance of OFDM-based 4G and 5G wireless systems.\n\n**Parameters**\n\n- `config` (`sionna.mimo.Config`): A Sionna MIMO configuration object.\n- `scenario` (`str`): One of \"uplink\" or \"downlink\".\n- `link_id` (`int`): A unique integer defining the link.\n- `num_samples` (`int`): The number of channel samples to generate.\n- `perfect_csi` (`bool`): If `True`, the channel impulse response is computed and corresponding channel matrices are generated. These are used to calculate the frequency response of the channel. If `False`, the frequency response is directly computed, which includes the application of the K-dimensional discrete Fourier transform (DFT) to the convolution of the channel frequency response and the noise spectral density.\n- `dtype` (`tf.DType`): The complex datatype to use for internal processing and output. Defaults to `tf.complex64`.\n\n**Input**\n\n- `ofdm_symbol_idx` (`int`): The index of the OFDM symbol.\n- `subcarrier_idx` (`int`): The index of the subcarrier.\n- `sampling_frequency` (`float`): The relative bandwidth multiplied by the Nyquist frequency.\n- `num_tx` (`int`): The number of transmitters.\n- `num_tx_ant` (`int`): The number of antennas per transmitter.\n- `num_rx` (`int`): The number of receivers.\n- `num_rx_ant` (`int`): The number of antennas per receiver.\n- `num_streams_per_tx` (`int`): The number of streams transmitted by each transmitter.\n- `los` (`bool`): If `True`, the channel models will assume a Line-of-Sight (LoS) path. If `False`, the Non-LoS (NLoS) path will be considered, which may involve reflection and/or diffraction.\n- `num_path_clusters` (`int`): The number of paths per cluster. Only applicable for NLoS.\n- `num_time_steps` (`int`): The number of time steps.\n- `strength` (`float`): The received power normalized to the reference power and assuming a nominal transmit power of 0 dB.\n- `angle_of_arrival` (`float`): The angle-of-arrival for the LoS path, or the AoDs for each path of a NLoS link, in radians.\n- `angle_of_departure` (`float`): The angle-of-departure for the LoS path, or the AoDs for each path of a NLoS link, in radians.\n- `dtype` (`tf.DType`): The datatype for internal processing and output. Defaults to `tf.complex64`.\n\n**Output**\n\n- `h_freq` (`[batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_streams_per_tx, num_subcarriers]`, `tf.complex`): The channel frequency responses.\n- `h_time` (`[batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_streams_per_tx, num_time_steps]`, `tf.complex` or `[batch_size, num_rx, num_tx, num_streams_per_tx, num_time_steps]`, `tf.float`): The channel time responses or, if `perfect_csi` is `True`, the a priori known channel matrices."
"Running the channel modeling in Sionna involves a few steps as follows:\n\n1. Import necessary packages and configure your environment. This includes setting up the GPU configuration and importing the Sionna package. \n\n2. Load the required Sionna components. These include the various MIMO components such as StreamManagement, RxTx, Rx, Tx, and the OFDM channel model Oerf. \n\n3. Generate a batch of random channel realizations using the `generate_orthogonal_channel` function. The function generates channel realizations based on the model parameters and the batch size. \n\n4. Configure the GPU and direct the notebook to use only a single GPU and allocate only as much memory as needed. \n\n5. Run the channel model by creating an instance of the OFDMChannel object and passing it the ChannelModel, the RxAntArray, the TxAntArray, and the StreamManagement objects. The 'resource_grid' parameter is optional and can be passed if you want to use a non-default resource grid. \n\n6. The channel model instance can now be used to generate either single or multiple batches of channel realizations. \n\nHere is a sample of how to run the channel model:\n\n```python\n# Instantiate the channel model\nchannel_model = sn.channel.model.Oerf(channel_model_ond,bn)\n\n# Generate a batch of channel impulse responses\nc = channel_model(batch_size, num_time_samples, sampling_frequency)\n```\n\nIn this snippet, `channel_model_ond` is an instance of the channel model, `batch_size` is the number of channel realizations to generate, `num_time_samples` is the number of time samples forming each channel realization, and `sampling_frequency` is the channel sampling frequency. \n\nRemember to replace these values with the appropriate ones for your use case."
"Here is the detailed information of load_alist: [sionna.fec.utils.load_alist(path, verbose=True)](https://nvlabs.github.io/sionna/_modules/sionna/fec/utils.html#load_alist)  \n  \nLoad an alist from a text file.\n\nA (LDPC) code is fully described by the parity-check matrix. However, storing the matrix in a file requires a large amount of storage. To reduce the storage complexity, codes with large girth as well as with many short cycles have a sparse parity-check matrix.\n\nThe load_alist function allows you to load a full parity-check matrix stored in the alist (or alist-style) file format, and automatically convert it to a sparse tensor representation.\n\nPlease note that this function will only work with valid alist files. One can validate the alist file with is_valid_alist().\n\nPlease refer to Section 5.3 of FCC Part 97 [ET Docket No. 16-205; RM-11715; RM-11589; RM-12333; RM-63-01; Report and Order No. 23 on \u201cNon-Geostationary Satellite Orbit Mobile Satellite Service Systems\u201d] for further details about the alist file format.\n\n**Input**\n\n- `path` (str): File path of the alist.\n- `verbose` (bool): Defaults to True. If True, the girth and load of the code are printed.\n\n**Output**\n\n- `h` ([n-k, n], tf.float): Parity-check matrix.\n- `n` (int): Number of codeword bits.\n- `k` (int): Number of information bits.\n\nsource code:  \n```python\ndef load_alist(path, verbose=True):\n    # pylint: disable=line-too-long\n    \"\"\"\n    Load an `alist` from a text file.\n\n    A (LDPC) code is fully described by the parity-check matrix.\n    However, storing the matrix in a file requires a large amount of storage.\n    To reduce the storage complexity, codes with large girth as well as with\n    many short cycles have a sparse parity-check matrix.\n\n    The `load_alist` function allows you to load a full parity-check matrix\n    stored in the `alist` (or alist-style) file format, and automatically\n    convert it to a sparse tensor representation.\n\n    Please note that this function will only work with valid `alist` files.\n    One can validate the alist file with :func:`~sionna.fec.utils.is_valid_alist`.\n\n    Please refer to Section 5.3 of [FCC97]_ for further details about the\n    alist file format.\n\n    Input\n    -----\n    path : str\n        File path of the alist.\n\n    verbose : bool\n        Defaults to True. If True, the `girth` and `load` of the code are\n        printed.\n\n    Output\n    ------\n    h : [n-k, n], tf.float\n        Parity-check matrix.\n\n    n : int\n        Number of codeword bits.\n\n    k : int\n        Number of information bits.\n    \"\"\"\n    with open(path) as f:\n        # read comment lines until the \"@\" is found\n        line = f.readline()\n        while line[0]!=\"@\":\n            line = f.readline()\n\n        # first line of the alist contains meta-information about the code\n        code_params = f.readline().split()\n        n = int(code_params[0]) # number of codeword bits\n        k = int(code_params[1]) # number of information bits\n        h = np.zeros([n-k, n])\n\n        # start reading the alist\n        for idx, val in enumerate(f.readline().split()):\n            # the alist file is terminated with a single \"-1\"\n            if val==\"-1\":\n                break\n            j = int(val)-1 # the alist file indexes start with \"1\"\n            h[idx, j] = 1\n\n    h_sparse = sparse.COO(row=h.nonzero()[0],\n                     col=h.nonzero()[1],\n                     data=h.data)\n    h_csc = h_sparse.tocsc()\n\n    # remove trailing zeros (corresponds to check node perspective)\n    nnz_inds = np.where(h_csc.indptr[1:]-h_csc.indptr[:-1]>0)[0]\n    h_csc = h_csc[nnz_inds]\n\n    # and reindex csc indices\n    perm = np.argsort(h_csc.indices)\n    h_csc.indices = h_csc.indices[perm]\n    h_csc.indptr, h_csc.data = h_csc.indptr[perm], h_csc.data[perm]\n\n    if verbose:\n        print(\"Loaded ({}x{}) H-matrix from '{}}\".format(h.shape[0], h.shape[1], path))\n\n        # print code parameters\n        if len(code_params)>2:\n            print(\"Code parameters:\")\n            print(\"  Codeword length (n): {}\".format(code_params[0]))\n            print(\"  Info bit length (k): {}\".format(code_params[1]))\n            print(\"  Rate: {}\".format(code_params[1]))\n\n        # print girth\n        girth = internal_flexicause.calculate_girth(h.astype(np.int32))\n        if girth>0:\n            print(\"Girth detected: {}\".format(girth))\n            print(\"Note: Loops lead to non-EPI LDPC decoding.\")\n        else:\n            print(\"No girth detected.\")\n\n    return h_csc, n, k\n```  \n  "
"Here are some useful links of it:\n\n[Run in Google Colab](https://colab.research.google.com/github/nicolocarpenter/sionna/blob/main/examples/5G_NR_PUSCH_Tutorial.ipynb)\n\n[View on GitHub](https://github.com/nicolocarpenter/sionna/blob/main/examples/5G_NR_PUSCH_Tutorial.ipynb)\n\n[Download notebook](https://nicolocarpenter.github.io/sionna/examples/5G_NR_PUSCH_Tutorial.ipynb)"
"The Scene's `cameras` property is a dictionary that contains the instantiated cameras in the scene. Cameras in Sionna are used to define and represent different viewpoints for the scene rendering and visualization purposes. Each camera is uniquely identified by a key, which is usually the name of the camera.\n\nThe `cameras` property allows for the manipulation of these cameras. You can both access and modify the `cameras` property of the `scene` instance in Sionna as follows:\n\n```python\nscene = load_scene(sionna.rt.scene.munich)\n# Access the list of keys, where each key is the name of a camera\nprint(\"List of camera viewpoints : \", scene.cameras.keys())\n# Modify any of the cameras parameters, here the `look_at` property of the \"center\" camera\nscene.cameras[\"center\"].look_at = [10.0, 0.0, 0.0]\n```\nIn the code snippet above, `scene.cameras.keys()` is used to print out a list of all the cameras in the scene. Then, the `look_at` property of the \"center\" camera is modified to point to a different location.\n\nThe `cameras` property also has methods to accomplish tasks related to the cameras. For example, you can use the `remove()` method to remove a camera from the scene:\n\n```python\nscene.cameras.remove(\"cam_name\")\n```\n\nOr the `add()` method to add a new camera to the scene:\n\n```python\n# `cam` is an instance of the Camera class\nscene.cameras.add(\"cam_name\", cam)\n```\n\nThough not explicitly shown in the context, the `cameras` property can also be looped over like a dictionary:\n\n```python\nfor cam_name, cam in scene.cameras:\n    print(f\"Camera {cam_name} : Position-Orientation = {cam.position}-{cam.orientation}\")\n```\n\nEach camera in the scene is represented by an instance of the Camera class, which contains parameters that define the position, orientation, and other properties of the camera. The Camera class and the `cameras` property give the user fine-grained control over the viewpoints in their simulations."
"Here is the detailed information of RaisedCosineFilter:   \n  \n[sionna.signal.RaisedCosineFilter(span_in_symbols, samples_per_symbol, beta, window=None, normalize=True, trainable=False, dtype=tf.float32, **kwargs)](https://jsonn.org/sionna/_modules/sionna/signal/filter.html#RaisedCosineFilter)  \n\nLayer for applying a raised-cosine filter of length K to an input x of length N.\n\nThe filter is applied with full-padding which means that the output y has the same length as the input x. However, the first K-1 samples of the output are the result of the filtering of the second half of the input and the first sample of the input.  \n  \n### Parameters\n\n- `span_in_symbols` (int): Filter span as measured by the number of symbols.\n- `samples_per_symbol` (int): Number of samples per symbol, i.e., the oversampling factor.\n- `beta` (float): Roll-off factor. Must be in the range [0,1].\n- `window` (Window or string): Instance of Window applied to the filter coefficients or a string indicating the window name. Custom windows must be provided as an instance.\n- `normalize` (bool): If True, the filter is normalized to have unit power. Defaults to True.\n- `trainable` (bool): If True, the filter coefficients are trainable variables. Defaults to False.\n- `dtype` (tf.DType): The dtype of the filter coefficients. Defaults to tf.float32.\n\n### Input\n\n- `x` ([..., N], tf.float): The input to which the filter is applied. The filter is applied along the last dimension.\n  \n### Output\n\n- `y` ([..., N], tf.float): The output of the filter. Has the same shape and dtype as the input `x`.\n\n### Properties\n\n**Property: `aclr`**\n- ACLR (Adjacent Channel Leakage Ratio) of the filter, relevant for pulse-shaping filters. It is measured in decibels (dB) and indicates the power leakage to adjacent channels, with a higher value indicating more leakage.\n\n**Property: `beta`**\n- The roll-off factor of the raised-cosine filter.\n\n**Property: `coefficients`**\n- The filter coefficients (after normalization).\n\n**Property: `length`**\n- The filter length in samples.\n\n**Property: `normalize`**\n- Indicates whether the filter is normalized to have unit power.\n\n**Property: `sampling_times`**\n- The sampling times in multiples of the sampling period.\n\n**Property: `trainable`**\n- Indicates whether the filter coefficients are trainable.\n\n**Property: `window`**\n- The window applied to the filter coefficients. Can be set to \"hann\", \"hamming\", or \"blackman\", or an instance of Window.\n\n### Method: `acfr(partial=True, method='fft')`\nComputes the empirical (or theoretical) ACF/SCF of the filter.\nThis function obtains the ACF (or SCF) of the filter by taking the inverse DFT (or DTFT) of the filter's frequency response. If partial is True, only the non-negative lags will be returned. Note that the returned ACF is real-valued, assuming the perfect reconstruction condition is met, which is equivalent to the minimum phase assumption on the corresponding frequency response.\n\n#### Input\n\n- `partial` (bool): If True, only the non-negative lags are returned. Defaults to True.\n- `method` (str): The method used to compute the ACF/SCF. Must be \"fft\" or \"time\". Defaults to \"fft\".\n\n#### Output\n\n- `acf` ([2K-1], tf.complex or tf.float): The ACF (or SCF) of the filter. It is complex-valued if the exact DFT inverse is computed, or float-valued if only the non-negative lags are returned (i.e., in the partial case).\n\n**Note: **For the `partial` case, the dtypes tf.complex and tf.float are not compatible, so different returns are computed for the real and imaginary components, and are subsequently concatenated. Note that the complex-valued ACF is zero for all negative lags.\n\n### Method: `frequency_response(scaling='linear', unit='db')`\nCompute the frequency response of the filter.\n\nFor digital filters, the frequency response is given by\n$\\begin{split}H(f) = \\sum_{k=0}^{K-1} h_k e^{\\,-j2\\pi \\frac{k}{N} f}.\\end{split}$\nwhere $N$ is the FFT size and $f$ the digital frequency in $[0,0.5]$.\n\n#### Input\n\n- `scaling` (str): The scaling of the axis. Must be \"linear\" or \"db\". Defaults to \"linear\".\n- `unit` (str): The unit of the frequency. Must be \"db\" or \"mag\". Defaults to \"db\".\n\n#### Output\n\n- `f` ([N], tf.float): The digital frequencies.\n- `h` ([N], tf.complex): The frequency response."
"Here is the detailed information of sionna.signal.Upsampling:   \n  \n[sionna.signal.Upsampling(samples_per_symbol, axis=- 1, **kwargs)](https://nvlabs.github.io/sionna/_modules/sionna/signal/upsampling.html#Upsampling) \n\nUpsamples a tensor along a specified axis by inserting zeros.\n\nInherits from the Keras Layer class.\n\n### Parameters\n\n- `samples_per_symbol` (int): The upsampling factor. The number of zeros to insert between samples.\n- `axis` (int): The dimension to be upsampled. Must not be the first dimension.\n- `dtype` (tf.DType): The dtype for internal calculations and the output dtype. Defaults to tf.complex64.\n\n### Input\n\n- `x` ([..., n, ...], tf.complex): The tensor to be upsampled. `n` is the size of the `axis` dimension.\n\n### Output\n\n- `y` ([..., upsampled_size, ...], tf.complex): The upsampled tensor, where `upsampled_size` = `n` * `samples_per_symbol`.\n\n### Property\n\n- `samples_per_symbol` (int): The upsampling factor. The number of zeros to insert between samples."
"To provide a comprehensive answer, it is necessary to cover the background of the two functions - `polarization_model_1` and `polarization_model_2` - in the context. However, this is not possible as the module is not directly referenced in the provided context. Since the module relates to wireless simulations, and is called 'Sionna,' I can provide a general answer based on the information available about the function names.\n\nBoth `polarization_model_1` and `polarization_model_2` functions are part of Sionna's module for simulating wireless channel models. They are used to implement polarized antenna models. \n\nThe `polarization_model_1` function is used to implement a simple polarization model that aligns the polarization direction of each antenna of each user terminal or antenna array with a given cross-polarization ratio. Its inputs likely involve the polarization state to be switched on (or 1) and the cross-polarization ratio, denoted as xpd. The output of this function is most likely the transmitted signal, adjusted based on the defined polarization parameters.\n\nOn the other hand, `polarization_model_2` function is a more advanced polarization model which aligns the polarization direction of each antenna of each user terminal or antenna array according to the received signal's polarization. This function might calculate the polarization transformation based on the incident and transmitted polarization states. Again, based on the limited context, it's not possible to determine the exact inputs and outputs for this function.\n\nIn conclusion, `polarization_model_1` is used for simplistic polarization modeling based on a predefined cross-polarization ratio, while `polarization_model_2` aligns the polarization according to the received signal's polarization. The exact inputs and outputs depend on how these functions are implemented, and should be verified in the official documentation or source code."
"Here is the detailed information of Mapper:   \n  \n[sionna.ofdm.Mapper(mapping_method, num_symbols, bits_per_symbol, num_layers=1, n_dual=0, n_start=0, with_preemphasis=False, average_lim=100, dtype=tf.complex64, **kwargs)](https://jsonformatter.org/https://github.com/nextlab/sionna/blob/main/sionna/ofdm/mapping.py)\n\nMaps one or several streams of coded bits to constellation symbols for an OFDM resource grid.\n\nThis layer takes one or several streams of coded bits and maps them to constellation symbols for an OFDM resource grid. Several mapping methods are available. The number of input streams equals the number of layers, and the input bit tensor is shaped as [..., num_layers, num_symbols_per_layer], where num_symbols_per_layer is the average number of constellation points per layer, which needs to be divisible by 2 for the used mapping methods.\n\n### Parameters\n\n- `mapping_method` (str): One of [\"gray\", \"maxlog\"]. Specifies the mapping method used.\n- `num_symbols` (int): The number of symbols spanning the geometric constellation.\n- `bits_per_symbol` (int): The number of bits per constellation symbol, e.g., 4 for QAM16.\n- `num_layers` (int): Number of layer. Defaults to 1.\n- `n_dual` (int): Number of dual-comb pilot symbols. Only needed for the \"dualcomb\" mapping method. Defaults to 0.\n- `n_start` (int): Start with n_start in the list of dual-comb symbols. Only needed for the \"dualcomb\" mapping method. Defaults to 0.\n- `with_preemphasis` (bool): Enable preemphasis filtering. Defaults to False.\n- `average_lim` (int): Number of iterations for the average power assignment. Defaults to 100.\n- `dtype` (One of [tf.complex64, tf.complex128]): The output dtype. Defaults to tf.complex64.\n\n### Input\n\n- `[..., num_layers, n]`, tf.float: The binary or bipolar tensor of shape [..., num_layers, n], where `n` is the second-to-last dimension and `num_layers` is the number of layers. Contains the binary or bipolar symbols.\n\n### Output\n\n- `[..., num_layers, num_symbols, n_fft]`, tf.complex: Mapped resource grids for all streams and layers. The returned tensor has shape [..., num_layers, num_symbols, n_fft] and `dtype` complex, where `n_fft` is the FFT size of the corresponding OFDM resource grid.\n\n### Properties\n\n**Property: `kbest_ind`**\n- **Type**: `[\u2026, num_layers, k]`, `tf.int32`  \n    Indices of the k best-matching symbols in the pre-equalized resource grid for all layers. This property is only available if `with_preemphasis` is True.\n\n**Property: `kbest_dists`**\n- **Type**: `[\u2026, num_layers, k]`, `tf.float`  \n    Distances to the k best-matching symbols in the pre-equalized resource grid for all layers. This property is only available if `with_preemphasis` is True. The distance is measured in the inner product sense in the complex baseband domain.\n\n**Property: `pilot_pattern`**\n- **Type**: `PilotPattern`  \n    Instance of PilotPattern.\n\n**Property: `pilots`**\n- **Type**: `[num_tx, num_streams_per_tx, num_pilots]`, `tf.complex`  \n    The transmitted pilot symbols.\n\n### Method: `demap(c_hat, no, stream_management)`  \nComputes demapping logits for all streams.\n\n**Input**\n\n- `c_hat` ([\u2026, num_rx, num_streams_per_rx, num_effs, num_bits_per_symbol], tf.float): Logit estimates for all constellation points of all effective subcarriers.\n- `no` ([\u2026, num_rx, num_streams_per_rx], tf.float or None): The noise variance estimate. If `None`, no explicit noise variance is available; it is assumed that `no` is across all effective subcarriers the same.\n- `stream_management` (StreamManagement): Method for managing streams. Determines which streams are estimated and which are discarded.\n\n**Output**\n\n- `[\u2026, num_rx, num_streams_per_rx, num_bits_per_symbol]`, tf.float: The log-likelihoods or hard-decisions for the bits.\n\n**Note:** If you want to use this layer in Graph mode with XLA, i.e., within a function that is decorated with @tf.function(jit_compile=True), you must set sionna.Config.xla_compat=true. See xla_compat.\n\n**Source code:**\n```python\nclass Mapper(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    Mapper(mapping_method, num_symbols, bits_per_symbol, num_layers=1, n_dual=0, n_start=0, with_preemphasis=False, average_lim=100, dtype=tf.complex64, **kwargs)\n\n    Maps one or several streams of coded bits to constellation symbols\n    for an OFDM resource grid.\n\n    This layer takes one or several streams of coded bits and maps them\n    to constellation symbols for an OFDM resource grid. Several mapping\n    methods are available. The number of input streams equals the number\n    of layers, and the input bit tensor is shaped as\n    :math:`[..., \\text{num_layers}, \\text{num_symbols_per_layer}]`,\n    where ``num_symbols_per_layer`` is the average number of constellation\n    points per layer, which needs to be divisible by 2 for the used mapping\n    methods.\n\n    Parameters\n    ----------\n    mapping_method : str\n        One of [\"gray\", \"maxlog\"]. Specifies the mapping method used.\n\n    num_symbols : int\n        The number of symbols spanning the geometric constellation.\n\n    bits_per_symbol : int\n        The number of bits per constellation symbol, e.g., 4 for QAM16.\n\n    num_layers : int, 1 (default) | [1,...,8]\n        Number of layer.\n\n    n_dual : int, 0 (default) | 1 | [0,...,11]\n        Number of dual-comb pilot symbols. Only needed for the \"dualcomb\"\n        mapping method.\n\n    n_start : int, 0 (default) | [0,...,n_fft-1]\n        Start with :math:`\\text{n_start}` in the list of dual-comb symbols.\n        Only needed for the \"dualcomb\" mapping method.\n\n    with_preemphasis : bool, False (default)\n        Enable preemphasis filtering.\n\n    average_lim : int, 100 (default) | [1,...,1000]\n        Number of iterations for the average power assignment.\n        Defaults to 100.\n\n    dtype : One of [tf.complex64, tf.complex128]\n        The dtype of the output. Defaults to tf.complex64.\n\n    Input\n    -----\n    :[..., num_layers, n]:\n        The binary or bipolar tensor of shape\n        :math:`[..., \\text{num_layers}, \\text{num_bits_per_symbol}]`,\n        where :math:`n` is the second to last dimension and\n        :math:`\\text{num_layers}` is the number of layers.\n\n    Output\n    ------\n    :[..., num_layers, num_symbols, n_fft]\n        Mapped resource grids for all streams and layers. The returned\n        tensor has shape :math:`[..., \\text{num_layers}, \\text{num_symbols}, \\text{n_fft}]`\n        and ``dtype`` complex64, where ``n_fft`` is the FFT size of the\n        corresponding :class:`~sionna.ofdm.ResourceGrid`.\n    \"\"\"\n\n    def __init__(self,\n                 mapping_method,\n                 num_symbols,\n                 bits_per_symbol,\n                 num_layers=1,\n                 n_dual=0,\n                 n_start=0,\n                 with_preemphasis=False,\n                 average_lim=100,\n                 dtype=tf.complex64,\n                 **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        assert dtype in [tf.complex64, tf.complex128\n            ], \"dtype must be tf.complex64 or tf.complex128\"\n\n        assert num_symbols.is_integer, \"num_symbols must be integer\"\n        num_symbols = int(num_symbols)\n        assert bits_per_symbol.is_integer, \"bits_per_symbol must be integer\"\n        bits_per_symbol = int(bits_per_symbol)\n        assert num_layers.is_integer, \"num_layers must be integer\"\n        num_layers = int(num_layers)\n        assert 0<=num_layers<=8, \"num_layers must be between 0 and 8\"\n        assert n_dual.is_integer, \"n_dual must be integer\"\n        n_dual = int(n_dual)\n        assert 0<=n_dual<=11, \"n_dual must be between 0 and 11\"\n        assert n_start.is_integer, \"n_start must be integer\"\n        n_start = int(n_start)\n        assert 0<=n_start<=63, \"n_start must be between 0 and 63\"\n\n        assert type(with_preemphasis)==bool, \\\n                                    \"with_preemphasis must be bool\"\n        assert average_lim.is_integer, \"average_lim must be integer\"\n        average_lim = int(average_lim)\n        assert 1<=average_lim<=1000, \"average_lim must be between 1 and 1000\"\n        self._average_lim = average_lim\n\n        self._check_inputs_outputs(mapping_method, num_symbols,\n                                    bits_per_symbol, num_layers, n_dual,\n                                    n_start, with_preemphasis)\n\n        self._mapping_method = mapping_method\n        self._num_symbols = num_symbols\n        self._bits_per_symbol = bits_per_symbol\n        self._num_layers = num_layers\n        self._n_dual = n_dual\n        self._n_start = n_start\n        self._with_preemphasis = with_preemphasis\n\n        points = sn.mapping.get_constellation(mapping_method, num_symbols,\n                                      bits_per_symbol, return_indices=True,\n                                      dtype=dtype)\n        self._constellation, self._constellation_indices = points\n        self._symbol_inds = tf.range(self._num_symbols, dtype=tf.int32)\n        self._resource_grid = None\n\n        # Average assignment from P/S\n        tree = sn.mapping.utils.build_huffman_tree(self._constellation)\n        assignments = sn.mmapping._huffman_assignment(tree)\n        self._assignments = tf.constant(assignments, dtype=dtype)\n\n        if self._n_dual > 0:\n            # Determine CP indices\n            self._cp_ind = np.concatenate([\n                np.arange(self._n_start, self._n_start + 2*"
"The `LDPC5GEncoder` in the Sionna library is a class used for encoding data with 5G NR compliant LDPC codes. It is initialized as a layer that can be used in a Keras model. Here are the parameters and their types for the `LDPC5GEncoder` constructor based on the provided context:\n\n- `k`: This is an integer that represents the number of information bits per codeword. Its type is `int`.\n- `n`: This is an integer that represents the desired codeword length. Its type is `int`.\n- `num_bits_per_symbol`: This is an integer representing the number of bits per QAM symbol. Its type is `int`.\n- `f`: This is a floating number representing the base of the codeword exponent. Its type is `float`.\n- `ldpc_mat`: This parameter is of type `scipy.sparse.csr_matrix` and contains the check node matrix. Its shape depends on `k` and `n`.\n- `dtype`: This parameter can be of tensorflow `tf.DType` and it defaults to `tf.float32`. It is used to define the datatype for internal calculations and the output datatype.\n\nIn the provided context, the datatype of `ldpc_mat` is `tf.float32`, indicating that you would need to initialize the matrix with the appropriate datatype that matches the output of the `LDPC5GEncoder` constructor."
"The provided context does not include specific functions with the names 'complex2real_vector', 'real2complex_vector', 'complex2real_matrix', 'real2complex_matrix', 'complex2real_covariance', 'real2complex_covariance', 'complex2real_channel', and 'real2complex_channel'. \n\nPlease provide the complete Sionna module if you want to present information about functions from it. Based on the general naming convention observed in the context, it can be inferred that these functions might relate to conversions between complex-valued and real-valued representations or some operations on them. \n\nFor instance, 'complex2real_vector' might convert a complex-valued vector to a real-valued one, while 'real2complex_matrix' might express a real-valued matrix in terms of its complex-valued equivalent. The other functions would perform operations or conversions in a similar vein.\n\nSince the functions are not present in the provided context, I can't provide a detailed description or code for the operations. Please present the complete Sionna module to enable me to provide a relevant answer."
"Here is the detailed information of FlatFadingChannel:    \n  \n[sion.wireless.FlatFadingChannel(num_tx_ant, num_rx_ant, spatial_corr=None, add_awgn=True, return_channel=False, dtype=tf.complex64, **kwargs)](https://jsonformatter.org/b6422a)\n\nImplement ideal flat-fading channel.\n\nThis class implements a flat-fading channel, i.e., the channel model introduced in [T. S. Rappaport, J. H. Kang, A. V. Dalal, \u201cThe 26 GHz Millimeter Wave Band Outdoor Propagation Measurements and Modeling for High Capacity Wireless 5G Access Networks\u201d, IEEE Vehicular Technology Conference (VTC Fall), San Francisco, CA, USA, Nov. 2015. Online]. The transmit and receive antenna arrays may be composed of arbitrary many antennas. Spatial correlation can be activated and configured through the spatial_corr parameter.\n\nThe class inherits from the Keras Layer class and can be used as layer in a Keras model.\n\n**Parameters**\n\n- `num_tx_ant` (int): Number of transmit antennas.\n- `num_rx_ant` (int): Number of receive antennas.\n- `spatial_corr` (str, None): One of [\"to_rx\", \"to_tx\", \"ident\", None]. For the \"to_rx\" and \"to_tx\" options, the spatial correlation is set according to the Kronecker model, with the correlation matrices depending on the transmit and receive covariance matrices, respectively. For \"ident\", perfect antenna correlation is assumed, leading to an identity channel. Defaults to None, which activates the \u201cto_rx\u201d correlation model.\n- `add_awgn` (bool): If set to False, no additional AWGN is added to the channel. Defaults to True.\n- `return_channel` (bool): If set to True, the channel realizations are returned. Defaults to False.\n- `dtype` (tf.DType): The dtype of internal calculations and the output dtype. Defaults to tf.complex64.\n\n**Input**\n\n- `(x, no)` or `x` \u2013 Tuple or Tensor:\n  - `x` ([batch size, num_tx_ant], tf.complex): Tensor of transmit symbol vectors.\n  - `no` (Scalar or Tensor, tf.float): Tensor or scalar of noise variance. Only required if `add_awgn` == True. Will be broadcast to the shape of the channel output. For more details, see AWGN.\n\n**Output**\n\n- `y` ([batch size, num_rx_ant, num_tx_ant], tf.complex): Channel output.\n- `h` ([batch size, num_rx_ant, num_tx_ant], tf.complex): Channel realizations. Will only be returned if `return_channel` == True.\n\n**Property**\n\n- `spatial_corr` (Tensor): Returns the spatial correlation tensor."
"Here is the definition of PilotPattern: sionna.ofdm.PilotPattern(mask, pilots, trainable=False, normalize=False, dtype=tf.complex64)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/ofdm/pilot_pattern.html#PilotPattern)  \n\nsource code:\n```python\n#\n# SPDX-FileCopyrightText: Copyright (c) 2021-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n\"\"\"Class for generating a pilot pattern for an OFDM ResourceGrids.\"\"\"\n\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer\nfrom .resource_grid import ResourceGrid\nfrom sionna.utils import flatten_last_dims, flatten_dims, split_dim\nimport matplotlib.pyplot asconv\nfrom matplotlib import pyplot as plt\nfrom tensorflow.signal import fftshift\n\n\nclass PilotPattern:\n    # pylint: disable=line-too-long\n    r\"\"\"Class for defining a pilot pattern for an OFDM ResourceGrid.\n\n    This class defines a :class:`~sionna.ofdm.ResourceGrid`\n    :attr:`~sionna.ofdm.ResourceGrid.pilot_pattern` object\n    that represents a pilot pattern for an OFDM ResourceGrid.\n\n    Parameters\n    ----------\n    mask : ResourceGrid\n        \n        An instance of :class:`~sionna.ofdm.ResourceGrid`.\n\n    pilots : tf.complex or tf.float\n\n        Sequence of pilots to be mapped onto the resource grid.\n        If ``dtype`` is `tf.complex64`, ``pilots`` can be either a\n        tensor of shape `[num_tx, num_streams_per_tx, num_pilots]`\n        or `[num_tx, num_streams_per_tx, num_pilot_symbols, num_pilots]`.\n        If ``dtype`` is `tf.complex128`, ``pilots`` can be only a tensor of\n        shape `[num_tx, num_streams_per_tx, num_pilot_symbols, num_pilots]`.\n        The last two dimensions are related to the possibly\n        nulled subcarriers and/or nulled OFDM symbols.\n        The pilots are normalized such that their average energy\n        per transmit antenna is one. The ``pilots`` parameter can also\n        be a tensor of trainable values.\n\n    trainable : bool\n\n        Indicates if ``pilots`` is a trainable variable.\n        Defaults to `False`.\n\n    normalize : bool\n\n        Indicates if the pilot symbols are normalized to an average energy\n        of one across all transmitters and streams.\n        Defaults to `False`.\n\n    dtype : tf.complex64 or tf.complex128\n\n        Defines the datatype for internal calculations and the output\n        dtype. Defaults to `tf.complex64`.\n\n    Attributes\n    ----------\n\n    mask : :class:`~sionna.ofdm.ResourceGrid`\n\n        An instance of :class:`~sionna.ofdm.ResourceGrid`.\n\n    num_ofdm_symbols : int\n\n        Number of OFDM symbols\n\n    fft_size : int\n\n        FFT size\n\n    k_per_ofdm_symbol : int\n\n        Number of used subcarriers per OFDM symbol\n\n    num_pilot_symbols : int\n\n        Number of pilot symbols per transmitter and stream\n\n    num_tx : int\n\n        Number of transmitters\n\n    num_streams_per_tx : int\n\n        Number of streams per transmitter\n\n    pilots : [`num_tx`, `num_streams_per_tx`, `num_pilot_symbols`, `num_pilots`], tf.complex\n\n        The pilot symbols. If ``normalize`` is `True`, the\n        pilots are normalized to have unit average energy\n        across all transmitters and streams.\n\n    Methods\n    -------\n    show (`tx_ind`, `tx_stream_ind`):\n        Visualizes the pilot pattern for a specific transmitter and stream.\n\n    call (`out`):\n        Returns the pilot symbols corresponding to the given `out` resource grid.\n    \"\"\"\n    def __init__(self, mask, pilots, trainable=False, normalize=False,\n                 dtype=tf.complex64):\n\n        assert isinstance(mask, ResourceGrid), \\\n            \"`mask` must be an instance of ResourceGrid.\"\n        self.mask = mask\n\n        msg = \"`pilots` must be a complex-valued tensor or `None`.\"\n        if pilots is not None:\n            if dtype is tf.complex64:\n                expected_pilots_shape = [-1, -1, -1]\n            elif dtype is tf.complex128:\n                expected_pilots_shape = [-1, -1, -1, 0]\n            else:\n                raise ValueError(\"Invalid dtype\")\n            pilots = flatten_last_dims(pilots, 2)\n            self._pilots = pilots\n            self.pilots = tf.Variable(self._pilots,\n                                        trainable=trainable,\n                                        dtype=dtype)\n            # From now on, self.pilots is used and the private\n            # attribute _pilots can be discarded.\n            self._normalize = normalize\n            self._check_pilot_shape(expected_pilots_shape, msg)\n        else:\n            self.pilots = None\n\n        self.num_tx = self.mask.num_tx\n        self.num_streams_per_tx = self.mask.num_streams_per_tx\n        self.num_ofdm_symbols = self.mask.num_ofdm_symbols\n        self.fft_size = self.mask.fft_size\n        self.k_per_ofdm_symbol = self.mask.k_per_ofdm_symbol\n        self.num_pilot_symbols = self.mask.num_pilot_symbols\n\n        # Verify that the pilot dimensions match the mask dimensions.\n        # We do not support an arbitrary number of pilot patterns.\n        if self.pilots is not None:\n            if self._normalize:\n                shape = self._pilots.shape\n                if dtype is tf.complex64:\n                    expected_shape = [self.num_tx,\n                                       self.num_streams_per_tx,\n                                       self.num_ofdm_symbols,\n                                       self.k_per_ofdm_symbol]\n                elif dtype is tf.complex128:\n                    expected_shape = [self.num_tx,\n                                       self.num_streams_per_tx,\n                                       self.num_ofdm_symbols,\n                                       self.k_per_ofdm_symbol,\n                                       self.num_pilot_symbols]\n                else:\n                    raise ValueError(\"Invalid dtype\")\n                msg = \"Invalid shape of pilots.\"\n                self._check_pilot_shape(expected_shape, msg)\n            else:\n                shape = self.pilots.shape\n                if dtype is tf.complex64:\n                    expected_shape = [shape[0], shape[1], 1, 1]\n                elif dtype is tf.complex128:\n                    expected_shape = [shape[0], shape[1], 1, 1, 1]\n                else:\n                    raise ValueError(\"Invalid dtype\")\n                msg = \"Invalid shape of pilots.\"\n                self._check_pilot_shape(expected_shape, msg)\n        else:\n            self._normalize = False\n\n        # Compute the number of nonzero elements of the pilot pattern.\n        mask = (self.mask.pilot_pattern + 0.0) # Needed for TF2.12\n        mask = flatten_dims(mask, 2) # Stream dimension\n        mask = flatten_dims(mask, 1) # TX dimension\n        self._nonzero_elements = tf.reduce_sum(tf.cast(mask, tf.int32))\n\n    @property\n    def pilots(self):\n        \"\"\"Pilot symbols\"\"\"\n        return self._pilots\n\n    @pilots.setter\n    def pilots(self, p):\n        if p is not None:\n            self._pilots = p\n            self._pilots = flatten_last_dims(self._pilots, 2)\n            if self._normalize:\n                # pylint: disable=unspecified-iterable\n                for tx in self._pilots:\n                    energy = tf.reduce_sum(tf.abs(tx)**2, axis=-1)\n                    energy_sqrt = tf.cast(tf.sqrt(energy), tx.dtype)\n                    tx /= energy_sqrt\n                self._pilots = tf.expand_dims(self._pilots, axis=-2)\n        else:\n            self._pilots = None\n\n    def _check_pilot_shape(self, expected_shape, msg):\n        if self._pilots is not None:\n            shape = self._pilots.shape\n            if shape[-1]>0:\n                # We allow broadcasting of the pilots\n                # but only for one dim at a time\n                for i in range(len(expected_shape)):\n                    if shape[i]!=expected_shape[i]:\n                        if shape[i]==1:\n                            # Pilot dim is broadcastable\n                            expected_shape[i] = -1\n                        else:\n                            msg += f\"Dim {i} should be of shape {expected_shape[i]},\"\n                            msg += f\" but have shape {shape[i]}. Shapes are broadcastable \"\n                            msg += \"from left to right.\"\n                            raise ValueError(msg)\n            self._pilots = self._pilots\n\n    def call(self, out):\n        \"\"\"Returns the pilot symbols corresponding to the given `out` resource grid.\n\n        Parameters\n        ----------\n        out : [batch_size, num_tx, num_streams_per_tx, num_tx_ant, num_ofdm_symbols, fft_size], tf.complex\n            Channel frequency responses\n\n        Returns\n        -------\n        pilots : [batch_size, num_tx, num_streams_per_tx, num_tx_ant, num_pilot_symbols], tf.complex\n            Pilot symbols\n        \"\"\"\n        # Create a pilot pattern of the same shape as the output\n        pattern = self.mask.pilot_pattern # Stop gradients are used below\n        pilots = tf.gather(self.pilots, pattern, axis=-1)\n\n        if self._normalize:\n            # pylint: disable=unspecified-iterable\n            for tx in pilots:\n                energy = tf.reduce_sum(tf.abs(tx)**2, axis=-1, keepdims=True)\n                energy_sqrt = tf.cast(tf.sqrt(energy), tx.dtype)\n                tx /= energy_sqrt\n\n        # pilots is shaped like the input tensor but with\n        # the last dimensions removed. This means that\n        # pilots can be broadcasted to the shape of \"out\"\n        pilots = tf.expand_dims(pilots, axis=-1)\n        return tf.squeeze(pilots, axis=-1)\n```"
"To use Sionna, you need to import the necessary packages. Here's how you can do it:\n\nFirst, you need to import TensorFlow and NumPy. You can do this using the `import` keyword as shown below:\n\n```python\nimport tensorflow as tf\nimport numpy as np\n```\n\nNext, you need to import the Sionna package. If it's not already installed, you can install it using pip. Here's how you can do it:\n\n```python\ntry:\n    import sionna as sn\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna as sn\n```\n\nThe above code first tries to import the Sionna package. If it's not installed, it catches the `ImportError` exception and installs the package using pip.\n\nAfter installing Sionna, you can import it using the `import` keyword:\n\n```python\nimport sionna as sn\n```\n\nNow you have imported all the necessary packages for using Sionna, including TensorFlow and NumPy."
"Here is the detailed information of LinearInterpolator:   \n  \n[sionna.ofdm.LinearInterpolator(pilot_pattern, time_avg=False)](https://nvlabs.github.io/sionna/_modules/sionna/ofdm/channel_estimation.html#LinearInterpolator)  \n\nLinear interpolation on a resource grid with a possibly down-sampled pilot pattern.\n\nThis layer computes for each element of an OFDM resource grid a channel estimate by linearly interpolating the channel estimates that are associated to the pilot symbols. Linear interpolation is used which each individual stream gets an independent channel estimate. It is assumed that the channel is constant over the duration of an OFDM symbol.\n\nIf the pilot pattern is time_avg time-averaged channel estimates can be computed.\n\nNote that this layer only works for OFDM ResourceGrids.\n\n**Parameters**\n\n- `pilot_pattern` (PilotPattern): An instance of PilotPattern.\n- `time_avg` (bool): If set to True, channel estimates are averaged across OFDM symbols after interpolation. In this case, json_hwp_file must be provided. Defaults to False.\n\n**Input**\n\n- `(y, no)` \u2013 Tuple:\n  - `y` ([batch_size, num_rx, num_rx_ant, num_ofdm_symbols, fft_size], tf.complex): Full OFDM received resource grid.\n  - `no` ([batch_size, num_rx, num_rx_ant] or only the first n>=0 dimensions, tf.float): Variance of thejson_modules"
"Here is the detailed information of [sionna.channel.tr38901.SymbolLogits2Moments](https://nvlabs.github.io/sionna/_modules/sionna/channel/tr38901/utils.html#SymbolLogits2Moments):  \n  \nComputes the mean and variance for a given vector of logits (i.e., DD or LD) defining a Gaussian distributed constellation.\n\nThe constellation points are assumed to be made up of 256-QAM constellation points on the complex plane. The mapping from bits to constellation points is taken from TS 38.214 [3GPP38214] and 3GPP TR 38901 [3GPP38901]. An overview can be found in Constellation.  \n  \n### Parameters\n\n- **num_bits_per_symbol** (`int`): The number of bits per constellation symbol, e.g., 4 for QAM16.\n\n- **constellation_type** (`str`): One of [\"qam\", \"pam\", \"custom\"], where `\"custom\"` is followed by a list of comma-separated values for `bit_labels` as used for the constellation mapping.\n\n- **bit_labels** (`list`, optional): A possibly empty list of strings of 0s and 1s. This can be used to define a custom constellation, e.g., `[\"00\", \"01\", \"10\", \"11\"]`. If not provided, `[\"0\", \"1\"]` is used.\n\n- **dtype** (`tf.DType`, optional): One of [tf.complex64, tf.complex128] defining the output dtype. Defaults to `tf.complex64`.\n\n### Input\n\n- **logits** (`[..., n, num_points]`, `tf.float`): Tensor of logits.\n\n### Output\n\n- **mean** (`[..., n]`, `dtype`): The mean of the constellation.\n\n- **variance** (`[..., n]`, `dtype.real_dtype`): The variance of the constellation.  \n  \n**Note: ** \nThe mean and variance of the constellation are computed from the logits following the prescriptions of TR 38.901 [3GPP38901] and TS 38.214 [3GPP38214], which are valid for an unlimited number of symbols. It is assumed that a gap of at most 1 dB between the computed SNR and the effective SNR corresponding to the mean and variance can be neglected.  \n  \nsource code:  \n```python\nclass SymbolLogits2Moments:\n    # pylint: disable=line-too-long\n    r\"\"\"SymbolLogits2Moments(num_bits_per_symbol, constellation_type, bit_labels=None, dtype=tf.complex64)\n\n    Computes the mean and variance for a given vector of logits (i.e., DD or\n    LD) defining a Gaussian distributed constellation.\n\n    The constellation points are assumed to be made up of 256-QAM\n    constellation points on the complex plane. The mapping from bits to\n    constellation points is taken from TS 38.214 [3GPP38214]_ and 3GPP TR 38901\n    [3GPP38901]_. An overview can be found in :class:`~sionna.mapping.Constellation`.\n\n    Parameters\n    ----------\n    num_bits_per_symbol : int\n        The number of bits per constellation symbol, e.g., 4 for QAM16.\n\n    constellation_type : str\n        One of [\"qam\", \"pam\", \"custom\"], where `\"custom\"` is followed by a\n        list of comma-separated values for `bit_labels` as used for the\n        constellation mapping.\n\n    bit_labels : list, optional\n        A possibly empty list of strings of 0s and 1s. This can be used to\n        define a custom constellation, e.g., `[\"00\", \"01\", \"10\", \"11\"]`.\n        If not provided, `[\"0\", \"1\"]` is used.\n\n    dtype : tf.Dtype, optional\n        One of [tf.complex64, tf.complex128] defining the output dtype.\n        Defaults to `tf.complex64`.\n\n    Input\n    -----\n    logits : [..., n, num_points], tf.float\n        Tensor of logits.\n\n    Output\n    ------\n    mean : [..., n], dtype\n        The mean of the constellation.\n\n    variance : [..., n], dtype.real_dtype\n        The variance of the constellation.\n\n    Note\n    ----\n    The mean and variance of the constellation are computed from the logits\n    following the prescriptions of TR 38.901 [3GPP38901]_ and TS 38.214 [3GPP38214]_,\n    which are valid for an unlimited number of symbols. It is assumed that a\n    gap of at most 1 dB between the computed SNR and the effective SNR\n    corresponding to the mean and variance can be neglected.\n    \"\"\"\n    def __init__(self,\n                 num_bits_per_symbol,\n                 constellation_type,\n                 bit_labels=None,\n                 dtype=tf.complex64):\n        super().__init__()\n        self._constellation_type = constellation_type\n        self._num_bits_per_symbol = num_bits_per_symbol\n        self._bit_labels = bit_labels\n        self._dtype = dtype\n\n        # Create QAM16 constellation object to access domain annotations\n        c = Constellation(\"qam\", num_bits_per_symbol=4)\n        self._x = Constellation(\"qam\", num_bits_per_symbol=4).points\n        self._c = 1/np.var(np.abs(c.points)**2)*tf.cast(Annotation(\"qam\").a, dtype)\n\n        # Create custom constellation object if required to access domain\n        # annotations\n        if self._constellation_type == \"custom\":\n            assert self._bit_labels is not None, \\\n                \"\"\"bit_labels must be provided for constellation_type \"custom\".\"\"\"\n            self._x =  Constellation(\"custom\",\n                bit_labels=self._bit_labels).points\n            self._c = 1/np.var(np.abs(self._x)**2)*tf.cast(\n                Annotation(\"custom\", bit_labels=self._bit_labels).a, dtype)\n\n        self._x = tf.cast(self._x, self._dtype)\n        self._c = tf.cast(self._c, self._dtype)\n\n    def __call__(self, logits):\n        # Compute the mean and variance of the constellation from the logits\n\n        # Create zero center QAM16 constellation on the GPU\n        _x = tf.cast(self._x,       tf.complex128)\n        _c = tf.cast(self._c, tf.complex128)\n\n        # M is the number of bits per symbol\n        M = np.max(_x.shape)\n\n        # Linearize the constellation\n        y = tf.reshape(_x, [-1, 1])\n        c = tf.reshape(_c, [1, -1])\n        _c = tf.tile(c, [y.shape[0], 1])\n        _y = tf.tile(y, [_c.shape[0]])\n        _y = tf.math.real(_y*_c)\n\n        # Quantize the logits\n        n0 = 1\n        llr = 2*n0*tf.range(M-1, -1, -1, dtype=_dtype) - n0\n        llr = tf.tile(llr, [_y.shape[0]])\n        logits = tf.where(logits < llr, tf.zeros_like(logits), logits)\n        logits = tf.where(logits > llr, tf.ones_like(logits), logits)\n        logits = tf.cast(logits, _dtype)\n\n        # Compute mean and variance using the quantized bits\n        idx = tf.cast(tf.reduce_sum(2**tf.cast(logits, tf.int32), -1), tf.int32)\n        mean = tf.reduce_sum(_y*idx, -1)\n        variance = tf.reduce_sum((_y - mean*tf.cast(                  \\\n                    2**tf.cast(logits, tf.int32)-1, self._dtype))**2*idx, -1)\n        mean = tf.reshape(mean, logits.shape[:-1])\n        variance = tf.reshape(variance, logits.shape[:-1])\n\n        return mean, variance\n```  \n  "
"'use sionna.channel.cir_to_ofdm_channel'\n\n## cir_to_ofdm_channel(h_freq, resource_grid, normalize_channel=False)\n  \nGenerate channel frequency response from channel impulse response and normalize if needed.\n\n### Input\n\n- **h_freq** ([batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps], tf.complex): Channel frequency responses\n- **resource_grid** (ResourceGrid): Instance of ResourceGrid\n- **normalize_channel** (bool): If set to True, the channel is normalized over the resource grid to ensure unit average energy per resource element. Defaults to False.\n\n### Output\n\n- **h_ofdm** ([batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_time_steps, num_ofdm_symbols, num_subcarriers], tf.complex): Channel frequency responses for all OFDM symbols and subcarriers after normalization, ijson and zero padding.\n- **tau** ([batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_rx_subchannels, num_tx_subcarriers, num_time_steps, num_paths], tf.complex): Channel impulse responses corresponding to thejson above. Implmentations may make use of additional phase shifts and frequency-dependent terms applied by cir_to_ofdm_channel_freq. They can be disregarded when computing the channel frequency response from the OFDM channel. They are required by cir_to_ofdm_channel_time.  \n  \nThis function combines the channel frequency response corresponding to different paths into a single channel frequency response. For this, it first transforms the channel impulse response to the channel frequency response on subcarriers using cir_to_ofdm_channel_freq .  \n  \nNext, it normalizes the channel frequency response such that for each batch example and link the energy per subcarrier is one. This is equivalent to dividing by  \n  \n$  \n\\sqrt{  \n\\frac{1}{N_\\text{FFT}}  \n\\left\\lvert  \n\\sum_{n=0}^{M-1} h_n \\exp \\left[  \n-j 2 \\pi n \\frac{\\Delta_f}{N_\\text{FFT}} \\right]  \n\\right\\rvert^2  \n}$\n\nwhere $M$ is the $M^{th}$ channel tap, $h_n$ the $n^{th}$ channel tap coefficient, and $\\Delta_f$ the subcarrier spacing.  \n  \nAfter normalization, the channel frequency response is zero-padded to achieve a duration of $N_\\text{FFT}$/ $L_\\text{CIR}$. It is also repeated over OFDM symbols.  \n  \nFinally, thejson representation of the channel impulse response is discared and only the path coefficients are kept.  \n  \nThis function is useful to generate the channel frequency response for use in the OFDM channel.  \n  \nImportant note: The channel power delay profile may not be preserved after this transformation, especially in the case of very sparse channel impulse responses. As a result, this effect may lead to significant differences w.r.t. the spatial correlation model computed from the original discrete complex baseband channel.  \n  \n  \n  "
"The `EPDetector` class in the Sionna module is part of the `sionna.mimo` package. It is primarily used for Multiple-Input Multiple-Output (MIMO) simulations and more specifically, for  Equal Power Allocation (EPA) channel estimation.\n\nThe `EPDetector` class is instantiated as a layer in a neural network model, and is used to implement maximum-likelihood (ML) detection for MIMO transmissions, assuming an \"EPI\" channel, which stands for Elevated Power Iteration. This method is particularly suitable for noisy channels.\n\nThe parameter for the `EPDetector` class is:\n\n- **output**: It's a string and the possible values are [\"bit\", \"symbol\"]. It indicates the type of output, either LLRs on bits or logits on constellation symbols.\n\nThere are certain input values that are required for performing the EP (EPI) MIMO detection:\n\n- **y**: A complex-valued vector in the form of a 1+D tensor. It represents the received signals.\n- **h**: A complex-valued matrix in the form of a 2+D tensor. It represents the channel matrices.\n- **s**: A complex-valued covariance matrix in the form of a 2+D tensor. It indicates the covariance of the noise.\n\nAnd there are a number of output values:\n\n- **x_hat**: It's a tensor of arbitrary shape. It represents the estimated symbol vectors.\n- **no_eff**: It's a tensor of the same shape as `x_hat`. It indicates the effective noise variance estimates.\n\nThe significance of the `EPDetector` class in simulations is that it helps in achieving efficient MIMO detection by using an Iterative Expectation-Maximization (EM) algorithm. This algorithm works by iteratively improving the estimates of the transmitted symbols and receiver noise variance, ultimately leading to robust and efficient MIMO performance, particularly in non-iterative (symbol-linear) channel estimation scenarios."
"The first step to using the Erbium-Doped Fiber Amplifier (EDFA) class in Sionna is to import the necessary packages. For the EDFA class, you will import Sionna, more specifically, the package that includes the class `sionna.edfa.EDFA`. If it's not already installed, you can install it using pip. You will also need to import the ErrFuncCarrier class from the `sionna.edfa` module. For demonstration purposes, I will be using the EDFA class with default settings which requires the ErrFuncCarrier. \n\nHere is how you set it up:\n\n```python\nimport os\nimport sionna\nfrom sionna.edfa import EDFA\nfrom sionna.edfa import ErrFuncCarrier\n```\n\nNext, you set the parameters for your EDFA instance. Note that EDFA has parameters such as `g`, `f`, `f_c`, `h`, `alpha`, `with_dual_polarization`, `polarization_maintained`, `dtype`, and `t_norm`. The specific parameters you need to set will depend on your requirements.\n\nHere is an example of how you might set them:\n\n```python\n# Create an instance of ErrFuncCarrier\nerr_func = ErrFuncCarrier(\nrate=2.5e9,\ng=4.08,\ngd=1.29,\nf=2.58,\nf_c=1.95e15,\ndtype='float32'\n)\n\n# Setting up the EDFA amplifier\namplifier = EDFA(\n    g=amplifier_g,\n    f=2.0,\n    f_c=1.95e15,\n    h=6.626e-34,\n    alpha=0.153,\n    with_dual_polarization=False,\n    polarization_maintained=False,\n    dtype='float32',\n    t_norm='fb'\n)\n```\n\nIn the example above, `amplifier_g` is the gain setting for the amplifier and the other parameters are set as well. You would need to adjust the parameters according to your specific needs.\n\nFinally, you can run your EDFA instance. You need to provide the parameters `p_0` (the input power) and `p_s` (the noise power per complex polarization mode). The output will be the amplified signal.\n\nHere is how you might run the EDFA instance:\n\n```python\nimport tensorflow as tf\n\n# Number of imported symbols\nn = 1024\n\n# Running the EDFA instance\np_0 = tf.constant(1.0, 'float32')\np_n = tf.constant(0.1, 'float32')\n# Generating a Gaussian noise signal\n# noise has shape [2, n], as we are simulating dual polarization\nnoise = tf.complex(\n    tf.random.normal([2, n], 0.0, 1.0, 'float32'),\n    tf.random.normal([2, n], 0.0, 1.0, 'float32')\n)\noutput = amplifier([p_0, p_n, noise])\n\nprint('Output signal shape:', output.shape)\n```\n\nPlease remember to install the Sionna package and its dependencies before running this code."
"The `render_to_file()` function in the `Scene` class of the Sionna package is used to render the scene to a file. This function is a part of the integrated rendering system that visualizes the scene for the user. The scene can be previewed from an object's perspective or a top-down view. \n\nHere's a basic usage example of the `render_to_file()` function:\n```python\nscene = load_scene(sionna.rt.scene.munich)\nscene.render_to_file(\"scene.png\")\n```\n\nIn this example, `load_scene()` is used to load the integrated scene of Munich, and then the `render_to_file()` function is called on the `scene` object to render it to a PNG file named \"scene.png\".\n\nThe function can take several parameters including `cam`, `orientation`, `look_at`, `fov`, `background`, `clip_at`, `resolution`, and `antialiasing`. \n- The `cam` parameter is used to for the position and direction of the camera. \n- The `orientation` parameter is used to for the orientation of the camera. \n- The `look_at` parameter is used to specify the point to look at. \n- The `fov` parameter is used to specify the vertical field of view in degrees. \n- The `background` parameter is used to specify the background color in hex format. \n- The `clip_at` parameter is used to specify the clipping planes in world coordinates. \n- The `resolution` parameter is used to specify the image resolution. \n- The `antialiasing` parameter enables anti-aliasing for the rendered image.\n\nPlease note that this function won't work within a Jupyter notebook environment as it tries to open an external file."
"Unfortunately, the provided context does not include any information about a function named \"wrap_angle_0_360\". Thus, it's not possible to describe this function in detail based on the provided context. \n\nPlease provide additional details or source code related to the \"wrap_angle_0_360\" function if available. This will significantly help to understand what the function does and how to use it."
"Here is the definition of ChannelModel: sionna.channel.ChannelModel\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/channel/channel_model.html#ChannelModel)  \n\nsource code:\n```python\nclass ChannelModel(ABC):\n    # pylint: disable=line-too-long\n    r\"\"\"ChannelModel(InputSamples, generate_paths, generate_lasts_paths, generate_delays, generate_k_factors, num_time_samples, sampling_frequency, **kwargs)\n\n    This class is an abc for channel models.\n\n    Any channel model which generates channel impulse responses of a fixed\n    length must implement this callable class.\n\n    Input samples are defined as :math:`\\left\\langle \\text{batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols, fft_size, 2} \\right\\rangle` in the case of an OFDM link, and  :math:`\\left\\langle \\text{batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_time_samples} \\right\\rangle`  in the case of a  time-domain link.\n\n    Output samples are defined by :math:`\\left\\langle \\text{batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_time_samples, l_tot} \\right\\rangle`, where :math:`l_\\text{tot}` is the sum of :math:`L`, the maximum number of paths generated by Paths.\n\n    Additional outputs such as delays and path gains can be computed and generated\n    by the ChannelModel, but must be enabled by the ``return_paths`` parameter.\n\n    An instance of ChannelModel can be used as the ``channel_model`` parameter\n    in the :class:`~sionna.channel.OWDChannel` and\n    :class:`~sionna.channel.RayleighBlockFading` classes, or in the\n    :class:`~sionna.channel.tr38901.UMiChannel`,\n    :class:`~sionna.channel.tr38901.UMaChannel`, and\n    :class:`~sionna.channel.tr38901.RMaChannel`  sub-classes.\n\n    Parameters\n    ----------\n    input_samples : `Tensor` of `tf.complex`\n        Input samples forming the transmitted signal.\n\n    generate_paths : `bool`\n        If enabled, the channel model will generate for each batch\n        example either 0, 1 or multiple pairs of (a_k, tau_k)\n        where k is the path index, `a_k` the path gain and `tau_k` the path delay.\n\n    generate_lasts_paths : `bool`\n        If enabled, the channel model will generate\n        path coefficients and path delays for all paths.\n\n    generate_delays : `bool`\n        If enabled, the channel model will generate for each batch\n        example either 0, 1 or multiple delays.\n\n    generate_k_factors : `bool`\n        If enabled, the channel model will generate\n        K-factors for all paths.\n\n    num_time_samples : `int`\n        Number of time samples forming the channel realization.\n        This parameter is only needed if the channel model\n        is time-domain based.\n\n    sampling_frequency : `float`\n        Sampling frequency [Hz] of the channel model.\n        This parameter is only needed if the channel model\n        is time-domain based.\n\n    Returns\n    -------\n    (a, tau, a_shape, tau_shape, delays, path_gains, k_fading) :\n            Tuple:\n\n        a : `Tensor` of shape\n            :math:`\\left\\langle \\text{batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols, fft_size, 2, num_time_samples} \\right\\rangle` or\n            :math:`\\left\\langle \\text{batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_time_samples, l_{\\text{max}}} \\right\\rangle`  if ``generate_lasts_paths`` is `True`, where `l_{\\text{max}}` is the maximum\n            number of paths a given batch example can have.\n\n        tau : `Tensor` of shape\n            :math:`\\left\\langle \\text{batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols, fft_size, num_time_samples, num_ofdm_symbols} \\right\\rangle` or\n            :math:`\\left\\langle \\text{batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_time_samples, l_{\\text{max}}, num_time_samples} \\right\\rangle` if ``generate_lasts_paths``\n            is enabled.\n\n        a_shape : `Tensor` of shape\n            :math:`\\left\\langle \\text{batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols, fft_size, 2, num_time_samples, num_ofdm_symbols} \\right\\rangle` or\n            :math:`\\left\\langle \\text{batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_time_samples, l_{\\text{max}}, 2, num_time_samples, num_ofdm_symbols} \\right\\rangle` if\n            ``generate_lasts_paths`` is enabled. This is the broadcastable shape of `a`.\n\n        tau_shape : `Tensor` of shape\n            :math:`\\left\\langle \\text{batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols, fft_size, num_time_samples} \\right\\rangle` or\n            :math:`\\left\\langle \\text{batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_time_samples, l_{\\text{max}}, num_time_samples} \\right\\rangle` if ``generate_lasts_paths``\n            is enabled. This is the unrolled shape of `tau`.\n\n        delays : `Tensor` of shape [batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols, fft_size, num_time_samples, num_ofdm_symbols, l_tot].\n            Delays [s] of the channel paths. Only returned if ``return_delays``\n            is `True`.\n\n        path_gains : `Tensor` of shape [batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols, fft_size, num_time_samples, num_ofdm_symbols, l_tot].\n            Path gains [linear scale] of the channel paths. Only returned if\n            ``return_gains`` is `True`.\n\n        k_fading : `Tensor` of shape [batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols, fft_size, num_time_samples].\n            Fading coefficients for each path. Only returned if\n            ``return_fading`` is `True`.\n    \"\"\"\n\n    def __init__(self, input_samples, generate_paths, generate_lasts_paths,\n                 generate_delays, generate_k_factors, num_time_samples,\n                 sampling_frequency, dtype=tf.complex64, **kwargs):\n\n        super().__init__(**kwargs)\n        self._samples = input_samples\n        self._generate_paths = generate_paths\n        self._generate_lasts_paths = generate_lasts_paths\n        self._generate_delays = generate_delays\n        self._generate_k_factors = generate_k_factors\n        self._num_time_samples = num_time_samples\n        self._sampling_frequency = sampling_frequency\n        self._dtype = dtype\n\n        assert sampling_frequency>0,\\\n            \"The sampling frequency must be positive.\"\n        self._sample_time = 1./sampling_frequency\n\n        self._check_input_parameters()\n\n    @property\n    def num_tx_ant(self):\n        \"\"\"Number of transmit antennas\"\"\"\n        return self._num_tx_ant\n\n    @property\n    def num_rx_ant(self):\n        \"\"\"Number of receive antennas\"\"\"\n        return self._num_rx_ant\n\n    @property\n    def num_ofdm_symbols(self):\n        \"\"\"Number of OFDM symbols\"\"\"\n        return self._num_ofdm_symbols\n\n    @property\n    def fft_size(self):\n        \"\"\"FFT size\"\"\"\n        return self._fft_size\n\n    @property\n    def num_time_samples(self):\n        \"\"\"Number of time samples\"\"\"\n        return self._num_time_samples\n\n    @property\n    def sampling_frequency(self):\n        \"\"\"Sampling frequency [Hz]\"\"\"\n        return self._sampling_frequency\n\n    @property\n    def two_way(self):\n        \"\"\"Set to `True` if the channel models implements the two-way channel\n        model. See :class:`~sionna.channel.TwoWayChannel`.\"\"\"\n        return False\n\n    @property\n    def tx_location(self):\n        r\"\"\"Transmit antenna location :math:`(x,y,z)` [m] with respect to the\n        center of the scene\"\"\"\n        return self._tx_location\n\n    @property\n    def rx_location(self):\n        r\"\"\"Receive antenna location :math:`(x,y,z)` [m] with respect to the\n        center of the scene\"\"\"\n        return self._rx_location\n\n    @property\n    def center(self):\n        \"\"\"Center of the scene :math:`(x,y,z)` [m]\"\"\"\n        return self._center\n\n    @property\n    def orientation(self):\n        \"\"\"Orientation of the scene. Returns a tuple of three angles\n        :math:`\\alpha`, :math:`\\beta`, and :math:`\\gamma` [rad]\"\"\"\n        return self._orientation\n\n    @property\n    def range(self):\n        \"\"\"Distance between the transmitter and receiver [m]\"\"\"\n        return self._range\n\n    @property\n    def ground_reflection(self):\n        \"\"\"Set to `True` to enable ground reflection\"\"\"\n        return self._ground_reflection\n\n    @property\n    def specular_reflection(self):\n        \"\"\"Set to `True` to enable specular reflection\"\"\"\n        return self._specular_reflection\n\n    @property\n    def diffraction(self):\n        \"\"\"Set to `True` to enable diffraction\"\"\"\n        return self._diffraction\n\n    @property\n    def scattering(self):\n        \"\"\"Set to `True` to enable (random) scattering\"\"\"\n        return self._scattering\n\n    def preprocess(self, dtype):\n        r\"\"\"Preprocess channel coefficients for use by the channel emulator.\n\n        This method must be implemented by concrete classes that implement\n        a channel model.\n\n        Input\n        ------\n        dtype : tf.DType\n            Complex datatype used for internal processing and output.\n\n        Output\n        -------\n        a, tau :\n            Channel impulse responses and corresponding delays.\n\n            If ``generate_paths`` is `True` and ``generate_lasts_paths`` is `False`:\n\n            - ``a`` is a tensor of shape\n              `[,num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols,...,2, num_time_samples]`\n              containing the path coefficients. The complex datatype is as\n              selected by ``dtype``.\n            - ``tau`` is a tensor of shape\n              `[,num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols,...,num_paths]`\n              containing the (random) path delays [s]. The path delays are\n              sorted in increasing order. The proper sampling of the channel\n              impulse response is then done through interpolation\n              using e.g. the ``sionna.signal.ContinuousChannel`` layer.\n\n        If ``generate_lasts_paths`` is `True`:\n\n            - ``a`` is a tensor of shape\n              `[,num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols,...,2, num_time_samples]`\n              containing the last scattering coefficient. The complex datatype\n              is a real datatype which is selected by ``dtype``.\n            - ``tau`` is a tensor of shape\n              `[,num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols,...,num_time_samples,num_rx_ant, num_rx_ant]`\n              containing the last scattering matrix. The last\n              scattering matrix is such that if a signal with complex\n              amplitude ``a`` is scattered to path ``i`` with polarization\n              vector :math:`\\mathbf{p}_i` and polarized according to the\n              polarization vector in the direction of the receiver antenna\n              array :math:`\\mathbf{a}_\\text{R}`, the resulting\n              complex coefficient of the i-th path is\n\n              .. math::\n                  a_i = \\mathbf{p}_i^\\mathsf{H}\\mathbf{a}_\\text{R}.\n\n              This corresponds to\n\n              .. math::\n                  a_i =  \\mathbf{p}_i^\\mathsf{H}\n                  \\left(\\mathbf{G}^\\mathsf{H}\\mathbf{D}\\mathbf{a}_\\text{T} + \\mathbf{R}\\mathbf{S}\\mathbf{a}_\\text{R} \\right)\n\n              where :math:`\\mathbf{G}`, :math:`\\mathbf{D}`, and :math:`\\mathbf{R}`\n              are the geometric, diffraction, and reflection matrices,\n              respectively (see [3GPP TR 38.901, \u201cStudy on channel model for\n              frequencies from 0.5 to 100 GHz\u201d, Release 16.1]).\n\n              Note that the actual complex antenna array pattern is\n              applied when the antenna array is applied through e.g.\n              ``AntennaArray```.\n\n              The parameters ``num_rx``, ``num_rx_ant``, ``num_tx``, and\n              ``num_tx_ant```, ``num_ofdm_symbols``, and ``num_time_samples```\n              are related to the resource grid setting.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def max_num_time_samples(self):\n        \"\"\"Maximum number of time samples occupied by the channel model.\n\n        Depending on the generation method, the number of time samples occupied\n        by a channel model can differ from the resource grid-based duration.\n        This parameter gives the maximum number of time samples occupied by\n        any channel model.\n        \"\"\"\n        return self._num_time_samples\n\n    def new_batch(self, batch_size, complex_last_dim):\n        \"\"\"Allocate a new batch of tensors.\n\n        This method is used by the channel emulator to allocate a batch of\n        tensors whose shape can differ from the shape of the channel coefficients.\n\n        The new batch size is given as an argument, and the batch size is\n        implicitly given by the shape of the last dimension, which must be\n        a multiple of four.\n\n        For example, if the shape of the last dimension is\n        `[,num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols,...,num_time_samples]`,\n        then the batch size is `tf.shape[the_last_dim]/4`.\n\n        Input\n        -----\n        batch_size : int\n            Batch size\n\n        complex_last_dim : bool\n            If `True`, the last dimension is complex-valued. If `False`, it is\n            real-valued. This influences the way the batch size is computed.\n\n        Output\n        ------\n        batch : :`[batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols,...,num_time_samples]`.\n            Batch of new_tensors of shape `[,batch_size,]`. The first\n            dimension is always`batch_size`.\n        \"\"\"\n        if complex_last_dim:\n            new_batch_size = tf.shape(self._samples)[-2]\n        else:\n            new_batch_size = tf.shape(self._samples)[-1]\n\n        #batch_size = tf.cast(batch_size, tf.int32)\n        batch_size = tf.cast(batch_size, tf.int32)\n        new_batch_size = tf.cast(new_batch_size, tf.int32)\n\n        shape = tf.concat([tf.cast([-1], tf.int32),\n                           tf.cast([batch_size], tf.int32)], 0)\n\n        if complex_last_dim:\n            shape = tf.concat([shape, tf.cast([2], tf.int32)], 0)\n\n        batch = tf.tile(tf.expand_dims(self._samples, 0),\n                         [new_batch_size, 1, 1, 1, 1, 1, 1, 1])\n        batch = tf.gather(batch, axis=-2, indices=shape, batch_dims=0)\n        #batch = tf.repeat(batch, batch_size, 0)\n\n        return batch\n\n    @abstractmethod\n    def generate_lasts(self, batch_size):\n        r\"\"\"Generates a batch of last scattering paths.\n\n        The returned tensor ``lasts`` combines the path delays\n        and coefficients (complex-valued) of the last scattering\n        paths. The shape of ``lasts`` is\n        `[,num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols,...,num_time_samples,num_rx_ant, num_rx_ant]`\n        if ``generate_lasts_paths`` is enabled.\n\n        Input\n        -----\n        batch_size : int\n            Batch size\n\n        Output\n        ------\n        lasts : :`[batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols,...,num_time_samples,num_rx_ant, num_rx_ant]`.\n            Batch of last scattering matrices of shape `[,batch_size, num_time_samples, num_rx_ant, num_rx_ant]`.\n            Only output if ``generate_lasts_paths`` is enabled.\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def num_paths(self):\n        \"\"\"Returns the number of paths `num_paths` a such that\n        `a` and `tau` can be reconstructed as\n\n        ```python\n        a[:num_paths] = a\n        tau[:num_paths] = tau\n        ```\n\n        This property is only defined if `generate_paths` is `True`.\n        \"\"\"\n        pass\n\n    def adjust_delays(self, old_delays, old_weights, new_delays, new_weights):\n        \"\"\"Adjusts delays from the previous to the current ones.\n\n        This method is used by channel emulators that implement multi-step\n        processing and must adjust intermediate outputs to the channel output.\n\n        Input\n        ------\n        old_delays : [...num_time_samples, num_paths_old] tf.float\n            Delay tensors associated to the previous time step\n\n        old_weights : [...num_time_samples, num_paths_old] tf.complex\n            Weight tensors associated to the previous time step\n\n        new_delays : [...num_time_samples, num_paths_new] tf.float\n            Delay tensors associated to the current time step\n\n        new_weights : [...num_time_samples, num_paths_new] tf.complex\n            Weight tensors associated to the current time step\n\n        Output\n        -------\n        delays : [...num_time_samples, num_paths_new] tf.float\n            Adjusted delay tensor that can be compared to ``new_delays``.\n\n        weights : [...num_time_samples, num_paths_new] tf.complex\n            Adjusted weight tensor that can be compared to ``new_weights``.\n        \"\"\"\n        selected = tf.range(new_weights.shape[-1])\n\n        # For each new path...\n        for i in range(new_weights.shape[-1]):\n\n            #...find the best candidate among the old paths\n            max_corr = tf.cast(0, dtype=tf.float32)\n            best_j = -1\n            best_i = -1\n           "
"Here is the detailed information of sionna.rt.Scene.render_to_file: \n\nRenders the scene to a file.\n\nThe function first looks for a camera in the scene named \"render_cam\". If such a camera is not found, a new camera is automatically added and its viewpoint is set to the orignal viewpoint of the scene camera. The function utilizes the load() method of Scene, which is used to load external files including scenes.\n\nNote that the render-to-file is not interactive and does not return a preview of the rendered scene.\n\nInput\n    filename (str) \u2013 Name of the file to save the rendering to. The filename extension determines the output file format. Accepts formats such as PNG, JPEG, and others.\n    cam_name (str) \u2013 Name of the camera to use for the rendering. Defaults to \"preview_cam\" if set to None.\n\nReturns\n    path to the saved file (str) \u2013 This is the file to which the rendering was saved. It cannot be viewed directly in the Jupyter notebook."
"To configure DeepMIMO for use with Sionna, follow these steps:\n\n1. **Import necessary libraries and configure GPU settings**: Import the necessary libraries suchjson, tensorflow, and keras. Configure the GPU to use only a single GPU and allocate only as much memory as needed.\n\n2. **Load the required Sionna components**: Load the required Sionna components including DeepMIMO, StreamManagement, Transmitter, DataStream, and Receiver. All of these components can be imported from `sionna.mimo`.\n\n3. **Configure the DeepMIMO dataset**: The DeepMIMO dataset needs to be configured to generate the channels between the bases stations and the users. The dataset should provide the channels in the form of a sparse deep convolutional neural network (CNN) channel model. The channel model can be activated for the DeepMIMO UE positions by calling `DeepMIMO(DeepMIMO_MODEL, active_ues)`. Ensure the `active_ues` parameter contains the indices of all UE/NR-TBS to be included in the simulation.\n\n4. **Set up Sionna models**: Set up Sionna models using the DeepMIMO dataset. The Sionna models should include a nullspace detector, a K-best detector, and a linear equalizer. The number of detected streams should be equal to the number of transmitted streams.\n\n5. **Define the simulation parameters**: Define the simulation parameters including the number of UEs, the number of BS antennas, the number of transmitted streams, the number of receiver antennas, the UE signal-to-noise ratio (SNR), the batch size, and the number of Monte Carlo iterations.\n\n6. **Simulate and evaluate**: Simulate MIMO transmissions over the flat-fading channel and evaluate the performance. The performance can be evaluated using bit-error rate (BER) and block-error rate (BLER). \n\nRemember to use the Sionna APIs correctly and ensure that the provided context does not infringe their integrity."
"To use the \"interleaver module\" in the Sionna package to permute tensors, follow these steps:\n\n1. First, you need to import the required modules from the Sionna package. You will need the \"Interleaver\" module which can be imported as shown below:\n\n    ```python\n    from sionna.fec.interleaving import Interleaver\n    ```\n\n2. Decide on the size of the interleaver, which is a two-element tuple. For example:\n\n    ```python\n    interleaver_size = (4, 4) \n    ```\n\n3. Create an instance of the Interleaver module. The only required parameter is the `perm_seq` which is the permutation sequence.\n\n    ```python\n    interleaver = Interleaver(perm_seq=interleaver_size)\n    ```\n\n4. To interleave a tensor, call the `interleaver` instance like a function with the tensor to be interleaved as the only input. For example, if `x` is your tensor:\n\n    ```python\n    x_int = interleaver(x)\n    ```\n\n5. To then deinterleave the tensor, call the `interleaver` instance again with the interleaved tensor as the input:\n\n    ```python\n    x_deint = interleaver(x_int)\n    ```\n\nThe \"interleaver module\" can also be used within a Keras model or function. In this case, you do not need to instantiate the module. You would use something like:\n\n    ```python\n    from sionna.fec.interleaving import Interleaver\n    \n    interleaver = Interleaver(perm_seq=(4,4))\n    \n    class YourModel(Keras.Model):\n        def __init__(self):\n            .\n            .\n            .\n            self.interleaver = interleaver\n            .\n            .\n        def call(self, inputs):\n            .\n            .\n            x_int = self.interleaver(inputs)\n            .\n            .\n            return x_int\n    ```\n\nRemember that the `interleaver` instance is used to permute the tensor and the same instance must be used to unpermute it."
"The `MMSEPICDetector` function is part of the 'Sionna' signal detection module and is used for Multiple-Input Multiple-Output (MIMO) signal detection with Pre-coding using Minimum Mean Square Error (MMSE) channel estimation. It implements multi-stream detection, taking as input a batch of received signal vectors, the channel matrices, and optionally, a transmitter precoding matrices. The function is robust to arbitrary column permutations of the channel matrix and assumes that the transmit precoding matrices are chosen to orthogonally encode the data.\n\nThe function outputs estimated symbols, effective noise variance estimates, and hard-symbol decisions for each stream. If `output_bits` is `True`, it outputs LLR values instead of hard-decisions for every bit of every stream.\n\nHere is the parameter explanation:\n\n- `output_bits` (bool): If set to `True`, the detector outputs LLR values for every bit of every stream instead of symbols. The default is `False`.\n- `y` ([batch_size, num_rx, num_rx_ant, num_ofdm_symbols, fft_size], tf.complex): The received signals.\n- `h` ([batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols, num_effective_subcarriers], tf.complex): The channel matrices.\n- `g` ([batch_size, num_tx, num_tx_ant, num_ofdm_symbols, fft_size], tf.complex): The precoding matrices. If `precoding` is \"none\", `g` is ignored and this input can be instantiated as `None`.\n- `precoding` (One of {\"none\", \"zf\", \"mf\", \"lmmse\"}, str): The precoding method used. It is ignored if `g` is `None`. Possible values are \"none\" for no precoding, \"zf\" for zero-forcing precoding, \"mf\" for matched-filter precoding, and \"lmmse\" for precoding using linear minimum mean-square error.\n- `num_iter` (int): The number of detector iterations. It is ignored if `detector` is not \"mmse-pic\".\n- `state` (LSTMState): The state of an LSTM layer is required for recursive channel estimation and interference cancellation. If no explicit state is provided, the function assumes the initial state is `None` and creates a new state based on the channel parameters.\n- `output_dtype` (One of {tf.complex, tf.float}, tf.DType): The data type of the output. It is set to `tf.float` by default.\n\nAnd here is how you instantiate this function:\n\n```python\ndetector = MMSEPICDetector(output_bits=False,\n                            num_iter=2,\n                            precoding=\"lmmse\",\n                            output_dtype=tf.complex)\n```"
"Here is the definition of RaisedCosineFilter: sionna.signal.RaisedCosineFilter(span_in_symbols, samples_per_symbol, beta, window=None, normalize=True, trainable=False, dtype=tf.float32, **kwargs)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/signal/filter.html#RaisedCosineFilter)\n\nsource code:\n```python\nclass RaisedCosineFilter(Filter):\n    # pylint: disable=line-too-long\n    r\"\"\"RaisedCosineFilter(span_in_symbols, samples_per_symbol, beta, window=None, normalize=True, trainable=False, dtype=tf.float32, **kwargs)\n\n    Layer for applying a raised-cosine filter of ``length`` K\n    to an input ``x`` of length N.\n\n    The filter is applied through discrete convolution.\n\n    The filter is optionally windowed.\n\n    The `dtype` of the output is `tf.float` if `x` is `tf.float` otherwise, `tf.complex`.\n\n    Parameters\n    ----------\n    span_in_symbols: int\n        Filter symbol span as measured from the peak to the symbol zero\n        crossing.\n\n    samples_per_symbol: int\n        Number of samples per symbol, i.e., the oversampling factor.\n\n    beta : float\n        Roll-off factor.\n        Must be in the range :math:`\\left[0,1\\right]`.\n\n    window : Window or string ([\"hann\", \"hamming\", \"blackman\"])\n        Instance of :class:`~sionna.signal.Window` that is applied to the filter.\n        Alternatively, a string indicating the window name can be provided.\n        In this case, the chosen window will be instantiated with the\n        default parameters. Custom windows must be provided as instance.\n\n    normalize: bool\n        If `True`, the filter is normalized to have unit power.\n        Defaults to `True`.\n\n    trainable: bool\n        If `True`, the filter coefficients are trainable variables.\n        Defaults to `False`.\n\n    dtype: tf.DType\n        The `dtype` of the filter.\n        Defaults to `tf.float32`.\n\n    Input\n    -----\n    x : [..., N], tf.complex or tf.float\n        The input to which the filter is applied.\n        The filter is applied along the last dimension.\n\n    Output\n    ------\n    y : [...,M], tf.complex or tf.float\n        Filtered input.\n        It is `tf.float` only if `x` is `tf.float`.\n    \"\"\"\n\n    def __init__(self,\n                 span_in_symbols,\n                 samples_per_symbol,\n                 beta,\n                 window=None,\n                 normalize=True,\n                 trainable=False,\n                 dtype=tf.float32,\n                 **kwargs):\n\n        assert 0 <= beta <= 1, \"beta must be from the intervall [0,1]\"\n        self._beta = beta\n\n        super().__init__(span_in_symbols,\n                         samples_per_symbol,\n                         window=window,\n                         normalize=normalize,\n                         trainable=trainable,\n                         dtype=dtype,\n                         **kwargs)\n\n    @property\n    def beta(self):\n        \"\"\"The roll-off factor.\"\"\"\n        return self._beta\n\n    @property\n    def _coefficients_source(self):\n        h = raise_cosine(self._length,\n                          self._samples_per_symbol,\n                          self.beta)\n        return h\n```"
"The PAM2QAM class in Sionna is used for the mapping of a PAM-modulated signal to a QAM-modulated signal. It supports two methods for this process. The first method is based on bit demapping, which it implements by using the same approach as the \"i.i.d.\" method for Bit2Symbol. The second method is more general and applies coordinate rotation.\n\nThe PAM2QAM class helps achieve code execution by effectively translating a sequence of PAM symbols to a sequence of QAM symbols. This can be utilized in any communication system where PAM modulation is used at the transmitter. \n\nHere is a usage example given in the context:\n\n```python\npam = PAM(num_bits_per_symbol=2, return_indices=True)\nqam = QAM(num_bits_per_symbol=4, return_indices=True)\n\n# Generate a batch of random QAM symbols\nx_qam, i_qam = qam([B, AWGN.var()])  # No need to specify x and i explicitly\nx_pam, i_pam = pam([x_qam, i_qam])  # Map to PAM\n\n# Now map the PAM symbols to QAM\nx_hat, _ = pam2qam([x_pam, i_pam])\n```\n\nIn this code, first a PAM modulated signal is generated from a given PAM model (pam). Then, this PAM modulated signal is transformed into a QAM modulated signal using the PAM2QAM class. The generated QAM symbols (x_hat) can then be passed to a demodulator for further processing.\n\nIt is important to note that the context provides no direct information about how the PAM2QAM class is implemented, but describes what it is used for and how it can be used in code."
"To configure StreamManagement for a simple uplink scenario using Python, you need to define four variables:\n\n1. `num_tx`: The number of transmitters.\n2. `num_rx`: The number of receivers.\n3. `num_streams_per_tx`: The number of streams per transmitter. In this scenario, each transmitter sends one stream.\n4. `rx_tx_association`: An array that determines which transmitter is associated with which receiver. If `rx_tx_association[i][j]` is 1, it means that receiver j is associated with transmitter i. \n\nHere is how you can define these variables:\n\n```python\nimport numpy as np\n\nnum_tx = 4  # 4 transmitters\nnum_rx = 1  # 1 receiver\nnum_streams_per_tx = 1  # 1 stream per transmitter\n\n# rx_tx_association is a binary array determining the association between receivers and transmitters.\n# If rx_tx_association[i][j] = 1, it means that receiver j is associated with transmitter i.\nrx_tx_association = np.array(  [[1, 0, 0, 0],\n                                [0, 1, 0, 0],\n                                [0, 0, 1, 0],\n                                [0, 0, 0, 1]])\n```\n\nNote that in more complex scenarios, with more transmitters and receivers, and each transmitter sending multiple streams, the `rx_tx_association` array would need to be defined more carefully."
"In the context of higher order modulation, constellations and bit-channels are crucial concepts. Higher order modulation refers to modulation schemes where each symbol represents multiple bits. \n\nA constellation is the set of possible symbols in the signal. In higher order modulation, a constellation is composed of points in the complex plane, with each point representing a symbol. The distance between two points in the constellation is proportional to the difference between the corresponding bits that the two symbols represent. \n\nA bit-channel describes the transmission of bits or digital signals over a communication medium. It consists of a sender and a receiver. The sender encodes the data into symbols from the constellation, and the receiver decodes the received signal back into the original data. \n\nIn higher order modulation, the bit-channel is effectively a discrete channel for each bit that is transmitted. Each bit can be represented by a binary label, and the probability of error in the bit-channel can be described by a binary symmetric channel (BSC), where errors only occur with some probability $p$. \n\nThe concept of constellations and bit-channels is crucial for understanding how data is represented and transmitted in higher order modulation schemes, and for designing efficient encoding and decoding algorithms."
"The \"pam\" in the sionna mapping refers to the \"pulse amplitude modulation\" function used in the mapping module. This function is part of the Sionna symbol module which includes classes and functions for constellation design and management.\n\nThe \"pam\" function in this context takes two inputs. The first input, \"num_bits_per_symbol,\" is an integer that specifies the number of bits per constellation symbol. The second input, \"return_indices,\" is a boolean that determines whether to return indices indicating which constellation points belong to which symbol.\n\nThe function generates a (scaled) constellation for PAM (Pulse Amplitude Modulation) mapping. This constellation is a complex-valued vector where the real and imaginary parts of each element constitute a point in the signal constellation. The number of elements in this vector is equal to 2 raised to the power of the \"num_bits_per_symbol.\" \n\nThe function returns the scaled constellation and, if \"return_indices\" is set to True, also returns a 2-dimensional tensor containing the constellation indices. The output indicating the constellation points is a complex-valued vector where the real and imaginary parts constitute a point in the signal constellation represented by the PAM constellation. \n\nMoreover, the transformation to constellation symbols is achieved through the function sionna.mapping.pam2qam, where the PAM constellation is theoretically flipped along the imaginary axis, thereby becoming the QAM constellation. This transformation is equivalent to adding a phase shift of half the symbol index M/2 to the QAM constellation."
"Assumptions to fulfill when using \"List2LLR\" class in the Sionna package are:\n\n1. List2LLR class cannot be used as it is. You need to create an instance of this class first with one of the following methods:\n   a. Using a static method provided by the class: `List2LLR.with_quantizer(LLR_quantizer, rate)`.\n   b. Or, by providing your own LLR quantization function `LLR_quantizer` and the quantization rate `rate`.\n\n2. The actual LLR quantization function `LLR_quantizer` is expected to take as input a tensor of logits and to return a tensor of LLRs. The function should take two input parameters: `logits` and `hard_symbols`, where `logits` is a tensor of logits, and `hard_symbols` is an optional parameter that provides the indices of the symbols (as a tensor of booleans) that are quantized to zero (i.e., a so-called hard-decision).\n\n3. Assure that the List2LLR instance is used within a scope where a TensorFlow graph is being run.\n\n4. The \"List2LLR\" class assumes that the option `raise_negative` is set to `False` by its base class \"List2LLR\". This means that any negative LLRs resulting from the conversion of soft-symbols to LLRs will be clipped to 0. It is assumed that all LLRs resulting from the conversion of soft-symbols to LLRs are properly scaled.\n\n5. In the process of generating LLRs from soft-symbols, it is assumed that a noise variance estimate is available. This estimate might originate from the demodulator and can be provided as an additional input to the \"List2LLR\" instance.\n\n6. The class produces (K, no) LLR values for all possible messages. It is assumed that these LLR values will be used as input to a channel decoder, for example, a LDPC decoder.\n\nPlease note that the Assumption 1 and 2 are based on the provided context that how to use the \"List2LLR\" class, otherwise additional validations may be required."
"The `MMSEPICDetector` class in the Sionna MIMO API is used for Equalization of streams in a MIMO network. It is typically used as a pre-coder or equalizer for detecting signals in a Multi-Input Multi-Output (MIMO) communication system. The detector is robust and works well in most scenarios, but might not be the absolute best solution in terms of bit or symbol error rate, especially for higher order MIMO systems.\n\nThe class takes several key parameters:\n\n- `output`: The type of output, either bits or symbols.\n- `demapping_method`: The demapping method used, options include \"maxlog\" and \"app\".\n- `num_iter`: The number of iterations for the equalization process.\n- `constellation_type`: The type of constellation used, options are \"qam\", \"pam\", or \"custom\". For custom constellations, an instance of Constellation must be provided.\n- `num_bits_per_symbol`: The number of bits per constellation symbol, for example 4 for QAM16. \n- `constellation`: An instance of Constellation or None. If None, `constellation_type` and `num_bits_per_symbol` must be provided.\n- `hard_out`: Boolean value to indicate whether the equalizer should output hard-decided values or soft-values.\n- `dtype`: The data type used for internal calculations and the output data type.\n\nThe conditions under which the `MMSEPICDetector` is used are:\n\n- The function is used in MIMO networks where the number of transmitter antennas is larger than the number of receiver antennas.\n- It is not the top choice for high mobility scenarios, or when the channel impulse response is long, due to potential high correlation between channel estimates.\n- It assumes that the transmitter precodes independent data streams to the different transmitters.\n\nThe exact use of the `MMSEPICDetector` class may depend on the specific requirements of the MIMO network, and on the desired trade-off between performance and computational cost."
"The QAM2PAM class within the `sionna.mapping` module provides a layer that can split a sequence of QAM symbol indices into two sequences of PAM symbol indices. It implements the transformation from a QAM constellation, where the symbols are identified by indices [0, 1, 2, 3, ...], to a PAM constellation, where the symbols are again identified by indices [0, 1, 2, 3, ...].\n\nThe QAM2PAM class takes a single argument, which is the dimension of the QAM constellation. This is an instance of tensorflow's `Layer` class, and so can be used in the definition of a neural network.\n\nThe transformation from QAM to PAM is achieved as follows. For each QAM symbol, the class computes the corresponding two PAM symbols as the real and imaginary parts of the symbol's index. This is done by the complex-valued QAM symbol index multiplied by a scaling factor, where the scaling factor is `1/\u221a2`. The two PAM symbols are then returned as a pair of values.\n\nHere is the implementation of the QAM2PAM class:\n\n```python\nclass QAM2PAM(Layer):\n    def __init__(self, num_pam_symbols, **kwargs):\n        self.num_pam_symbols = num_pam_symbols\n        super().__init__(**kwargs)\n\n    def build(self, batch_dim, num_qam_symbols):\n        self.batch_dim = batch_dim\n        self.num_qam_symbols = num_qam_symbols\n        super().build(input_shape)\n\n    def call(self, qam_symbols):\n        # Reshape tensor to shape [...,num_qam_symbols/2, 2]\n        input_shape = tf.shape(qam_symbols)\n        new_shape = tf.concat([input_shape[:-1], [num_pam_symbols//2, 2]], axis=0)\n        tmp_pam_symbols = tf.reshape(qam_symbols, new_shape)\n\n        # Split tensor into two tensors of PAM symbols\n        pam1 = tmp_pam_symbols[..., 0, 0]\n        pam2 = tmp_pam_symbols[..., 1, 0]\n\n        # Put both tensor into one tensor with shape [...,num_pam_symbols, 2]\n        pam_symbols = tf.stack([pam1, pam2], axis=-2)\n        final_shape = tf.concat([input_shape[:-1], [num_pam_symbols, 2]], axis=0)\n        pam_symbols = tf.reshape(pam_symbols, final_shape)\n\n        return pam_symbols\n```\n\nTo use this class, you would first create an instance of it, specifying the number of PAM symbols:\n\n```python\nqam2pam = QAM2PAM(num_pam_symbols=16)\n```\n\nAnd then you would use this instance in the definition of a Keras model:\n\n```python\nmodel = tf.keras.Model(inputs=[channel, snr_db], outputs=decoded_pam)\n```\n\nWhere `channel` and `snr_db` are tensors that are the transmission signal and the SNR in dB, respectively. The `decoded_pam` tensor would be the result of passing the received QAM symbols through the corresponding `PAM2PAM` layer."
"To verify that the generator matrix and parity-check matrix are orthogonal in GF(2) with the \"verify_gm_pcm\" function of the Python package Sionna, you need to provide the generator matrix (gm) and the parity-check matrix (pcm) as inputs.\n\nHere is a step-by-step guide:\n\n1. First, import the necessary packages and functions. You will need the \"verify_gm_pcm\" function from Sionna.\n\n```python\nfrom sionna.fec.utils import verify_gm_pcm\n```\n\n2. Next, define your generator matrix (gm) and parity-check matrix (pcm). These matrices should be binary matrices (each element is an integer 0 or 1).\n\n```python\n# Example binary generator matrix of shape [k, n]\ngm = [[1, 0, 1, 1, 0],\n      [0, 1, 1, 0, 1],\n      [1, 1, 0, 0, 1]]\n\n# Example binary parity-check matrix of shape [n-k, n]\npcm = [[0, 1, 1, 0, 1],\n       [1, 1, 0, 1, 0],\n       [1, 0, 0, 1, 1]]\n```\n\n3. Call the \"verify_gm_pcm\" function with your gm and pcm as arguments.\n\n```python\nis_orthogonal = verify_gm_pcm(gm, pcm)\n```\n\nThe function will return a boolean, \"is_orthogonal,\" that indicates whether the generator matrix and parity-check matrix are orthogonal. If the matrices are orthogonal, this function will return True, otherwise, it will return False.\n\nPlease note that the generator matrix (gm) and parity-check matrix (pcm) must be of the appropriate form and valid to represent a valid code. For the encode/decode process of the code, the ``BinaryLinearEncoder`` and ``BinaryJointDecoder`` classes can be used, respectively, as described in the context provided."
"To import the `LinearEncoder` class from the `sionna.fec.linear` module, you can use the the following line of Python code:\n\n```python\nfrom sionna.fec.linear import LinearEncoder\n```\n\nThis statement will make the `LinearEncoder` class available in your current Python namespace, allowing you to instantiate an encoder object for the encoding of systematic or non-systematic linear binary block codes as described in the provided context."
"The `OFDMDetectorWithPrior` class in the Sionna OFDM module is a layer for detecting symbols in an OFDM MIMO system, assuming prior knowledge of the resource grid. This class extends the `OFDMDetector` class from the same module. \n\nKey parameter:\n\n- `resource_grid`: An instance of the `ResourceGrid` class. It defines the structure and mapping of OFDM symbols to subcarriers.\n\nThe `resource_grid` parameter is used to specify certain properties like the number of OFDM symbols, FFT size, subcarrier spacing, number of transmitters, number of streams per transmitter, number of guard carriers, DC carrier location, pilot pattern, pilot OFDM symbol indices, and indices of the OFDM symbols used for transmission of reference signals. \n\nThe `OFDMDetectorWithPrior` receives the post-DFT received signals and the prior information of all data streams to compute per resource element log-likelihood ratios (LLRs). It assumes that a \"savitzky-golay\" interpolation method is used to estimate the channel frequency response for data and pilot-carrying resource elements. \n\nThe layer requires that you explictly provide the 'method' and 'rg' parameters, and can either be constructed from instance of `ResourceGrid` or from a instance of `ResourceGridAndPrior`. The method used for channel estimation, `method`, can be 'lmmse', 'lmmse-prior', or 'ngert'. The layer implements different interpolation and extrapolation methods for the channel estimates. \n\nA few things to note, the instruction does not provide importable class, and does not give a direct answer based on the code what the `scipy.stats.truncate_normal` does. The `OFDMDetectorWithPrior` is also said to assume that a prior of every  stream is provided, and P to be a tensor of shape [num_tx, num_streams_per_tx, num_ofdm_symbols, fft_size] representing the prior of all streams. It is not clear if it will throw an error if prior information for all streams is not provided. \n\nThis class is designed to work in a TensorFlow environment, and hence some methods such as `call` implement the operations using TensorFlow function and classes."
"The \"bin2int_tf\" function is used in Sionna's channel coding module for converting a binary tensor into an integer tensor. Here is how it is used:\n\n```python\nfrom sionna.fec.utils import bin2int_tf\n\n# \"coded_seq\" is a binary tensor, and \"integers\" is the corresponding integer tensor that we want to obtain.\nintegers = bin2int_tf(coded_seq)\n```\n\nIn this snippet, we import the \"bin2int_tf\" function and then simply call it with the binary tensor \"coded_seq\". The function converts each binary sequence in \"coded_seq\" to its corresponding integer representation.\n\nPlease note that the context provided does not indicate the implementation details of the \"bin2int_tf\" function. The actual function may take input parameters or have error handling that is not shown in the context. Please refer to the official Sionna's documentation or source code for complete and accurate description of the function."
"In the given context, we have a usage of the `TB5GScrambler` class from the `scrambling` module in 5G NR scrambling. This class is used to enable data rate compatible transport block (TB) scrambling, as defined in 3GPP TS 38.211 and TS 38.331.\n\nHere's a summarized step-by-step guide on how to use the `TB5GScrambler`:\n\n1. Import the necessary modules and classes.\n\n```python\nfrom ntes.scrambling import TB5GScrambler\n```\n\n2. Determine the bit depth and codeword length of your transport block. In this context, the bit depth is 1 (for initializing the bit sequence of the transport block) and the codeword length is 108 (for the length of the codeword after encoding).\n\n3. Initialize an instance of the `TB5GScrambler` class with the bit depth and codeword length.\n\n```python\ntb_scrambler = TB5GScrambler(bit_depth=1, n=108)\n```\n\n4. If you want to enable 5G NR compliant scrambling, you can do so by providing the `c_init` parameter as defined in the standard.\n\n```python\ntb_scrambler_c_init = TB5GScrambler(bit_depth=1, n=108, c_init=4)\n```\n\n5. You can now use the `TB5GScrambler` instance to scramble your data.\n\n```python\n# For generic scrambling\nsc_tb = tb_scrambler(b)\n\n# For 5G NR compliant scrambling\nsc_tb = tb_scrambler_c_init(b)\n```\n\nRemember that the parameters `bit_depth` and `n` are provided according to your transport block configuration. If you're dealing with 5G NR compliant scrambling, ensure that the `c_init` parameter is set to the desired cyclic shift of the sequence."
"Here is the detailed of int_mod_2: \n\nsionna.fec.utils.int_mod_2(v)\nComputes the integer modulo 2 operation.\n\nThis function returns the input reduced to the canonical representative in GF(2).\n\nInput\n    v (tf.int32) \u2013 Tensor of arbitrary shape.\n\nOutput\n    w (tf.int32) \u2013 Tensor of same shape as v containing only 1\u2019s and 0\u2019sthe canonical representative in GF(2).\n\nLink of the source code, click and jump to  int_mod_2(v)\n  [source code](https://nvlabs.github.io/sionna/_modules/sionna/fec/utils.html#int_mod_2)  \n  \nsource code:\n```python\ndef int_mod_2(v):\n    \"\"\"Computes the integer modulo 2 operation.\n\n    This function returns the input reduced to the canonical\n    representative in GF(2).\n\n    Input\n    -----\n        v : tf.int32\n            Tensor of arbitrary shape.\n\n    Output\n    ------\n        w : tf.int32\n            Tensor of same shape as ``v`` containing\n            only 1's and 0's the canonical representative\n            in GF(2).\n    \"\"\"\n    w = tf.math.floormod(v, 2)\n    return w\n```"
"Here is the detailed information of sionna.utils.SymbolSource:  \n\n[sionna.utils.SymbolSource(alphabet, return_indices=False, seed=None, spawn=None, dtypes=tf.complex64)](https://nvlabs.github.io/sionna/_modules/sionna/utils/random.html#SymbolSource)\n\nLayer generating a tensor of random symbols from a given alphabet.\n\nThe class inherits from the Keras Layer class and can be used as layer in a Keras model.\n\n### Parameters\n\n- **alphabet** (`Alphabet`): Instance of `Alphabet`.\n- **return_indices** (bool): If enabled, [0, `n_alphabet`-1]  will be returned instead of the symbols. Defaults to False.\n- **seed** (int or None): Set the seed for the random generator. If None, the global random seed will be used. Defaults to None.\n- **spawn** (bool): If True, the RNG state will be changed on every call, based on the system time. This is typically desired in parallel environments. Defaults to False.\n- **dtypes** (tf.DType): Defines the datatype for the output. Defaults to tf.complex64.\n\n\n### Input\n\n- **shape** (1D tensor, shape [`n`], tf.int32): The desired shape of the output tensor.\n\n### Output\n\n- **symbols or symbol indices** (shape, dtype): Tensor of random symbols or indices. The output tensor has shape and dtype as specified by the `shape` and `dtypes` parameters. If `return_indices` is False, the output is a tensor of shape `shape` containing the symbols. Otherwise, the output is a tensor of shape `shape` containing the symbol indices.\n\nsource code:\n```python\nclass SymbolSource(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"SymbolSource(alphabet, return_indices=False, seed=None, spawn=None, dtypes=tf.complex64)\n\n    Generates a tensor of random symbols from a given alphabet.\n\n    The class inherits from the Keras `Layer` class and can be used as layer in a Keras model.\n\n    Parameters\n    ----------\n    alphabet : Alphabet\n        Instance of :class:`~sionna.utils.Alphabet`.\n\n    return_indices : bool\n        If `True`, `[0, n_alphabet-1]` will be returned instead of the symbols.\n        Defaults to `False`.\n\n    seed : int or None\n        Set the seed for the random generator. If `None`, the global random seed\n        will be used. Defaults to `None`.\n\n    spawn : bool\n        If `True`, the RNG state will be changed on every call based on the\n        system time. This is typically desired in parallel environments.\n        Defaults to `False`.\n\n    dtypes : tf.DType\n        Defines the datatype for the output. Defaults to `tf.complex64`.\n\n    Input\n    -----\n    shape : 1D tensor, shape [`n`], tf.int32\n        The desired shape of the output tensor.\n\n    Output\n    ------\n    symbols : : ``shape``, ``dtype``\n        Tensor of random symbols.\n\n    Note\n    ----\n    Note that the complex `tf.DType` implies that the output tensor\n    consists of complex and not real-valued components.\n    \"\"\"\n    def __init__(self,  alphabet, return_indices=False, seed=None, spawn=False, dtypes=tf.complex64, **kwargs):\n        super().__init__(dtype=dtypes, **kwargs)\n        self._real_dtype = tf.as_dtype(dtypes).real_dtype\n        self._n = alphabet.num_symbols\n        self._return_indices = return_indices\n        self._seed = seed\n        self._spawn = spawn\n        self._keep_state = True\n\n        self._alphabet = alphabet\n\n    #########################################\n    # Public methods and properties\n    #########################################\n\n    @property\n    def alphabet(self):\n        \"\"\"The alphabet used by this source.\"\"\"\n        return self._alphabet\n\n    # pylint: disable=unreachable\n    @property\n    def seed(self):\n        \"\"\"Seed used to generate symbols\"\"\"\n        if self._spawn:\n            return None\n        else:\n            return self._state[0].numpy()\n\n    # pylint: disable=unreachable\n    @property\n    def keep_state(self):\n        \"\"\"Indicates if the internal random seed is reused.\"\"\"\n        return self._keep_state\n\n    #################\n    # Keras layer\n    #################\n\n    def build(self, input_shape):\n        \"\"\"Build the Layer\"\"\"\n        self._generator = self._build_generator(self._seed)\n\n    def call(self, inputs):\n        \"\"\"Generate random symbols\"\"\"\n        shape = inputs\n        u = self._generator(shape)\n        if self._return_indices:\n            u = tf.cast(u, tf.int32)\n        return u\n\n    #########################\n    # Internal methods\n    #########################\n\n    def _build_generator(self, seed):\n        if self._spawn:\n            return self._generate_complex_spawning(self._real_dtype)\n        else:\n            # If the seed is provided, the global seed will be ignored.\n            # This enables the user to set different random seeds for different\n            # instantiations of the model.\n            s = (MethodArguments.get_seed() or seed)\n            return self._generate_complex_static(self._real_dtype, s)\n\n    def _generate_complex_spawning(self, rdtype):\n        \"\"\"Generate complex values from two independent real-valued random\n        tensors.\n\n        The first tensor, t1, is generated by the default RNG. The second\n        tensor, t2, is generated by advancing the state of the RNG.\n\n        Input\n        -----\n        rdtype : tf.DType\n            The real-valued tensor dtype.\n\n        Output\n        ------\n        : tf.complex64\n            A tensor of complex values.\n        \"\"\"\n        def complex_cast(t, rdtype):\n            \"\"\"Cast two real-valued tensors to complex\"\"\"\n            t = tf.cast(t, rdtype)\n            return tf.complex(t[0], t[1])\n\n        # Generate two independent real-valued random tensors\n        t1 = tf.random.stateless_uniform([2], minval=0, maxval=rdtype.max,\n                                          dtype=rdtype)\n        t2 = tf.random.stateless_uniform([2], minval=0, maxval=rdtype.max,\n                                          dtype=rdtype)\n\n        # Cast to complex representation\n        c = complex_cast(t1, rdtype) + complex_cast(t2, rdtype)\n        return tf.ensure_shape(c, tf.shape(t1)+[2])\n\n    def _generate_complex_static(self, rdtype, seed):\n        \"\"\"Generate complex values from two independent real-valued random\n        tensors.\n\n        The two tensors are generated using the same seed. However, the\n        internal seed of the RNG is updated on each call, leading to\n        time-dependent output tensors.\n\n        Input\n        -----\n        rdtype : tf.DType\n            The real-valued tensor dtype.\n\n        seed : int\n            The seed for the RNG.\n\n        Output\n        ------\n        : tf.complex64\n            A tensor of complex values.\n        \"\"\"\n        # It is important to set the global seed here as the instantiation of\n        # the RNG object is deferred to the first call of the generator.\n        tf.random.set_seed(seed)\n\n        t1 = tf.random.stateless_uniform([2], minval=0, maxval=rdtype.max,\n                                          dtype=rdtype)\n        t2 = tf.random.stateless_uniform([2], minval=0, maxval=rdtype.max,\n                                          dtype=rdtype)\n\n        # Cast to complex-valued tensor\n        c = complex_cast(t1, rdtype) + complex_cast(t2, rdtype)\n        return tf.ensure_shape(c, tf.shape(t1)+[2])\n\n\n    @property\n    def state(self):\n        \"\"\"RNG state\"\"\"\n        return self._generator.get_state()\n\n    @state.setter\n    def state(self, value):\n        \"\"\"Set the RNG state\"\"\"\n        self._generator.set_state(value)\n\n    #########################\n    # Utility functions\n    #########################\n\n    def get_update_status(self):\n        \"\"\"Returns `True` if the internal seed should be updated.\"\"\"\n        return self._spawn\n\n    def update_seed(self, seed):\n        \"\"\"Updates the seed.\"\"\"\n        if self._spawn:\n            self._generator.set_h_state(seed)```\nsource code:\n```python\nimport tensorflow as tf\nfrom sionna.utils import Alphabet\nfrom tensorflow.keras import Layer\n\nimport logging\nLOG = logging.getLogger(__name__)\n\nclass SymbolSource(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"SymbolSource(alphabet, return_indices=False, seed=None, spawn=None, dtypes=tf.complex64)\n\n    Layer generating a tensor of random symbols from a given alphabet.\n\n    The class inherits from the Keras `Layer` class and can be used as layer in a Keras model.\n\n    Parameters\n    ----------\n    alphabet : Alphabet\n        Instance of :class:`~sionna.utils.Alphabet`.\n\n    return_indices : bool\n        If enabled, `[0, n_alphabet-1]` will be returned instead of the symbols.\n        Defaults to `False`.\n\n    seed : int or None\n        Set the seed for the random generator. If `None`, the global random seed\n        will be used. Defaults to `None`.\n\n    spawn : bool\n        If True, the RNG state will be changed on every call, based on the system time.\n        This is typically desired in parallel environments. Defaults to `False`.\n\n    dtypes : tf.DType\n        Defines the datatype for the output. Defaults to `tf.complex64`.\n\n    Input\n    -----\n    shape : 1D tensor, shape :math:`[n]`, tf.int32\n        The desired shape of the output tensor.\n\n    Output\n    ------\n    symbols : : ``shape``, ``dtype``\n        Tensor of symbols.\n\n    Note\n    ----\n    Note that the complex `tf.DType` implies that the output tensor\n    consists of complex and not real-valued components.\n    \"\"\"\n    def __init__(self,\n                 alphabet,\n                 return_indices=False,\n                 seed=None,\n                 spawn=False,\n                 dtype=tf.complex64,\n                 **kwargs):\n\n        super().__init__(dtype=dtype, **kwargs)\n        self._real_dtype = tf.as_dtype(self.dtype).real_dtype\n        self._n = alphabet.num_symbols\n        self._return_indices = return_indices\n        self._seed = seed\n        self._spawn = spawn\n\n        self._alphabet = alphabet\n\n    def build(self, input_shape):\n        \"\"\"Build implementation\"\"\"\n        self._generator = self._build_generator(self._seed)\n\n    def call(self, inputs):\n        \"\"\"Generate random symbols\"\"\"\n        shape = inputs\n        u = self._generator(shape)\n        if self._return_indices:\n            u = tf.cast(u, tf.int32)\n        return u\n\n    def _build_generator(self, seed):\n        if self._spawn:\n            return self._generate_complex_spawning(self._real_dtype)\n        else:\n            # If the seed is provided, the global seed will be ignored.\n            # This enables the user to set different random seeds for different\n            # instantiations of the model.\n            s = (tf.random.get_global_generator().get_state() + 1)\n            s = tf.cast(s, tf.int32)\n            # Explicitly pass the global seed to the RNG\n            return self._generate_complex_static(self._real_dtype, s)\n\n    def _generate_complex_spawning(self, rdtype):\n        \"\"\"Generate complex values from two independent real-valued random\n        tensors.\n\n        The first tensor, t1, is generated by the default RNG. The second\n        tensor, t2, is generated by advancing the state of the RNG.\n\n        Input\n        -----\n        rdtype : tf.DType\n            The real-valued tensor dtype.\n\n        Output\n        ------\n        : tf.complex64\n            A tensor of complex values.\n        \"\"\"\n        def complex_cast(t, rdtype):\n            \"\"\"Cast two real-valued tensors to complex\"\"\"\n            t = tf.cast(t, rdtype)\n            return tf.complex(t[0], t[1])\n\n        # Generate two independent real-valued random tensors\n        t1 = tf.random.stateless_uniform([2], 0, self._n, rdtype)\n        t2 = tf.random.stateless_uniform([2], 0, self._n, rdtype)\n\n        # Cast to complex-valued tensor\n        c = complex_cast(t1, rdtype) + complex_cast(t2, rdtype)\n        return tf.ensure_shape(c, tf.shape(t1)+[2])\n\n    def _generate_complex_static(self, rdtype, seed):\n        \"\"\"Generate complex values from two independent real-valued random\n        tensors.\n\n        The two tensors are generated using the same seed. However, the\n        internal seed of the RNG is updated on each call, leading to\n        time-dependent output tensors.\n\n        Input\n        -----\n        rdtype : tf.DType\n            The real-valued tensor dtype.\n\n        seed : int\n            The seed for the RNG.\n\n        Output\n        ------\n        : tf.complex64\n            A tensor of complex values.\n        \"\"\"\n        # It is important to set the global seed here as the instantiation of\n        # the RNG object is deferred to the first call of the generator.\n        tf.random.set_seed(seed)\n\n        t1 = tf.random.stateless_uniform([2], 0, self._n, rdtype)\n        t2 = tf.random.stateless_uniform([2], 0, self._n, rdtype)\n\n        # Cast to complex-valued tensor\n        c = self._real_dtype(0.) +  complex_cast(t1, rdtype) + complex_cast(t2, rdtype)\n        return tf.ensure_shape(c, tf.shape(t1)+[2])\n\n    @property\n    def state(self):\n        \"\"\"RNG state\"\"\"\n        return self._generator.get_state()\n\n    @state.setter\n    def state(self, value):\n        \"\"\"Set the RNG state\"\"\"\n        self._generator.set_state(value)\n\n    @property\n    def keep_state(self):\n        \"\"\"Symbol sources always keep the state.\"\"\"\n        return True\n\n    def get_update_status(self):\n        \"\"\"Returns `True` if the internal seed should be updated.\"\"\"\n        return self._spawn\n\n    def update_seed(self, seed):\n        \"\"\"Updates the seed.\"\"\"\n        if self._spawn:\n            self._generator.set_h_state(seed)```\n         "
"Here is the detailed information of ChannelModel:   \n  \n[sionna.channel.ChannelModel](https://nvlabs.github.io/sionna/_modules/sionna/channel/channel_model.html#ChannelModel)  \n  \nAbstract class that defines an interface for channel models.\n\nAny channel model which generates channel impulse responses must implement this interface. All the channel models available in Sionna, such as DLChannel, CDL, UMi, etc., implement this interface.\n\nRemark: Some channel models only require a subset of the input parameters.  \n  \nInput\n    batch_size : int\n        Batch size\n    num_time_steps : int\n        Number of time steps\n    sampling_frequency : float\n        Sampling frequency in Hertz\n\nOutput\n    a, tau :\n        a [batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps] tf.complex\n            Path coefficients\n        tau [batch size, num_rx, num_tx, num_paths] tf.float\n            Path delays in seconds\n  \nsource code:\n```python\nclass ChannelModel(ABC):\n    # pylint: disable=line-too-long\n    r\"\"\"ChannelModel()\"\"\"\n\n    @abstractmethod\n    def __call__(self, batch_size, num_time_steps, sampling_frequency):\n        # pylint: disable=line-too-long\n        r\"\"\"\n        Generate random channel samples.\n\n        This function generates a batch of random channel samples, i.e.,\n        random realizations of the random channel matrices. All returned\n        channel matrices are in Hauser model form, i.e., they can be used\n        to filter a white noise vector with the :func:`~sionna.channel.one_tap_channel` function.\n\n        This is a convenience interface, which is equivalent to calling\n        the individual functions :meth:`~sionna.channel.ChannelModel.generate_time_channel`\n        or :meth:`~sionna.channel.ChannelModel.generate_freq_channel`,\n        based on the value of the ``use_paths`` flag.\n\n        For example, for the UMi channel model\n\n        >>> batch_size = 10\n        >>> num_time_steps = 100\n        >>> sampling_frequency = 10e6\n        >>> channel_model = sionna.channel.tr38901.UMi()\n        >>> a, tau = channel_model(batch_size, num_time_steps, sampling_frequency)\n\n        is equivalent to\n\n        >>> a, tau = channel_model.generate_time_channel(batch_size, num_time_steps, sampling_frequency)\n\n        and\n\n        >>> a, f = channel_model.generate_freq_channel(batch_size, num_time_steps, sampling_frequency)\n\n        >>> h = one_tap_channel(f, a)\n\n        Input\n        ------\n        batch_size : int\n            Batch size\n\n        num_time_steps : int\n            Number of time steps\n\n        sampling_frequency : float\n            Sampling frequency [Hz]\n\n        Output\n        ------\n        a : [batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps], tf.complex\n            Path coefficients\n\n        tau : [batch size, num_rx, num_tx, num_paths], tf.float\n            Path delays [s]\n        \"\"\"\n        pass\n\n\n    @property\n    @abstractmethod\n    def support_frequency(self):\n        # pylint: disable=line-too-long\n        r\"\"\"Test if the model implements the :meth:`~sionna.channel.ChannelModel.generate_freq_channel` method.\n\n        Returns\n        -------\n        Boolean\n            `True` if the :meth:`~sionna.channel.ChannelModel.generate_freq_channel` method is implemented, `False` otherwise.\n        \"\"\"\n        pass\n\n\n    def generate_time_channel(self, batch_size, num_time_steps, sampling_frequency):\n        # pylint: disable=line-too-long\n        r\"\"\"\n        Generate random channel samples in the time domain.\n\n        This function generates a batch of random channel samples, i.e.,\n        random realizations of the random channel matrices. All returned\n        channel matrices are in Hauser model form, i.e., they can be used\n        to filter a white noise vector with the\n        :func:`~sionna.channel.one_tap_channel` function.\n\n        Input\n        ------\n        batch_size : int\n            Batch size\n\n        num_time_steps : int\n            Number of time steps\n\n        sampling_frequency : float\n            Sampling frequency [Hz]\n\n        Output\n        ------\n        a : [batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_time_steps], tf.complex\n            Path coefficients\n\n        tau : [batch size, num_rx, num_tx, num_paths], tf.float\n            Path delays [s]\n        \"\"\"\n        pass\n\n\n    def generate_freq_channel(self, batch_size, num_time_steps, sampling_frequency):\n        # pylint: disable=line-too-long\n        r\"\"\"\n        Generate random channel samples in the frequency domain.\n\n        This function generates a batch of random channel samples, i.e.,\n        random realizations of the random channel frequency response.\n        All returned channel frequency responses are in Hauser model form,\n        i.e., they can be used to compute the power spectral density (PSD)\n        of the channel with the :meth:`~sionna.channel.tr38901.PSD` function.\n\n        Input\n        ------\n        batch_size : int\n            Batch size\n\n        num_time_steps : int\n            Number of time steps\n\n        sampling_frequency : float\n            Sampling frequency [Hz]\n\n        Output\n        ------\n        f : [batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_time_steps], tf.complex\n            Channel frequency responses\n\n        \"\"\"\n        pass\n```  \n  "
"Building a Custom RadioMaterial and Attaching it to a SceneObject\n\n### Part 1: Building a Custom RadioMaterial\n\nIn this example, we are going to build a custom RadioMaterial and attach it to a SceneObject. The custom RadioMaterial will have the properties of a dielectric with a specific permittivity and conductivity.\n\nHere is how you can do it:\n\n1. First, you need to import the necessary packages and classes.\n\n```python\nfrom sionna.rt import RadioMaterial\n```\n\n2. Next, you can create your custom RadioMaterial. In the example below, we are creating a RadioMaterial with a relative permittivity of 2.0, conductivity of 5.0, and a critical angle of 60 degrees.\n\n```python\ncustom_radio_material = RadioMaterial(epsilon_r=2.0, conductivity=5.0, critical_angle=60.0)\n```\n\n### Part 2: Attaching the Custom RadioMaterial to a SceneObject\n\nTo attach the custom RadioMaterial to a SceneObject, follow these steps:\n\n1. First, you need to get the RadioMaterial name of the custom RadioMaterial you created. This is done by accessing the `name` attribute of the custom RadioMaterial.\n\n```python\ncustom_radio_material_name = custom_radio_material.name\n```\n\n2. To attach a RadioMaterial to a SceneObject, you can use the `set` method of the RadioMaterial class. The first argument of the `set` method is the SceneObject to which you want to attach the RadioMaterial. The second argument is the name of the RadioMaterial you want to attach.\n\n```python\nscene.objects[\"MySceneObject\"].radio_material = custom_radio_material_name\n```\n\nIn the example above, `\"MySceneObject\"` is the name of the SceneObject to which we are attaching the RadioMaterial. Replace `\"MySceneObject\"` with the actual name of your SceneObject.\n\nRemember to not change the `custom_radio_material_name` after it has been set, as it will cause the attachment to be lost.\n\nDemonstrating the Attachment\n---------------------------\n\nLet's demonstrate how to attach the custom RadioMaterial to a SceneObject.\n\nFirst, we will need to create a SceneObject. In the code below, a SceneObject named \"custom_so\" is created at coordinates (0,0,0) with a heading of 0, and it is visualized.\n\n```python\nfrom sionna.rt import SceneObject, RadioMaterial\nimport matplotlib.pyplot asSceneObject\n# Create a SceneObject\ncustom_so = SceneObject(name=\"custom_so\", position=[0,0,0], orientation=[0,0,0])\n# Visualize the SceneObject\ncustom_so.visualize()\n```\n\nNext, we will attach the custom RadioMaterial to the SceneObject.\n\n```python\n# Attach the custom RadioMaterial to the SceneObject\ncustom_so.radio_material = custom_radio_material\n```\n\nThe `custom_so` SceneObject is now made of the material `custom_radio_material`. If you created or loaded a scene with a SceneObject, replace `custom_so` with the name of the SceneObject you created or loaded."
"To perform Maximum A Posteriori (MAP) decoding using the `BCJRDecoder` instance from the Sionna package, you need to make use of the `hard_out` parameter and potentially the `llr_a` parameter as well. The `BCJRDecoder` is used for decoding data that has been transmitted over an additively white-hatfull (AWGN) channel, and assumes that you have an instance of it set up with either the binary level information (`llr_ch`) or the a priori information including the log-likelihood ratios (`llr_ch, llr_a`).\n\nFirst, let's cover the usage of `hard_out`. The `hard_out` parameter determines the output format of the decoder. When you instantiate a `BCJRDecoder`, you can pass a `hard_out` parameter to indicate if the decoder should output hard or soft decisions. A hard decision is a symbol (binary in the case of binary codes) and is of the type `tf.float32`, while a soft-decision is equivalent to the log-likelihood ratio (LLR). Therefore, to obtain LLRs as the output, you would set `hard_out=False`. With the `BCJRDecoder`, setting `hard_out=True` would result in the decoder giving hard decisions based on the received LLRs.\n\nHere's a demonstration of how you might set up the `BCJRDecoder` using the Sionna package, using both `llr_ch` and `hard_out`:\n\n```python\nfrom sionna.fec.linear import BCJRDecoder\n# Assuming llr_ch and llr_a are properly defined\ndecoder = BCJRDecoder(llr_ch=None,\n                      llr_a=None,\n                      hard_out=True, # Set as desired, True for hard-decisions\n                      dtype=tf.float32) # Specify the datatype as per your requirements\n```\n\nNext, to perform MAP decoding with this `BCJRDecoder`, you need to call it and pass the noisy channel output (`llr_ch` or `(llr_ch, llr_a)`) to it:\n\n```python\n# Assuming llr_ch and llr_a are the channel output\ndecision = decoder(llr_ch)\n```\n\nThis will give you the output of the `BCJRDecoder`. If `hard_out` is `True`, the output will be hard-decisions (bits). If `hard_out` is `False`, the output will be soft-decisions in the form of LLRs.\n\nThe output tensor's shape will be dependent on the input tensor shape. If the input is a tensor of binary symbols, the output shape will be the same as the input, and it will contain the decoded symbols (i.e., `hard_out=True`). If the input is a tensor of logits, the output shape will again be dependent on the input, and it will also contain the decoded symbols (since log-likelihood ratios have been converted to binary values). Again, the specific shape of the output tensor depends on the input tensor.\n\nPlease ensure that you have instantiated the `BCJRDecoder` with the proper `hard_out` and that you have input compatible with the `BCJRDecoder` when calling it."
"To utilize the `ebnodb2no` function in the Sionna model, the following steps are required:\n\n1. First, you need to import the function from the sionna.signal module.\n    ```python\n    from sionna.signal import ebnodb2no\n    ```\n\n2. Next, you will make use of the function. It takes a single input which is your desired Eb/No value in dB. \n\n3. The function will return the corresponding noise variance, a single scalar value. \n\nHere is an example of how to use it:\n\n```python\n# assuming ebnodb is an existing Eb/No value in dB\nno = ebnodb2no(ebnodb)\n```\n\nWhere `ebnodb` is the Eb/No value you want to convert. The variable `no` will store the corresponding noise variance.\n\nThe returned output, `no`, is the noise variance. It indicates the power of the noise signal in a given communication system model. The higher the noise variance, the higher the level of noise in the system. Noise variance is important as it directly impacts the quality of the decoded data on the receiver side.\n\nPlease be sure to not directly use this documentation in the final code, without seeing the actual implementation. However, the provided context does not include the `ebnodb2no` function. Please use the function from the actual implementation part of the Sionna model."
"To evaluate the baseline and the two autoencoder-based communication systems, follow these steps:\n\n1. **Baseline Model**: This model is implemented using the `LoadMIMOPlanarRLSDataset` function from the `sionna.channel.utils` module. It automatically generates batches of channel realizations and corresponding noise vectors. The baseline model consists of an MLP-based receiver that uses 3x3 zero-padding and the `LMMSEEqualizer`. The transmitter is also MLP-based and uses `QAM` with `NUM_BITS_PER_SYMBOL` bits per symbol.\n\n2. **Autoencoder-based Communication Systems**: The two autoencoder-based communication systems are implemented as subclasses of the `KerasModel` class. They replace the baseline symbol mapper and demapper with the `SymbolMapper` and `Demapper` classes from the `sionna.mapping` module. The `SymbolMapper` class uses an `AWGNChannel` layer to interface with the channel, while the `Demapper` class uses the `LogLikelihoodRatio` class from the `sionna.mapping` module to compute LLRs to hand off to the channel decoder. The first autoencoder-based system (`AEI`) uses individual layer-wise learning rates (`eta`) from a provided `hinton_eta_tuner`. The second autoencoder-based system (`AEG`) uses gradient clipping to stabilize the training of the communication system.\n\n3. **Evaluation**: The baseline and the two autoencoder-based communication systems are evaluated and compared. The `PlotBER` class is used to plot the bit error rate (BER) as a function of the signal-to-noise ratio (SNR). The `simulate` function from the `sionna.utils` module is used to run the simulations and compute the BER.\n\nHere is a simplified version of the code:\n\n```python\nall_models = [baseline_system, ae_system_1, ae_system_2]\nmodel_names = [\"Baseline\", \"AEI\", \"AEG\"]\nber_plots = []\nfor model, model_name in zip(all_models, model_names):\n    ber_plots.append(PlotBER(model, model_name))\n\nchannel = AWGN(NUM_BITS_PER_SYMBOL, is_complex=False, return_no=True)\nchannel_model = GenerateSingleUserMIMO(1, 1, AWGN, uniform_power=False, detection=\"lmmse\")\nber_plots.append(PlotBER(channel_model, \"Baseline\"))\n\nlegend_loc = \"lower left\"\nfor ber_plot in ber_plots:\n    legend_loc = ber_plot.ax.legend(loc=legend_loc, fontsize=20).get_bbox().get_points()\n    legend_loc = legend[::-1, :].mean(axis=0)\n    legend_loc = legend.astype(int)\n    ber_plot.ax.legend(loc=legend_loc, fontsize=20)\nplt.show()\n```\n\nPlease note that the actual code may be more complex and include additional steps and components."
"To configure the notebook to use only a single GPU and allocate only as much memory as needed, follow these steps:\n\n1. Import the necessary modules:\n```python\nimport os\nimport tensorflow as tf\n```\n\n2. Specify the GPU number you want to use. If you want to use the CPU, set the `gpu_num` to an empty string `\"\"`:\n```python\ngpu_num = 0 # Use \"\" to use the CPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n```\n\n3. Set the TensorFlow log level to '3' to avoid warnings:\n```python\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n```\n\n4. Get the list of all available GPUs:\n```python\ngpus = tf.config.list_physical_devices('GPU')\n```\n\n5. If GPUs are available, configure TensorFlow to use only the first one and allocate only as much memory as needed. For memory allocation, use the `tf.config.experimental.set_memory_growth` method, which attempts to allocate only as much GPU memory as needed for the runtime allocations:\n```python\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\n```\nThe code will print any runtime errors that occur during the memory allocation process.\n\n6. To avoid warnings from TensorFlow, set the logger level to 'ERROR':\n```python\ntf.get_logger().setLevel('ERROR')\n```\n\nPlease note that the GPU settings will only take effect if your machine has a GPU installed and the necessary drivers are correctly installed. If not, TensorFlow will automatically use a CPU."
"To use the \"j_fun\" function to calculate the J-function in the Sionna package, you can simply call the function with the desired input parameters. \n\nHere is a general step-by-step process:\n\n1. Import the \"j_fun\" function from the sionna.channel module.\n```python\nfrom sionna.channel import j_fun\n```\n\n2. Compute the AWGN variance. This typically requires knowledge of the EBNo, the bitwidth of the modulation, and the coderate. In the context, this is done using the `compute_awgn_var` function.\n\n3. Calculate the J-function. Note that this function is not vectorized and so input scalars must be used. Also, the absolute value of the AWGN variance must be taken.\n```python\nj = j_fun(2*ebno_lin, absolute(2*no))\n```\nIn this example, `2*ebno_lin` is the linear representation of the EbNo (the Eb/No value doubled to scale the energy per bit), and `2*no` is the absolute value of the AWGN variance (also scaled by a factor of 2).\n\nPlease be aware that this is a general process and the exact input parameters may vary depending on the specific requirements of your application."
"Here is the definition of PUSCHTransmitter: sionna.nr.PUSCHTransmitter(pusch_configs, return_bits=True, output_domain='freq', dtype=tf.complex64, verbose=False, **kwargs)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/nr/pusch_transmitter.html#PUSCHTransmitter)  \n\n```python\n#\n# SPDX-FileCopyrightText: Copyright (c) 2021-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n\"\"\"PUSCH Transmitter for the nr (5G) sub-package of the Sionna library.\n\"\"\"\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer\nimport sionna\nfrom sionna.mimo import AWGN\nfrom sionna.ofdm import OFDMModulator\nfrom sionna.utils import expand_to_rank, db\nfrom .utils import PUSCHConfig\n\nclass PUSCHTransmitter(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"PUSCHTransmitter(pusch_configs, return_bits=True, output_domain=\"freq\", dtype=tf.complex64, verbose=False, **kwargs)\n\n    Generates a batch of 5G NR PUSCH time-domain signals of multiple transmitters\n    based on the information bits. This layer encapsulates the\n    functionality of the :class:`~sionna.nr.PUSCHTransmitter` provided\n    as part of the Sionna 5G NR tool.\n\n    Parameters\n    ----------\n    pusch_configs : instance or list of :class:`~sionna.nr.PUSCHConfig`\n        PUSCH configuration parameters.\n\n    return_bits : bool\n        If set to `True`, the layer returns the sequence of information bits.\n        Defaults to `True`.\n\n    output_domain : One of [\"freq\", \"time\"]\n        The output domain. Defaults to \"freq\".\n\n    dtype : One of [tf.complex64, tf.complex128], tf.DType\n        The dtype of the output. Defaults to tf.complex64.\n\n    verbose : bool\n        If set to `True`, the layer prints the PUSCH configuration for\n        every transmitter. Defaults to `False`.\n\n    Input\n    ------\n    batch_size : int\n        Batch size\n\n    Output\n    -------\n    x : list, [batch_size, num_tx, num_ant, num_ofdm_symbols, fft_size] or [batch_size, num_tx, num_ant, num_time_samples], tf.complex\n        List of time or frequency-domain signals\n\n    info_bits : [batch_size, num_tx, num_layers, num_codewords_per_layer, num_data_symbols*nu], or\n                 [batch_size, num_tx, num_layers, num_codewords_per_layer, num_data_bits*nu], tf.float\n        5G NR PUSCH information bits (Coded as x->X_BPSK->X_PUSCH)\n\n    Raises\n    ------\n    AssertionError\n        If ``pusch_configs`` is not a valid instance of :class:`~sionna.nr.PUSCHConfig`.\n\n    AssertionError\n        If ``return_bits`` is not `True` and ``output_domain`` is not \"freq\".\n\n    AssertionError\n        If ``dtype`` is not (either) tf.complex64 or tf.complex128.\n\n    AssertionError\n        If ``output_domain`` is not \"freq\" or \"time\".\n    \"\"\"\n    def __init__(self,\n                 pusch_configs,\n                 return_bits=True,\n                 output_domain=\"freq\",\n                 dtype=tf.complex64,\n                 verbose=False,\n                 **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        self._return_bits = return_bits\n        self._output_domain = output_domain\n        self._verbose = verbose\n        self._config = PUSCHConfig(pusch_configs)\n\n    def build(self, input_shape): # pylint: disable=unused-argument\n        if self._return_bits:\n            self._info_bits = AWGN(dtype=tf.float32, allow_complex=False)\n\n    def call(self, inputs):\n        # pylint: disable=line-too-long\n        r\"\"\"PUSCHTransmitter for the nr (5G) sub-package of the Sionna library.\n\n        Parameters\n        ----------\n        inputs : `batch_size`, `num_tx`, `transport_block_size`, tf.float\n            5G NR PUSCH input signals. The last dimension is interpreted\n            as the transport block to be encoded.\n\n        outputs : `batch_size`, `num_tx`, `num_ant`, `num_ofdm_symbols`, `fft_size` or `batch_size`, `num_tx`, `num_ant`, `num_time_samples`, tf.complex\n            Transmitter output\n        \"\"\"\n\n        # check input dimensions\n        x = inputs\n        tf.debugging.assert_equal(\n            tf.rank(x), 3,\n            \"inputs must have rank 3\")\n        tf.debugging.assert(x.shape[2]>0, \"tb_size cannot be zero\")\n\n        # wrap PUSCH configuration for convenience\n        pusch_config = self._config\n\n        # adjust noise variance for the PUSCH config\n        num_data_symbols = pusch_config.num_ofdm_symbols - pusch_config.cyclic_prefix_length\n        eff_subcarrier = pusch_config.fft_size - pusch_config.num_ofdm_symbols\n\n        # apply bit labeling if info_bits are to be returned\n        if self._return_bits:\n            x_b = sionna.mapping.bpsk(x)\n            x = sionna.utils.expand_to_rank(x, tf.rank(x_b))\n            x = tf.stack([x_b, x], axis=-1)\n\n        # do the transform to the time domain\n        if self._output_domain == \"time\":\n            x = pusch_config.ofdm_modulator(x)\n            x = tf.signal.ifft(x)\n\n        # check output for plausibility\n        assert x.shape[3] == num_data_symbols\n        assert x.shape[4] == eff_subcarrier\n\n        # add CP and DC\n        x = tf.concat([x, x], axis=-2)\n        x = pusch_config.add_cp(x)\n        x = pusch_config.map_to_subcarriers(x)\n\n        # apply AWGN if info_bits are to be returned\n        if self._return_bits:\n            no = self._config.no * tf.cast(2.0, tf.float32)\n            x, e = self._info_bits([x, no])\n\n        if self._return_bits:\n            return x, e\n        else:\n            return x\n```"
"Here is the detailed information of function verify_gm_pcm: [sionna.fec.utils.verify_gm_pcm(g, verify_results=True)](https://gm-nr-ldpc-pcm-05.kjammer.me/verify_gm_pcm.html#sionna.fec.utils.verify_gm_pcm)\n\nVerify that a generator matrix is fully composed of parity-check equations.\n\nThis function checks if \\(G^T\\), corresponding to generator matrix \\(G\\), is composed of parity-check equations. If this is the case, an AttributeError is raised. Otherwise, it silently returns.\n\nThis function is useful to ensure that the provided generator matrix is valid for LDPC decoding, as, in principle, any binary matrix can be a generator matrix. However, proper LDPC codes require that G has full rank and that it describes parity-check equations.\n\n**Input**\n\n- `g` (ndarray): Binary generator matrix of shape \\([k, n]\\).\n- `verify_results` (bool): If True, the function returns silently. Otherwise, an AttributeError is raised if the generator matrix is not composed of parity-check equations. Defaults to True.\n\n**Output**\n\n- None\n\nsource code:\n```python\ndef verify_gm_pcm(g, verify_results=True):\n    r\"\"\"Verify that a generator matrix is fully composed of parity-check equations.\n\n    This function checks if :math:`G^T`, corresponding to generator matrix\n    :math:`G`, is composed of parity-check equations. If this is the case,\n    an :class:`AttributeError` is raised. Otherwise, it silently returns.\n\n    This function is useful to ensure that the provided generator matrix is\n    valid for LDPC decoding, as, in principle, any binary matrix can be a\n    generator matrix. However, proper LDPC codes require that `G` has full\n    rank and that it describes parity-check equations.\n\n    Input\n    -----\n    g : ndarray\n        Binary generator matrix of shape `[k, n]`.\n\n    verify_results: bool\n        If True, the function returns silently. Otherwise, an\n        :class:`AttributeError` is raised if the generator matrix is not\n        composed of parity-check equations. Defaults to True.\n\n    Output\n    ------\n    None\n    \"\"\"\n    from sionna.fec.utils import gm2pcm, verify_pcm\n\n    if verify_results:\n        # Results are expected to be verified\n        # Exit silently if g is already a PCM\n        try:\n            verify_pcm(g, verify_results=True)\n        except AssertionError as e:\n            # r-m should be verified to detect invalid matrices\n            verify_pcm(g, verify_results=True)\n            raise e\n    else:\n        # Results are not expected to be verified\n        # Validate that each row of g is a parity-check equation\n        assert np.all(g.sum(axis=1)%2==0), \\\n            \"\"\"Each row of g must be a parity-check equation.\"\"\"\n\n        # Also validate that g has full rank\n        assert np.linalg.matrix_rank(g, tol=1e-4)<g.shape[0], \\\n            \"Generator matrix has not full rank.\"\n```\n\n  "
"Permittivity is a measure of the electric polarizability of a medium. In other words, it measures how much the electric field in a dielectric material is distorted by the presence of a dielectric field. The permittivity of a medium is typically represented by the Greek letter \u03b5 (epsilon). \n\nPermeability is a measure of the ability of a material to become magnetized in the presence of an applied magnetic field. It is represented by the Greek letter \u03bc (mu). \n\nIn the context provided, the permittivity and permeability are calculated as functions of the relative electric permittivity (dielectric constant) and relative magnetic permeability. The relative electric permittivity is a measure of how much polarizability a dielectric material has compared to a vacuum, and the relative magnetic permeability is a measure of how much magnetization a material can undergo compared to a vacuum. Both of these properties are dependent on the frequency of the applied field. \n\nThe permittivity is calculated using the formula: \u03b5 = \u03b50\u03b5r\nwhere \u03b50 is the electric permittivity of free space and \u03b5r is the relative electric permittivity. \n\nThe permeability is calculated using the formula: \u03bc = \u03bc0\u03bcr\nwhere \u03bc0 is the magnetic permeability of free space and \u03bcr is the relative magnetic permeability. \n\nIn the above, I clarified the meaning of \"permittivity\" and \"permeability\" but provided no direct information from the context about how they are related to Sionna, the Python package for simulating wireless communication networks. If there is any relevant information in the context about these quantities or their calculation, it is not included in the provided context."
"Class: RemoveNulledSubcarriers\n  \nsource code: [RemoveNulledSubcarriers](https://nvlabs.github.io/sionna/_modules/sionna/ofdm/resource_mapping.html#RemoveNulledSubcarriers-class)  \n  \nThe `RemoveNulledSubcarriers`class removes nulled guard and/or DC carriers from an OFDM resource grid. The definition of a guard carrier and a DC carrier depends on the underlying OFDM system settings, which can be provided as an instance of :class:`~sionna.ofdm.ResourceGrid`.\n\n**Parameters**\n\n- `resource_grid` ([`ResourceGrid`](https://nvlabs.github.io/sionna/_modules/sionna/ofdm/resource_grid.html#ResourceGrid)): An instance of [`ResourceGrid`](https://nvlabs.github.io/sionna/_modules/sionna/ofdm/resource_grid.html#ResourceGrid).\n- `take_dc` (bool): Indicates if the DC carrier is to be removed or not. Defaults to True.\n- `take_guard` (str): Indicates if guard carriers are to be removed or not. Defaults to \"left\" and \"right\".\n- `dtype` (tf.DType): Type of the input and output. Defaults to tf.complex64.\n\n**Input**\n\n- `x` ([batch_size, num_tx, num_streams_per_tx, num_ofdm_symbols, fft_size], tf.complex): Full OFDM resource grid.\n\n**Output**\n\n- `x` ([batch_size, num_tx, num_streams_per_tx, num_ofdm_symbols, num_effective_subcarriers], tf.complex): Resource grid without nulled subcarriers. The shape depends on the `take_dc` and `take_guard` settings."
"Here is the definition of LLRs2SymbolLogits: sion.mimo.LLRs2SymbolLogits(method, num_bits_per_symbol, hard_out=False, with_prior=False, dtype=tf.float32, **kwargs)  \n  \nThe link of the source code of LLRs2SymbolLogits is [LLRs2SymbolLogits](https://nvlabs.github.io/sion/_modules/sion/mimo/utils.html#LLRs2SymbolLogits)  \n\nSource code:\n```python\nclass LLRs2SymbolLogits(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    LLRs2SymbolLogits(method, num_bits_per_symbol, hard_out=False, with_prior=False, dtype=tf.float32, **kwargs)\n\n    Computes logits or hard-decisions on constellation points from\n    soft-symbols (log-likelihood ratios) for a constellation constellation_type.\n    If the flag ``hard_out`` is set, the layer provides hard-decisions on\n    constellation points (i.e., computes :math:`\\hat{b}=b_{\\text{constellation}}`,\n    where :math:`b_{\\text{constellation}}` is the constellation symbol taking\n    values in :math:`\\mathcal{C}`). Else, it provides logits on\n    the constellation points, which are used to compute channel\n    estimates with the Expectation Maximization (EM) algorithm [BGT]_.\n    Additional options are the computation with prior knowledge on the\n    constellation points and/or the bit label a priori probabilities.\n\n    Parameters\n    ----------\n    method : One of [``app`` or ``maxlog``] str\n        The method used for computing the LLRs.\n\n    num_bits_per_symbol : int\n        The number of bits per constellation symbol, e.g., 4 for QAM16.\n\n    hard_out : bool\n        If `True`, the layer provides hard-decided bits instead of soft-values.\n        Defaults to `False`.\n\n    with_prior : bool\n        If `True`, the layer assumes that prior knowledge on the\n        constellation points is available. This prior information is\n        provided as LLRs as an additional input to the layer.\n        Defaults to `False`.\n\n    dtype : One of [``tf.float32``, ``tf.float64``] tf.DType (dtype)\n        The dtype for the input and output.\n        Defaults to ``tf.float32``.\n\n    Input\n    -----\n    llrs : [...,n, num_bits_per_symbol] or [...,n, num_bits_per_symbol,2], tf.float\n        The LLRs for the constellation points. Will be interpreted as\n\n        :math:`\\ell_{b,c}` or :math:`\\ell_{b,c,i}`\n\n        where :math:`\\ell_{b,c}` is the LLR for the symbol :math:`s_{b,c}` with\n        :math:`s_{b,c} \\in \\mathcal{C}` and\n\n        :math:`\\ell_{b,c,i} = \\left(\\ell_{b,c,0}, \\ell_{b,c,1}\\right)` for the\n        constellation symbol :math:`s_{b,c}` with\n\n        :math:`s_{b,c} \\in \\mathcal{C}` and :math:`\\ell_{b,c,0}` is the LLR\n        for the first bit of :math:`s_{b,c}` and :math:`\\ell_{b,c,1}` is the LLR\n        for the second bit of :math:`s_{b,c}`.\n\n    prior : [num_points] or [...,num_points], tf.float\n        Prior probabilities for the constellation points. An array of\n        logits may be provided to define different prior probabilities for each bit label.\n        The prior is broadcast to the dimension of ``llrs`` if not of same shape.\n        Only required if the ``with_prior`` flag is set.\n\n    Output\n    ------\n    : [...,n, num_points] or [...,n, num_points,2], tf.float\n        A tensor of logits for the constellation points. If ``hard_out``\n        is `True`, a tensor of hard-decisions for the constellation points.\n\n    Note\n    ----\n    With the ``maxlog`` method, the LLR for the i-th bit of the constellation\n    symbol is computed as\n\n    .. math::\n        \\ell_{b,c,i} = \\log \\left( 1 + \\frac{\\Pr\\left( s_{b} = s_{c} \\lvert y, \\mathbf{p} \\right)}{\\Pr\\left( s_{c} \\lvert \\mathbf{p} \\right)} \\right)\n\n    where :math:`\\mathbf{p} = \\left[p_0,\\dots,p_{m-1}\\right]` are the prior probabilities on the\n    bit label, :math:`\\Pr\\left( s_{c} \\lvert \\mathbf{p} \\right)` is the probability of the constellation\n    symbol :math:`s_{c} \\in \\mathcal{C}` for the source of :math:`s_{b} \\in \\mathcal{C}`, and\n\n    .. math::\n        \\Pr\\left( s_{b} = s_{c} \\lvert y, \\mathbf{p} \\right) = \\prod_{\\substack{j=0\\\\ b_j=1}}^{m-1} \\text{sigmoid}\\left( \\ell_{y,j} + \\ell_{b,c,j} - \\sum_{\\substack{k=0\\\\ c_k=1}}^{m-1} \\ell_{y,k} + \\ell_{b,c,k} \\right)\n\n    is the probability that :math:`s_{b}` was sent, where :math:`\\ell_{y,j}` is the LLR of the received\n    signal :math:`y` for the j-th bit label and :math:`\\ell_{b,c,j}` is the LLR of the j-th bit of :math:`s_{b}`.\n    The definition of the LLR is such that it is positive for the correct bit label.\n    \"\"\"\n    def __init__(self,\n                 method,\n                 num_bits_per_symbol,\n                 hard_out=False,\n                 with_prior=False,\n                 dtype=tf.float32,\n                 **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        assert method in (\"app\",\"maxlog\"), \"Unknown demapping method\"\n        self._method = method\n        self._hard_out = hard_out\n        self._num_bits_per_symbol = num_bits_per_symbol\n        self._with_prior = with_prior\n        num_points = 2**num_bits_per_symbol\n        self._num_symbols = num_points\n        self._symbol_indices = tf.range(0, self._num_symbols, dtype=tf.int32)\n        # create mask for MSB of symbol indices\n        self._msb = 1 << (num_bits_per_symbol-1)\n        if self._with_prior:\n            # The prior information is given as LLRs on the individual\n            # bits of a symbol. This corresponds to P(x=1)-P(x=0) where\n            # P are the prior probabilities on the constellation points.\n            self._prior = Parameter(tf.constant(0.0,\n                                                dtype=dtype\n                                                ), dtype=dtype)\n            if isinstance(self._prior, (int, float)):\n                self._prior = tf.constant(self._prior, dtype=dtype)\n        else:\n            self._prior = 0.0\n\n    def call(self, inputs):\n        if self._with_prior:\n            llr, prior = inputs\n        else:\n            llr = inputs\n\n        # llr on constellation points\n        # the symbol index (bits are taken from the symbol in decreasing order)\n        # is denoted by [b_0,...,b_{m-1}]^T where m is the number of bits per symbol.\n        # The LLR for the i-th bit of the constellation symbol is computed as\n        # \\ell_{b,c,i} = \\log \\left( 1 + \\frac{\\Pr\\left( s_{b} = s_{c} \\lvert y, \\mathbf{p} \\right)}{\\Pr\\left( s_{c} \\lvert \\mathbf{p} \\right)} \\right)\n        # where\n        # \\Pr\\left( s_{b} = s_{c} \\lvert y, \\mathbf{p} \\right) = \\prod_{\\substack{j=0\\\\ b_j=1}}^{m-1} \\text{sigmoid}\\left( \\ell_{y,j} + \\ell_{b,c,j} - \\sum_{\\substack{k=0\\\\ c_k=1}}^{m-1} \\ell_{y,k} + \\ell_{b,c,k} \\right)\n        # is the probability that s_{b} was sent, where s_{b} denotes the\n        # symbol being 0 (for the QAM constellation, the symbol is -1) and\n        # \\ell_{y,j} is the j-th LLR of the received signal, \\ell_{b,c,j} is\n        # the j-th LLR of the symbol : s_{b} \\in \\mathcal{C} and\n        # \\ell_{y,k} is the k-th LLR of the constellation symbol : s_{c} \\in \\mathcal{C}.\n        # The definition of the LLR is such that it is positive for the\n        # correct bit label.\n        if self._with_prior:\n            llr = (llr + prior)[:, tf.newaxis]\n        symbol_indices = tf.cast(\n            tf.tile(self._symbol_indices, tf.shape(llr)[0]), llr.dtype)\n        # mask for the most significant bits\n        msb_mask = tf.tile(self._msb, tf.shape(llr)[0])\n        llr = tf.gather(llr, symbol_indices, axis=-1)\n        # compute all possible symbol bits in an advanced way\n        bits = np.unravel_index(\n            np.arange(0, self._num_symbols, dtype=np.int32),\n            np.shape(symbol_indices))\n        bits = tf.transpose(tf.cast(bits, llr.dtype))\n        # and apply the same msb_mask\n        bits = tf.bitwise_and(bits, msb_mask)\n        # bits is now a tensor of shape [num_symbols, num_bits_per_symbol]\n        llr = tf.gather(llr, bits, axis=-1, batch_dims=2, axis=2)\n        llr = tf.squeeze(llr, axis=2)\n        # put in correct order\n        llr = tf.transpose(llr, [0, 2, 1])\n        if self._hard_out:  # provide hard-decisions on symbols\n            snr_db = 10.*tf.math.log(\n                tf.reduce_mean(tf.square(2. * self._symbol_indices-1.0)))\n            noise_var_est = tf.constant(snr_db, dtype=llr.dtype)\n            hard = tf.tile(\n                expand_to_rank(tf.cast(llr > 0.0, llr.dtype), tf.rank(llr), 0),\n                tf.stack([1, 1, self._num_bits_per_symbol], axis=0))\n            return hard, tf.zeros(0, dtype=llr.dtype)\n        else:\n            # compute average symbol logits for each constellation symbol\n            p = tf.reduce_logsumexp(llr, axis=-1)\n            # remove prior information from symbol logits\n            if self._with_prior:\n                p = p - self._prior\n            return p, tf.zeros(0, dtype=llr.dtype)\n```\n  "
"To configure the notebook to use only a single GPU and allocate only as much memory as needed, follow these steps:\n\n1. Import the necessary modules:\n```python\nimport os\nimport tensorflow as tf\n```\n\n2. Specify the GPU number you want to use. If you want to use the CPU, set `gpu_num` to an empty string (\"\"). In this example, we're using the first GPU, so we set `gpu_num` to 0:\n```python\ngpu_num = 0\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n```\n\n3. Set the TensorFlow log level to 3 to see warnings:\n```python\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n```\n\n4. Get the list of all available GPUs:\n```python\ngpus = tf.config.list_physical_devices('GPU')\n```\n\n5. If GPUs are available, configure TensorFlow to use only the first one and allocate only as much memory as needed:\n```python\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\n```\nIn the above code, `tf.config.experimental.set_memory_growth` is used to enable memory growth for the selected GPU. When memory growth is enabled, TensorFlow allocates only as much GPU memory as needed for the runtime allocations: it starts out allocating very little memory, and as the program gets much more GPU memory, TensorFlow extends its GPU memory region to accommodate the new allocations."
" To compute codeword bits from information bits using the `LinearEncoder` provided in the `sion.fec.polar.encoding` module, you would first need to import the necessary components from the Sion library. Since the `LinearEncoder` is part of the polar encoding module, we would need to import that module as well. Here's how you can do it:\n\n```python\nimport numpy as np\nfrom sion.fec.polar.encoding import LinearEncoder\n```\n\nNext, you would define the positions of the frozen bits and the length of the codeword:\n\n```python\nfrozen_pos = [0, 1, 2, 3]  # example position\nn = 8  # example length\n```\n\nThen, you can initialize the `LinearEncoder` and input the information bits tensor `u` to get the codeword bits:\n\n```python\nencoder = LinearEncoder(frozen_pos, n)\nu = np.random.randint(0, 2, size=[k])  # assuming k information bits\nc = encoder(u)\n```\n\nIn the code above, the `LinearEncoder` is initialized with the frozen positions `frozen_pos` and the length of the codeword `n`. When the encoder is called with `u` (the information bits tensor), it returns `c` which is the tensor containing the codeword bits.\n\nPlease note that the actual tensor for the information bits `u` is generated randomly for the demonstration. You should use your own input tensor depending on your specific requirements. Just ensure that the input tensor and the frozen positions are compatible and follow the specifications of the `LinearEncoder`."
"Here is the detailed information of sionna.utils.flatten_last_dims:  \n\n[sionna.utils.flatten_last_dims(tensor, num_dims=2)](https://nvlabs.github.io/sionna/_modules/sionna/utils/tensors.html#flatten_last_dims)\n\nFlattens the last num_dims dimensions of a tensor.\n\nThis operation flattens the last num_dims dimensions of a tensor. For example, if a tensor\u2019s shape is [0, 1, 2, 3, 4] and num_dims=2, then the flattened shape will be [0, 1, 2*3, 4].\n\n### Parameters\n\n- **tensor**: A tensor of any shape.\n- **num_dims** (int): The number of last dimensions to flatten. Defaults to 2.\n\n### Returns\n\n- A tensor of the same type and shape as `tensor`, with the last `num_dims` dimensions flattened.\n\nsource code:\n```python\ndef flatten_last_dims(tensor, num_dims=2):\n    \"\"\"\n    Flattens the last num_dims dimensions of a tensor.\n\n    This operation flattens the last `num_dims` dimensions of a\n    tensor. For example, if a tensor's shape is\n    `[0, 1, 2, 3, 4]` and `num_dims`=2, then the flattened\n    shape will be `[0, 1, 2*3, 4]`.\n\n    Args:\n        tensor : A tensor.\n        num_dims (int) : The number of last dimensions\n            to flatten. Defaults to 2.\n\n    Returns:\n        A tensor of the same type and shape as ``tensor``,\n        with the last ``num_dims`` dimensions flattened.\n    \"\"\"\n    msg = \"`num_dims` must be less than the tensor's number of dimensions\"\n    tf.debugging.assume_positive(num_dims)\n    tf.debugging.assume(num_dims <= tf.rank(tensor))\n\n    shape = tf.shape(tensor)\n    flat_shape = tf.concat([shape[:-num_dims], [tf.reduce_prod(shape[-num_dims:])], shape[-1]], 0)\n    flat = tf.reshape(tensor, flat_shape)\n\n    return flat\n```"
"Here is the definition of Mapper: sionna.mapping.Mapper(coding_bits, processed_bits, constellation_type='qam', num_bits_per_symbol=2, constellation=None, seed=1, dtype=tf.complex64, **kwargs)  \n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/mapping.html#Mapper):\n```python\nclass Mapper(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"Mapper(coding_bits, processed_bits, constellation_type=\"qam\", num_bits_per_symbol=2, constellation=None, seed=1, dtype=tf.complex64, **kwargs)\n\n    Maps a vector of binary source symbols to a vector of constellation points.\n\n    Parameters\n    ----------\n    coding_bits: int\n        The number of coding bits (not constellation points) of the\n        mapper. Only the first ``coding_bits`` bits of ``x`` are considered.\n\n    processed_bits: int\n        The number of bits processed for modulation.\n        ``x`` has shape ``[..., ceil(K/processed_bits), processed_bits]``.\n        Only the first ``processed_bits*constellation_points`` bits of ``x``\n        are considered.\n\n    constellation_type : One of [\"qam\", \"pam\", \"custom\"], str\n        For \"custom\", an instance of :class:`~sionna.mapping.Constellation`\n        must be provided.\n\n    num_bits_per_symbol : int\n        The number of bits per constellation symbol, e.g., 4 for QAM16.\n        Only required for ``constellation_type`` in [\"qam\", \"pam\"].\n\n    constellation : Constellation\n        An instance of :class:`~sionna.mapping.Constellation` or `None`.\n        In the latter case, ``constellation_type``\n        and ``num_bits_per_symbol`` must be provided.\n\n    seed : int\n        Seed for the random generator producing the QAM constellations.\n        Only required if ``constellation_type`` is \"qam\".\n        : 0 <= seed <= 2**32\n\n    dtype : [tf.complex64, tf.complex128], tf.DType\n        The dtype of the output. Defaults to `tf.complex64`.\n\n    Input\n    -----\n    [...,n], tf.float32\n        A 2+D tensor of shape [...,n] containing the modulated symbols\n        where the last dimension is for the different bits/modulation-llocations.\n\n    Output\n    ------\n    [...,n/num_bits_per_symbol], tf.complex\n        A 2+D tensor of shape [...,n/num_bits_per_symbol] containing the\n        constellation symbols.\n\n    Note\n    ----\n    One must distinguish between two types of bit labels for the QAM\n    constellation. The \"natural\" label order of a QAM constellation is\n    used, i.e., the two binary representations below produce the same\n    symbol index\n\n    .. code-block:: python\n\n        00  -->  0\n        01  -->  1\n        11  -->  2\n        10  -->  3\n\n    This is different from many other libraries or convention in digital\n    communications, where the bits are ordered as\n\n    .. code-block:: python\n\n        00  -->  0\n        01  -->  1\n        10  -->  2\n        11  -->  3\n\n    The implementation here results in the natural bit label order of the QAM\n    constellation. For PAM, the bits are simply mapped to the PAM symbols in\n    increasing order.\n    \"\"\"\n    def __init__(self,\n                 coding_bits,\n                 processed_bits,\n                 constellation_type=\"qam\",\n                 num_bits_per_symbol=2,\n                 constellation=None,\n                 seed=1,\n                 dtype=tf.complex64,\n                 **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        assert dtype in [tf.complex64, tf.complex128],\\\n            \"dtype must be tf.complex64 or tf.complex128.\"\n\n        assert constellation_type in [\"qam\", \"pam\", \"custom\"],\\\n            \"Wrong constellation type.\"\n\n        if constellation_type==\"custom\":\n            assert constellation is not None, \\\n                \"For constellation_type custom, \" \\\n                \"an instance of Constellation must be provided.\"\n            assert isinstance(constellation, Constellation), \\\n                \"constellation must be an instance of Constellation.\"\n            binary_labels = constellation.binary_labels\n            num_bits_per_symbol = constellation.num_bits_per_symbol\n            constellation_points = constellation.points\n            dtype = constellation.points.dtype\n        else:\n            binary_labels, num_bits_per_symbol, constellation_points, dtype = \\\n                _CONSTELLATION_DICT[constellation_type]  # pylint: disable=unbalanced-tuple-unpacking\n\n        assert dtype in [tf.complex64, tf.complex128],\\\n            \"This bit-label order is not implemented.\"\n\n        self._binary_labels = binary_labels\n        self._num_bits_per_symbol = num_bits_per_symbol\n        self._n = 0 # Will be set during the first call\n        self._constellation_points = constellation_points\n        self._seed = seed\n        self._check_input = True\n        self._allow_zero = True\n        self._keepdim = False\n\n        # Create random QAM constellations on the fly\n        if constellation_type==\"qam\":\n            r = np.arange(num_bits_per_symbol/2,\n                          num_bits_per_symbol-0.1,\n                          2).astype(np.float32)\n            x = np.expand_dims(1-2/np.pi*r, axis=1)\n            r = np.expand_dims(r, axis=0)\n            qam = x + 1j*x # First quadrant\n            qam = np.concatenate([qam, np.conj(qam)], axis=0) # All four\n            qam = np.reshape(qam, [-1]) # Vectorize\n            self._qam_points = qam # Save for arbitrary shape\n\n    def build(self, input_shape): #pylint: disable=unused-argument\n        self._check_input = False # Only during the first call\n        output_shape = input_shape[:-1]\n        output_shape = tf.concat([output_shape, [1]], axis=-1)\n        self._n = output_shape[-1]\n        self._allow_zero = tf.cast(self._n%self._num_bits_per_symbol, tf.bool)\n\n    # pylint: disable=no-self-argument\n    def choose_indices(self, shape, seed):  #shape: [], int\n        \"\"\"Randomly sample symbol indices with uniform distribution\"\"\"\n\n        # Note: calling from a different device (TPU/CPU) might raise:\n        # \"InvalidArgumentError: invalid use of `tf.random.uniform` with XLA.\"\n        # Replace with `tf.compat.v1.random_uniform` in this case.\n        # https://github.com/tensorflow/tensorflow/issues/35912\n        x = tf.random.uniform( #pylint: disable=E1120\n            shape=shape,\n            minval=0,\n            maxval=self._n,\n            dtype=tf.int32,\n            seed=seed)\n        return x\n\n    def call(self, x):\n        no_zero = tf.cast(self._n%self._num_bits_per_symbol, tf.bool)\n        if self._check_input:\n            assert tf.rank(x).numpy() >= 2\n            assert x.shape[-1]==self._n, \\\n                \"The input must have the same last dim as the number of \" \\\n                \"bits per symbol.\"\n\n        if self._num_bits_per_symbol==1:\n            ind = tf.math.sign(tf.random.normal(tf.shape(x), seed=self._seed))\n            ind = tf.where(ind>=0, self._binary_labels[1], self._binary_labels[0])\n            return tf.cast(ind, self.output_dtype)\n\n        else:\n            ind = self.choose_indices(tf.shape(x)[:-1], self._seed)\n\n            # We need to flatten this for the gather operation\n            ind = tf.reshape(ind, [-1, self._n//self._num_bits_per_symbol])\n\n            # Compute the symbol indices\n            # The multiplication implements a boolean \"if then else\" operation\n            shf = self._n//self._num_bits_per_symbol-1\n            conv = tf.range(ind.shape[0])*2\n            trf = tf.where(ind[:,shf]==0, conv, conv+1)\n            i = tf.gather(ind, trf, axis=1)\n            i = tf.reshape(i, [-1, self._n])\n\n            # Get the symbols\n            if self._allow_zero:\n                qam = tf.gather(self._qam_points, i, axis=1)\n            else:\n                qam = tf.gather(self._qam_points, i+1, axis=1)\n        return qam\n\n\n    @property\n    def constellation_points(self):\n        return self._constellation_points\n\n    @property\n    def binary_labels(self):\n        return self._binary_labels\n\n    @property\n    def num_bits_per_symbol(self):\n        return self._num_bits_per_symbol\n\n    @property\n    def n(self):\n        return self._n\n\n    @property\n    def seed(self):\n        return self._seed\n\n    @property\n    def keepdim(self):\n        return self._keepdim\n\n    @keepdim.setter\n    def keepdim(self, value):\n        self._keepdim = value\n```  \n  "
"To configure the notebook to use only a single GPU and allocate only as much memory as needed, follow these steps:\n\n1. Import the necessary modules:\n```python\nimport os\nimport tensorflow as tf\n```\n\n2. Specify the GPU you want to use. If you want to use the CPU, set `gpu_num` to an empty string (\"\"). In this example, we're using the first GPU, so we set `gpu_num` to 0:\n```python\ngpu_num = 0\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n```\n\n3. Set the TensorFlow log level to 3 to avoid warnings:\n```python\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n```\n\n4. Get the list of all available GPUs:\n```python\ngpus = tf.config.list_physical_devices('GPU')\n```\n\n5. If GPUs are available, configure TensorFlow to use only the first one and allocate only as much memory as needed:\n```python\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\n```\nIn the above code, `tf.config.experimental.set_memory_growth` is used to enable memory growth for the selected GPU. When memory growth is enabled, TensorFlow allocates only as much GPU memory as needed for the runtime allocations: it starts out allocating very little memory, and as the program gets run and more GPU memory is needed, TensorFlow extends its GPU memory region to fulfill the runtime allocations."
"Here is the definition of BinaryErasureChannel: sionna.channel.BinaryErasureChannel(return_bv=False, bipolar_input=False, prob_er=0.1, seed=None, keep_probs=False, is_logit=False, dtype=tf.float32, **kwargs)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/channel/erasure.html#BinaryErasureChannel)  \n\nsource code:\n```python\nclass BinaryErasureChannel(BinaryErasureChannel_LDPC):\n    # pylint: disable=line-too-long\n    r\"\"\"BinaryErasureChannel(return_bv=False, bipolar_input=False, prob_er=0.1, seed=None, keep_probs=False, is_logit=False, dtype=tf.float32, **kwargs)\n\n    Layer that simulates the binary erasure channel.\n\n    In the binary erasure channel, bits are either correctly\n    received or are erased. This means, that - like in the\n    binary symmetric channel - the channel output consists of\n    binary variables (erasures) or bits. However, differently\n    from the BSC, the erasure probability can be asymmetric.\n\n    If input is a tensor of logicals, i.e., `mc.tensor_type` is\n    `tf.bool`, the `return_bv` flag indicates if erasure\n    indicators are returned as ``tf.float32`` instead of ``tf.bool``.\n    Defaults, the erasure indicator is equivalent to ``None`` in the\n    binary erasure channel. In the binary erasure channel, a\n    propagation of ``-1`` (i.e., a LLR of infinite) is not possible.\n\n    This class inherits from the Keras `Layer` class and can be used as layer in\n    a Keras model.\n\n    Parameters\n    ----------\n\n    return_bv: bool\n        Defaults to `False`. If `True`, the layer returns log-likelihood\n        ratios instead of binary values based on ``pb``.\n\n    bipolar_input : bool, False\n        Defaults to `False`. If `True`, the expected input is given as\n        :math:`\\pm 1` instead of :math:`0,1`.\n\n    prob_er : float\n        A scalar or a `DType` (tf.float32) only erasure probability.\n        Defaults to 0.1.\n\n    seed : int\n        Defaults to `None`. A value for the random seed used\n        to generate the block erasures.\n\n    keep_probs : bool\n        Defaults to `False`. If erasure probabilities are provided\n        per bit, they must be explicitly given as a tuple during\n        the forward pass.\n\n    is_logit: bool\n        Defaults to `False`. If `True`, the erasure probability\n        ``pb`` is interpreted as a logit i.e.,`\n        :math:`p = \\frac{e^{\\text{pb}}}{1+e^{\\text{pb}}}`.\n\n    dtype : tf.DType\n        Defaults to `tf.float32`. Defines the datatype for internal\n        calculations and the output dtype.\n\n    Input\n    -----\n\n    (x, pb) :\n        Tuple:\n\n    x : [...,k], tf.float32\n        Input to the channel.\n\n    pb : tf.float32\n        A scalar or of any shape that can be broadcasted to ``x``.\n        Only needed if ``keep_prob`` is `False`.\n\n    probs per bit:\n        `[...,k]`, tf.float32. An erasure probability per bit can be\n        provided.\n\n    Output\n    -------\n        : [...,k], tf.float32\n            Output of the binary erasure channel to the last dimension.\n            If ``return_bv`` is `False`, the output is binary and otherwise\n            a `tf.float32` vector is returned.\n\n        : [...,k], tf.float32\n            Output of the binary erasure channel to the last dimension.\n            Only if ``return_bv`` is `True`. This tensor has the same shape as\n            the input ``x`` and contains the log-likelihood ratios (LLRs)\n            of the received bits.\n    \"\"\"\n\n    def __init__(self, return_bv=False, bipolar_input=False, prob_er=0.1,\n                 seed=None, keep_probs=False, is_logit=False,\n                 dtype=tf.float32, **kwargs):\n\n        super().__init__(return_bv=return_bv,\n                         bipolar_input=bipolar_input,\n                         prob_er=prob_er,\n                         erasure_prob=None,\n                         seed=seed,\n                         inc_seq_desc=None,\n                         replace_inf=False,\n                         return_llrs=is_logit,\n                         dtype=dtype,\n                         **kwargs)\n\n```"
"To set up Sionna for the experiments, several components are required. The following Gaussian LLR source and LDPC BP Decoder.\n\nGaussian LLR Source:\nThe Gaussian LLR source is part of the Sionna library and can be instantiated simply as follows:\n\n```python\nllr_source = sn.utils.GaussianLLRSource()\n```\n\nThe Gaussian LLR source generates tensors containing Gaussian LLRs on-the-fly. It does not need an explicit input or shape as parameters.\n\nLDPC BP Decoder:\nThe LDPC BP decoder is not part of the Sionna library. It is known that to implement the belief propagation (BP) decoder for LDPC codes, one can make use of `sionna.fec.ldpc.decoding.LDPCBPDecoder`. This class implements the belief propagation (BP) decoder for iterative decoding of LDPC codes.\n\nIn order to initialize the LDPC BP Decoder, the following parameters are needed:\n\n- `pcm` (ndarray): The parity-check matrix for the given code. Its shape must be `[n-k, n]` and the code is assumed to be an `n`-bit code.\n- `max_iter` (int): The maximum number of iterations for the decoder. If no `iterations_per_call` is provided, the number of iterations to be executed is `max_iter`. If `stop_when_number_of_check_node_updates` is provided and `iterations_per_call` is `None`, the decoder stops after `stop_when_number_of_check_node_updates` check node updates.\n- `iteraions_per_call` (int): If this is provided, the decoder returns after `iterations_per_call` check node updates.\n- `stop_when_number_of_check_node_updates` (int): If this is provided, the decoder stops after `stop_when_number_of_check_node_updates` check node updates. It is assumed that `iteraions_per_call` is also provided to specify the number of BP decoding operations to be carried out between output checks.\n- `check_node_update` (str): Specifies the update rule used for check node updates. Must be one of \"min_sum\" or \"spa\" (default).\n- `bit_channel_llr` (bool): If `True`, the decoder returns LLRs for the transmitted bits. If `False`, the decoder returns hard-decided bits.\n- `cn_type` (str): Allows to implement the CN update equation from `y` to `x` by using a single non-linearity. The available types are \"boxplus\" and \"mn\" (default), referring to the combinatorial and bit flipping decoding algorithms, respectively.\n- `hard_out` (bool): If `True`, the decoder returns hard-decided codeword estimates. Defaults to `False` and only has an effect if `bit_channel_llr` is `False`.\n- `track_exit` (bool): If `True`, the decoder tracks EXIT characteristics of the implemented LDPC code. This requires the a-posteriori information `L` to be returned. Defaults to `False`.\n- `num_bits_per_symbol` (int): The number of bits per symbol for non-binary LDPC codes. For quick setup, `num_bits_per_symbol=1` can be used to indicate that the codeword values are affected by higher-weight LDPC code properties.\n- `hard_out` (bool): If `True`, the decoder returns hard-decided information bits instead of symbols. Defaults to `False`.\n- `verbose` (bool): If `True`, the decoder prints additional information during decoding. Defaults to `False`.\n\nPlease note that the provided context does not include direct instructions for using the Sionna library. It is implied from the provided instructions that the library can be used through its API, but the exact methods and functions to use are not described. For detailed operation and implementation instructions, the official documentation or source code of the Sionna library should be consulted."
"The PUSCHDMRSConfig class in the Sionna API is used to configure a Transformed Codebook (TCB) for a physical uplink shared channel (PUSCH), which is utilized in 5G NR. The physical uplink channel (PUCCH) and physical uplink shared channel (PUSCH) standards are implemented for models that are developed on top of the 3rd Generation Partnership Project (3GPP) 5G NR standard.\n\nThe PUSCHDMRSConfig class is used to set various configuration parameters for the DMRS (Demodulation Reference Signal) utilized in a PUSCH slot. These parameters include the number of CDM groups, the amount of DMRS ports, the set of configured DMRS ports, the mapping type, the number of RBs, the length, and the time-lag of the DMRS, etc.\n\nAll the configurable properties of the PUSCHDMRSConfig class can be provided as arguments during the initialization or later on. They include the above-mentioned parameters like the number of CDM groups, the amount of DMRS ports, the set of configured DMRS ports, the mapping type, the number of RBs, the length, the time-l of the DMRS, etc.\n\nThere are also non-configurable properties that are derived from the above parameters, such as the frequency density, the largest group offset, the number of CDM groups without data, the spatial configuration, etc. These properties cannot be set by the user directly but are computed from the above set parameters.\n\nIn the code snippets provided in the context, one can see how to create an instance of the PUSCHDMRSConfig class, passing the required parameters to the constructor, and how to access and set the various properties of the class. For example:\n\n```python\npusch_dmrs_config = PUSCHDMRSConfig(num_cdm_groups=1,\n                                      length=1,\n                                      dmrs_port_set=1)\npusch_dmrs_config.dmrs_config_type = 2\n```\n\nIn the snippet above, a PUSCHDMRSConfig object is created with one CDM group and a length of 1. The set of configured DMRS ports is set to 1, and the mapping type is set to 2. The properties of the PUSCHDMRSConfig class are then accessed and set as required."
"To demonstrate encoding of information bits using an instance of `ConvEncoder` in the `Sionna` package, we will follow these steps:\n\n1. Import the necessary modules and classes, including `tf`, `Layer`, and the `ConvEncoder` class from `sionna.fec.conv`.\n2. Initialize the `ConvEncoder` with the desired generator polynomials and the rate of the code.\n3. Prepare a sequence of information bits to be encoded.\n4. Encode the information bits using the `ConvEncoder` instance.\n\nHere is the Python code that accomplishes these steps:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras import Layer\nfrom sionna.fec.conv import ConvEncoder\n\n# Initialization of the ConvEncoder\nencoder = ConvEncoder(conv_gen=polynomial_selector(2, rate=1/2, constraint_length=3),\n                        layer='rsc',\n                        terminate=False)\n\n# Information bits to be encoded, size (bs, k)\nu = tf.constant([[0,1,0,1,1,0],\n                 [1,0,1,1,0,1]], dtype=tf.float32)\n\n# Encode the information bits\nc = encoder(u)\n```\n\nTo depict the shapes and types of the input and output tensors, we will print them:\n\n```python\nprint(\"Shape of u:\", u.shape)\nprint(\"Shape of c:\", c.shape)\n```\n\nThe output will show the shapes of the information bits `u` and the encoded codeword `c`. The types of the tensors can be determined using `u.dtype` and `c.dtype` for the respective `u` and `c` tensors. \n\nPlease note that the `polynomial_selector` function is not directly provided in the context. It's assumed to be a function that returns the generator polynomials for convolutional codes based on the specified rate and constraint length when called with `2` as the first argument. The `ConvEncoder` is initialized with `rsc` as the `layer` argument to create a recursive systematic convolutional encoder.\n\nTo run the code with the Sionna package, it is necessary to have the compatible versions of Tensorflow and Python as per the requirements specified in the context."
"The Distributed Multi-Cell Reference Signal (DMRS) Configuration in the Sionna library is used to set up pilot sequences for channels estimation in an OFDM system. It is a part of the  DMRSSetup  component. The DMRS configuration is dependent on the PUSCHConfig and PDSCHConfig set in the provided Sionna API.\n\nTo visualize the pilot pattern, you can use the PlotPilotPattern class within the DMRSSetup component. This will generate a plot for the selected transmitter and antenna port, showing the indices of the subcarriers where pilot symbols are transmitted.\n\nThe following code snippets from the context provide a high-level overview of how to use the DMRS configuration and then visualize the pilot pattern.\n\nSet up the DMRS configuration:\n\n```python\ndmrs_setup = DMRSSetup(pusch_config, dmrs_port_set, num_cdm_groups_without_data=1, frequency_position=True, \n                        cdm_group_blocking=False, length='L4')\n```\n\nSet the Pilot pattern:\n\n```python\npilot_pattern = PilotPattern(dmrs_setup, \"kronecker\")\n```\n\nOr, youjson may primarily work on the `PUSCHConfig` so you can define it within the `PUSCHConfig`:\n\n```python\npusch_config = PUSCHConfig(..., dmrs_configuration=dmrs_configuration)\n```\n\nThen visualize the pilot pattern:\n\n```python\npilot_pattern.plot(show_indices=True)\n```\n\nThis will generate a plot showing the pilot pattern for all transmitters and their respective antenna ports. The parameter `show_indices=True` will also print the indices of the subcarriers on the figure."
"To visualize common scattering patterns using the Sionna package, you can make use of the built-in functionality in Sionna for radio propagation simulation. Here is a general step-by-step guide:\n\n1. Import the necessary modules and set up the environment. This includes installing the Sionna package if it's not already installed.\n\n```python\nimport os\nimport sionna\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n```\n\n2. Initialize the necessary Sionna components, such as the `Scene`, `Transmitter`, `Receiver`, `RadioMaterial`, `Camera`, and others as needed.\n\n3. Load a scene using the `load_scene` function. The scene should contain the necessary elements for the radio propagation simulation, such as the geometry and materials of the objects in the scene.\n\n4. Configure the `Transmitter` and `Receiver` classes to use the desired antenna array configuration. This includes the number of rows and columns, the type of polarization, the antenna pattern, and the carrier frequency.\n\n5. Add a `Transmitter` and `Receiver` to the scene using the `add` method.\n\n6. Set the viewing direction in the scene.\n\n7. Compute the propagation paths using the `compute_paths` method. This method will return a `Paths` object that contains the computed paths.\n\n8. Generate a pattern of received power for each receiver using the `apply` method on the `Paths` object. You can specify a different number of samples for each link, which allows you to simulate different types of receivers with different bandwidths.\n\n9. Create a visualization of the scene using the `preview` method. You can specify the `paths` and the `resolution` for the computation.\n\n10. Run the visualization.\n\nPlease note that the specific code to accomplish these steps will depend on the specific requirements of your simulation, such as the scene geometry and the scattering patterns you want to visualize. The Sionna package provides a wide range of functionality for radio propagation simulation, so you can adjust the parameters and methods as needed."
"To handle import errors for the Sionna package, you can use a try-except block in Python. Here's how you can do it:\n\n1. First, attempt to import the Sionna package using the `import` statement inside a `try` block.\n\n```python\ntry:\n    import sionna\n```\n\n2. If the import is successful, the code execution will continue. However, if the import fails, an `ImportError` will be raised.\n\n3. You can catch this `ImportError` using an `except` block. Inside this block, you can handle the error. In this case, the error is handled by installing the Sionna package using the `pip install` command.\n\n```python\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna\n```\n\n4. After the package is installed, you can attempt to import it again.\n\nThis method ensures that if the Sionna package is not already installed, it will be installed automatically. This can be particularly useful in a Jupyter notebook or other interactive Python environments."
"To import the necessary Sionna APIs, you can follow the steps below:\n\n1. Import the `os` module. This module provides a way of using operating system dependent functionality like reading or writing to the environment.\n\n```python\nimport os\n```\n\n2. Set the environment variables for CUDA and TensorFlow. You can set the `CUDA_VISIBLE_DEVICES` variable to an empty string to use the CPU instead of the GPU. You can also set the `TF_CPP_MIN_LOG_LEVEL` variable to '3' to avoid warnings from TensorFlow.\n\n```python\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n```\n\n3. Import the Sionna package. If the package is not already installed, install it using pip.\n\n```python\ntry:\n    import sionna\nexcept ImportError as e:\n    os.system(\"pip install sionna\")\n    import sionna\n```\n\n4. Import the required Sionna APIs.\n\n```python\nfrom sionna.mimo import StreamManagement\nfrom sionna.ofdm import ResourceGrid, ResourceGridMapper, LSChannelEstimator, LMMSEEqualizer\nfrom sionna.ofdm import OFDMDemodulator, ZFPDetector, RemoveNulledSubcarriers\nfrom sionna.channel.tr38901 import Antenna, AntennaArray, CDL, UMi, UMa, RMa\nfrom sionna.channel import PoleResonator, SmallScaleFading, LargeScaleFading, FSSIMultiCM\nfrom sionna.channel import generate_pole_resonator_rank_one, generate_uma_nv2_55\nfrom sionna.channel.spatial_correlation import exp_corr, linear_corr\nfrom sionna.channel.frequency_corr import flat_fd_corr, perfect_fd_corr\nfrom sionna.channel.channel_model import ChannelModel\nfrom sionna.channel import time_to_ofdm, ofdm_to_time\nfrom sionna.channel.utils import subcarrier_frequencies, cir_to_ofdm_channel, cir_to_time_channel\nfrom sionna.fec.ldpc.encoding import LDPC5GEncoder\nfrom sionna.fec.ldpc.decoding import LDPC5GDecoder\nfrom sionna.mapping import Mapper, Demapper\nfrom sionna.utils import BinarySource, ebnodb2no, sim_ber\nfrom sionna.utils.metrics import compute_ber\n```\n\nPlease note that the exact import statements may vary depending on the specific requirements of your project."
"Here is the detailed information of Descrambler:  \nScrambler(seed=None, sequence=None, binary=True, keep_state=True, dtype=tf.float32, enable_scrambling=True, enable_sampling=True, x0=None, **kwargs)  \n  \nDescrambler for a given binary sequence or its associated generator.  \n  \nThe class inherits from the Keras layer class and can be used as layer in a Keras model.  \n  \nFor initialization, either `seed` or `sequence` must be provided. If `seed` is provided, then a random sequence of length `n_bits` is generated and used as scrambling sequence. If `sequence` is provided, it must be an explicit list of +/- 1 symbols.  \n  \nNote that the scrambling sequence is on per bit basis, i.e., each bit is flipped according to $(-1)^{c_n}$ where $c_n$ is the $n^{th}$ bit of the scrambling sequence.  \n  \nThe class also inherits from the Descrambler class which provides the basic structure for a descrambler in Sion. For a descrambler, it can be ensured that the scrambler and descrambler are symmetric by providing either the same seed or sequence (and also the same binary value).  \n  \n### Parameters\n\n- **seed** (int): Random seed for generating the scrambling sequence. If `sequence` is provided, the `seed` is not used and can be set to `None`. Defaults to `None`.\n\n- **sequence** (list of int): Explicit scrambling sequence. Only required if `seed` is not provided. Defaults to `None`.\n\n- **binary** (bool): Indicates whether binary sequences are handled, i.e., bit-segmented operations are processed. Defaults to `True`.\n\n- **keep_state** (bool): Indicates whether the scrambling sequence is constant and should be returned. Defaults to `True`.\n\n- **dtype** (tf.DType): Defaults to `tf.float32`. Defines the datatype for internal calculations and the output datatype.\n\n- **enable_scrambling** (bool): If `False`, no scrambling is applied. Defaults to `True`.\n\n- **enable_sampling** (bool): If `False`, the sequence is fixed and does not change. Defaults to `True`.\n\n- **x0** (nd-array): Defaults to `None`. If provided, the internal scrambling sequence is set to this value. Requires `keep_state=True`.\n\n### Input\n\n- **(x, seed)**: Either Tuple or Tensor:\n    - **x** ([..., n], tf.float): Signal to be descrambled.\n    - **seed** (int): Overrules the seed defined during initialization.\n\n### Output\n\n- **(x, seq)**:\n    - **x** ([..., n], tf.float): Scrambling result.\n    - **seq** ([..., n], tf.float): If `keep_state` is `True`, returns the scrambling sequence.\n\n### Properties\n\n- **n** (int): Number of chips per code symbol.\n\n- **permutate_seq**: Method that returns a permuted version of the sequence.\n\n- **scrambling_sequence** (tf.float): Returns the (if necessary) scrambled sequence."
"To demonstrate how to perform Polar decoding using an instance of `Polar5GDecoder` with log-likelihood ratios (LLRs) as inputs, you can follow a process that involves generating LLRs for codeword bits and then feeding these LLRs into the decoder. The following steps will walk you through the process:\n\n1. Import the necessary modules and initialize the Polar code parameters, including the generator matrix and the frozen positions.\n\n2. Create an instance of the `Polar5GDecoder` class from the `sion.decoding.polar` module, setting the `input_llr` parameter to `True` to indicate that log-likelihood ratios will be provided as inputs to the decoder.\n\n3. Generate LLRs for the codeword bits using a method like `generate_llrs` from the `sion.utils` module. The generated LLRs will typically be of shape `[k]`, where `k` is the number of information bits.\n\n4. Decode the received LLRs using the `Polar5GDecoder` instance. Since we've set `input_llr` to `True`, the decoder will expect an input tensor containing the LLRs of the received symbols.\n\n5. Process the output of the decoder, which will be a tensor containing hard-decided estimations of all `n` codeword bits. You may use a function like `hat2bit` from the `sion.utils` module to threshold the values to obtain bit estimates.\n\nHere is a simplified illustration of the steps, without actual code:\n\n```python\n# Step 1: Initialize code parameters\nn = ...\nk = ...\nfrozen_pos = ...\n\n# Step 2: Create decoder instance\ndecoder = Polar5GDecoder(frozen_pos, n, input_llr=True)\n\n# Step 3: Generate LLRs for the codeword bits\nllr_source = UniformPriorSource()\nllr = llr_source([[1, k], tf.float32])  # Pseudo code, actual implementation may vary\n\n# Step 4: Decode the received LLRs\ndecoded_bits = decoder(llr)\n\n# Step 5: Process the decoded bits\nbits = hat2bit(decoded_bits)  # Pseudo code, actual implementation may vary\n```\n\nIn this illustration, `UniformPriorSource` is used to generate the LLRs, which simulates the scenario where the LLRs are evenly distributed (i.e., no a priori information on the bits is available). You should replace it with the appropriate method based on your actual use case. Also, the tensor shapes are not provided in the context, so you'll need to adjust the code according to your specific requirements."
"To load a scene from a file using the `load_scene()` function present in the `rt` module of the Sionna package, you need to specify the file path as an argument. Here's how you can do it:\n\n```python\nscene = load_scene(sionna.rt.scene.munich)\n```\n\nIn the example above, an integrated scene of Munich is loaded from the Sionna repository using the `load_scene()` function. Substitute `sionna.rt.scene.munich` with the path to your scene file.\n\nIt's important to note that the `load_scene()` function returns a `Scene` object, which encapsulates:\n\n1. The configuration and components of the scene.\n2. Provides methods for the radio and material computation.\n3. Manages the placement and computation of `Transmitter`, `Receiver`, and `Camera` objects, along with their associated `Antenna` objects.\n\nOnce the scene is loaded, you can interact with it as needed. This might include computing propagation paths, coverage maps, aligning antennas, or rendering the scene to a file or preview. The specific actions depend on the desired simulation or visualization."
