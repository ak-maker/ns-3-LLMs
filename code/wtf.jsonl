"When creating a `Polar5GEncoder` instance in Sionna, the parameters `k` and `n` are required to define the number of information bits per codeword and the codeword length, respectively. These parameters are necessary for setting up the 5G compliant Polar encoder that will be used to encode the provided information bit sequences.\n\nHere is how you define these parameters when creating a `Polar5GEncoder` instance:\n\n```python\nfrom sionna.fec.polar.encoding import Polar5GEncoder\n\n# Define the number of information bits per codeword and the codeword length\nk = ...  # Replace ellipsis with the actual value for the number of information bits\nn = ...  # Replace ellipsis with the actual value for the codeword length\n\n# Create a Polar5GEncoder instance with the defined k and n\nencoder = Polar5GEncoder(k=k, n=n)\n```\n\nReplace the ellipsis (`...`) with the actual integer values appropriate for the parameters `k` and `n` that you wish to use. It is important to ensure that `k` does not exceed the maximum value allowed for the specific Polar codeword segment size defined in the 3GPP TS 38.212 standard, and that both `k` and `n` are integers."
"To import DeepMIMO and generate a dataset, follow these steps:\n\n1. Import the DeepMIMO module. If it's not installed, install it using pip:\n\n```python\ntry:\n    import DeepMIMO\nexcept ImportError as e:\n    # Install DeepMIMO if package is not already installed\n    import os\n    os.system(\"pip install DeepMIMO\")\n    import DeepMIMO\n```\n\n2. Configure the DeepMIMO parameters for the dataset. This includes the scenario, the number of propagation paths, the active basestations, and the user rows to generate channels for.\n\n3. Generate the DeepMIMO dataset using the configured parameters:\n\n```python\nDeepMIMO_params = DeepMIMO.default_params()\nDeepMIMO_params['dataset_folder'] = r'./scenarios'\nDeepMIMO_params['scenario'] = 'O1_60'\nDeepMIMO_params['num_paths'] = 10\nDeepMIMO_params['active_BS'] = np.array([6])\nDeepMIMO_params['user_row_first'] = 400\nDeepMIMO_params['user_row_last'] = 450\nDeepMIMO_params['bs_antenna']['shape'] = np.array([16, 1, 1])\nDeepMIMO_params['ue_antenna']['shape'] = np.array([1, 1, 1])\nDeepMIMO_params['OFDM_channels'] = 0\nDeepMIMO_dataset = DeepMIMO.generate_data(DeepMIMO_params)\n```\n\nThis code will import the DeepMIMO module and generate a dataset using the specified parameters. The dataset will contain the channels for different locations of the users and basestations. The layout of the scenario will be shown in the provided context."
"To plot the weights after training and compare the new Bit Error Rate (BER) performance, you can use the following steps:\n\n1. Plot the Weights: The weights of the model can be plotted after training using the `show_weights()` function of the decoder object in the model. This function will display the learned weights for each parity bit position.\n\n```python\nmodel.decoder.show_weights()\n```\n\n2. Compare the new BER Performance: The new BER performance after training can be compared by simulating the model and plotting the BER. This can be done by calling the `simulate()` function of the `ber_plot` object. The `simulate()` function will simulate the model for a range of SNR values and store the BER results internally, which can then be plotted using the `plot()` function of the `ber_plot` object.\n\n```python\nber_plot.simulate(model, \n                  ebno_dbs=np.linspace(EBN0_DB_MIN, EBN0_DB_MAX, 20), \n                  batch_size=SIM_BATCH_SIZE, \n                  num_target_block_errors=100, \n                  show_fig=False)\nber_plot.plot()\n```\n\n3. Save the Plot: You can save the plot for future reference using the `savefig()` function of the `ber_plot` object.\n\n```python\nber_plot.plot()\nplt.legend([\"Before Training\", \"After Training\"])\nplt.title(\"BER Comparison Before and After Weight's Training\")\nplt.xlabel(\"SNR (dB)\")\nplt.ylabel(\"Block Error Rate\")\nplt.grid()\nplt.savefig(\"BerComparison.png\")\n```\n\nPlease note that the exact code may vary depending on the specific model and plot functions used in your setup. The code provided above is a general approach and should be adjusted to fit your particular case."
"To create a Polar transformation matrix of Kronecker power `n_lift` using the `generate_polar_transform_mat` function from sionna.fec.polar.utils, the following steps should be taken:\n\n1. Import the required function from the appropriate library.\n```python\nfrom sionna.fec.polar.utils import generate_polar_transform_mat\n```\n\n2. Determine the value of `n_lift`, which specifies how often the matrix will be lifted by a Kronecker power operation. For a given matrix `A`, the Kronecker power operation with `n` will create a new matrix where each element of `A` is replaced by the `n`th power of itself.\n\n3. Call the `generate_polar_transform_mat` function with `n_lift` as the parameter.\n```python\n# n_lift specifies how often the matrix will be lifted\ntransform_mat = generate_polar_transform_mat(n_lift)\n```\n\nReplace the `n_lift` with the integer value for which you want to create the Polar transformation matrix. The value of `n_lift` must be in the range from 0 to 10 (inclusive), as it is specified as an integer.\n\nThe function will return an ndarray that represents the Polar transformation matrix after the Kronecker operations. This matrix can then be used as needed, e.g., in Polar encoding for 5G NR LDPC codes as discussed in the Sionna framework documentation. \n\nPlease ensure that you use the function within the appropriate error limits of the matrix dimensions as mentioned in the original documentation, and use the returned matrix in a way compatible with the further operations you intend to perform."
"Here is the definition of LayerDemapper: sionna.nr.LayerDemapper(layer_mapper, num_bits_per_symbol=1, **kwargs)  \nAnd the source code is, [LayerDemapper](https://nvlabs.github.io/sionna/_modules/sionna/nr/layer_mapping.html#LayerDemapper)  \n```python\nclass LayerDemapper(Layer):\n    \"\"\"Layer demapper for a MIMO OFDM link.\n\n    Connected to a :class:`~sionna.nr.LayerMapper` that uses a\n    :class:`~sionna.ofdm.ResourceGrid` to map modulated data streams\n    to OFDM resource grid.\n\n    In multi-cell MIMO links, the :class:`~sionna.nr.Precoder`\n    and :class:`~sionna.nr.PrecodingLayerMapper` must be used,\n    rather than the :class:`~sionna.nr.LayerMapper` and\n    :class:`~sionna.nr.LayerMapper`,\n\n    Parameters\n    ----------\n    layer_mapper : :class:`~sionna.nr.LayerMapper`\n        Associated layer mapper.\n\n    num_bits_per_symbol : int, 1 (default)\n        Modulation order. Assumed to be equal for all data streams and\n        defaults to 1. Only required for ``method`` = ``bit``.\n\n    Input\n    -----\n    : [batch_size, num_rx, num_streams_per_rx, num_ofdm_symbols, fft_size, data_sym], tf.complex or tf.float\n        Demapped resource grid.\n        If ``no`` , the demapper does not compute hard-decided bit values\n        but only log-likelihood ratios (LLRs).\n        The shape, data types and values of the ``[...num_streams_per_rx, data_sym]``\n        portion of the input remain unchanged by the layer demapper.\n\n    Output\n    ------\n    : [batch_size, num_rx, num_streams, num_data_symbols * num_bits_per_symbol], tf.float\n        Demapped data streams. Streams are flattened (within a\n        batch) across the last dimension from\n        ``[num_streams_per_rx, data_sym, num_bits_per_symbol]`` to\n        ``[num_streams_per_rx, data_sym*num_bits_per_symbol]``.\n\n    Note\n    ----\n    As it is more convenient to apply the layer demapper after\n    demapping symbols to LLRs, this layer groups the input sequence\n    into groups of ``num_bits_per_symbol`` before restoring the\n    original symbol sequence. This behavior can be deactivated\n    by setting ``num_bits_per_symbol`` =1.\n    \"\"\"\n\n    def __init__(self,\n                 layer_mapper,\n                 num_bits_per_symbol=1,\n                 **kwargs):\n\n        assert isinstance(layer_mapper, LayerMapper), \\\n                         \"layer_mapper must be a LayerMapper\"\n        self._layer_mapper = layer_mapper\n\n        assert isinstance(num_bits_per_symbol, int), \\\n                         \"num_bits_per_symbol must be int.\"\n        self._num_bits_per_symbol = num_bits_per_symbol\n        super().__init__(**kwargs)\n\n    #########################################\n    # Public methods and properties\n    #########################################\n\n    @property\n    def layer_mapper(self):\n        \"\"\"Associated ``LayerMapper``\"\"\"\n        return self._layer_mapper\n\n    @property\n    def num_bits_per_symbol(self):\n        \"\"\"Num bits per symbol ``: int``\"\"\"\n        return self._num_bits_per_symbol\n\n    def call(self, inputs):\n        # Save datatype for resource grid multiplication\n        dtype = inputs.dtype\n\n        # Number of symbols for which the channel was estimated\n        num_est_symbols = self._layer_mapper.num_est_syms\n\n        # Apply LS channel estimation at pilot subcarriers\n        y, h_err_var, no = channel_ls_estimation(inputs, num_est_symbols)\n\n        # Resource grid of channel estimates\n        # (HLS, the layer mapper already computed h_ls_var)\n        h_ls = tf.gather(y, self._layer_mapper.pilot)\n\n        # Build the channel covariance matrix R\n        r = tf.cast(tf.linalg.diag(h_err_var), h_ls.dtype)\n        r = tf.expand_dims(r, -1)\n        r = tf.tile(r, [1, 1, 1, tf.shape(h_ls)[-1]])\n        r = tf.transpose(r, [0,1,3,2])\n\n        # Build the channel matrix H\n        h = h_ls\n        h = tf.expand_dims(h, -1)\n        h = tf.tile(h, [1, 1, 1, tf.shape(r)[-2]])\n        h = tf.transpose(h, [0,1,3,2])\n        # (h,h_err_var) = self._channel_sampler(s) # only for real\n\n        # Demapping\n        if self._layer_mapper.modorder is not None:\n            # Reshape to [..., num_streams_per_rx, data_symbols]\n            h = tdl.utils.pack_symbol_indices(h.shape, 2,\n                                              self._layer_mapper.symbol_ind)\n            h = tf.squeeze(h, 1+self._layer_mapper.cyclic_prefix)\n\n            # Let's compute the effective noise variance for each receiver\n            # and each stream\n            no_eff = tf.reduce_sum(no, axis=-2)\n            no_eff = tf.expand_dims(no_eff, -1)\n            no_eff = tf.tile(no_eff, [1, 1, tf.shape(h)[-1]])\n            no = tf.expand_dims(no, -1)\n            no = tf.tile(no, [1, 1, tf.shape(h)[-1]])\n\n            # Prepare for list processing\n            h_list = tf.unstack(h, num=self._layer_mapper.num_streams, axis=1)\n            no_list = tf.unstack(no, num=self._layer_mapper.num_streams,\n                                 axis=1)\n            c_list = tf.unstack(self._layer_mapper.c, num=self._layer_mapper.num_streams,\n                               axis=1)\n\n            llr = tf.zeros_like(c_list[0].real) # only used for init\n            llr = tf.stack((tf.zeros_like(llr),\n                            tf.zeros_like(llr)), axis=-1)\n            llr = tf.tile(llr, [tf.shape(h)[0], 1]) # broadcastable shape\n\n            # Go through all streams and compute LLR\n            for h_, no_, c_ in zip(h_list, no_list, c_list):\n                llr_ = llr_max_metric(h_, no_, h_err_var)\n                llr_ = jacobian_max_metric(llr_, h_, no_, h_err_var)\n                llr_ = jacobian_max_metric(llr_, h_, no_, h_err_var)\n                llr_ = tf.where(c_.real > 0, llr_, -30.) # mask padding symbols\n                llr_ = add_parity_pilots(llr_, h_ls, c_, no_eff)\n                llr_ = remove_parity_pilots(llr_)\n                llr_ = puncture(llr_, c_)\n                llr_ = precoder_puncturer_prg(h_, llr_, no_, c_, 0)\n                llr_ = remove_nulled_subcarriers(llr_, h_ls, \"ls\")\n                llr_ = reorder_prg_to_rg(h_ls, llr_)\n                llr_ = tdl.flatten_last_dims(llr_)\n                llr_ = tf.cast(llr_, dtype)\n                llr += llr_\n\n            # All LLRs in one tensor: [..., num_streams_per_rx*num_bits_per_symbol]\n            llr = tf.concat(llr, axis=1)\n            llr = tdl.utils.expand_to_rank(llr, tf.rank(inputs), 0)\n\n        else:\n            shape = tf.shape(h)\n            h = tf.reshape(h, tf.concat([shape[:-2],\n                                          [self._num_bits_per_symbol,\n                                           self._layer_mapper.modorder//2]],\n                                         axis=0))\n            c = tf.reshape(self._layer_mapper.c, tf.concat([shape[:-2],\n                                          [self._num_bits_per_symbol,\n                                           self._layer_mapper.modorder//2]],\n                                         axis=0))\n            s = tf.cast(tf.reshape(inputs, tf.concat([shape[:-1],\n                                       [self._num_bits_per_symbol,\n                                        num_est_symbols]])), dtype)\n            llr = llr_max_metric(s)\n            llr = tf.where(c.real > 0, llr, -30.) # mask padding symbols\n            llr = add_parity_pilots(llr, h, c, no)\n            llr = remove_parity_pilots(llr)\n            llr = puncture(llr, c)\n            llr = precoder_puncturer_prg(h, llr, no, 1) # The 1 is stream index\n            llr = tf.reshape(llr, tf.concat([shape[:-2],\n                                      [self._layer_mapper.num_streams,\n                                       num_est_symbols*self._num_bits_per_symbol]],\n                                      axis=0))\n            llr = tf.cast(llr, dtype)\n\n        return llr\n```  \n  "
"To create a PUSCHReceiver using the PUSCHTransmitter and simulate transmissions over the AWGN channel, follow these steps:\n\n1. First, create a PUSCH configuration with default settings:\n\n```python\npusch_config = PUSCHConfig()\n```\n\n2. Instantiate a PUSCHTransmitter from the PUSCHConfig:\n\n```python\npusch_transmitter = PUSCHTransmitter(pusch_config)\n```\n\n3. Create a PUSCHReceiver using the PUSCHTransmitter:\n\n```python\npusch_receiver = PUSCHReceiver(pusch_transmitter)\n```\n\n4. Set up the AWGN channel:\n\n```python\nchannel = AWGN()\n```\n\n5. Simulate transmissions over the AWGN channel. You can choose the batch size and the noise variance for the simulation:\n\n```python\nbatch_size = 16\nno = 0.1 # Noise variance\nx, b = pusch_transmitter(batch_size) # Generate transmit signal and info bits\ny = channel([x, no]) # Simulate channel output\nb_hat = pusch_receiver([x, no]) # Recover the info bits\n```\n\n6. Compute the Bit Error Rate (BER) using the helper function `compute_ber`:\n\n```python\nprint(\"BER:\", compute_ber(b, b_hat).numpy())\n```\n\nThis will print the Bit Error Rate for the simulated transmissions."
"Here is the definition of SymbolInds2Bits: sionna.mapping.SymbolInds2Bits(num_bits_per_symbol, dtype=tf.float32, **kwargs)  \n  \nThe source code link of SymbolInds2Bits is [SymbolInds2Bits](https://nvlabs.github.io/sionna/_modules/sionna/mapping.html#SymbolInds2Bits)\n\n```python\nclass SymbolInds2Bits(Layer):\n    \"\"\"Transforms symbol indices to their binary representations.\n\n    This class behaves like a Keras layer. It takes as input an integer tensor\n    of symbol indices and maps these indices to binary representation.\n    Note that the class only returns the second part of the\n    split triple of a similar [`Bits2Symbol`](./mapping.html#Bits2Symbol),\n    as it does not have access to the\n    (optionally) provided bit table that `Bits2Symbol` expects as input.\n\n    Parameters\n    ----------\n    num_bits_per_symbol : int\n        Number of bits per constellation symbol, e.g., 4 for QAM16.\n\n    dtype : tf.DType\n        Output dtype, either `tf.float32` or `tf.int32`.\n        Defaults to `tf.float32`.\n\n    Input\n    -----\n    (..., n), tf.int\n        Tensor of symbol indices to be converted into binary representation.\n        The symbol indices must be within the range [0,2**num_bits_per_symbol).\n\n    Output\n    ------\n    (..., num_bits_per_symbol), ``dtype``\n        The binary representation of the symbol indices.\n    \"\"\"\n    def __init__(self, num_bits_per_symbol, dtype=tf.float32, **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        assert dtype in [tf.int32, tf.float32, tf.float16, tf.uint8, tf.int8,\n                        tf.uint16, tf.uint32, tf.uint64, tf.int64], \\\n                            \"Unsupported dtype.\"\n\n        assert num_bits_per_symbol in range(1, 17), \\\n                \"num_bits_per_symbol must be in the range [1,16]\"\n\n        num_bits = num_bits_per_symbol\n        two = tf.constant(2, dtype=dtype)\n        # Create vector of exponents of two in decreasing order\n        exponents = tf.range(num_bits-1, -1, delta=-1, dtype=dtype)\n        exponents = tf.reshape(exponents, [1, -1])\n\n        # Generate a tensor with binary representation of the symbol indices\n        pow2exponents = two**exponents\n        self._template = tf.constant(\n                        np.fliplr([np.binary_repr(i, num_bits)\n                                  for i in range(2**num_bits)]),\n                        dtype=dtype)\n        self._template = pow2exponents * tf.cast(self._template, dtype=dtype)\n\n    #########################################\n    # Public methods and properties\n    #########################################\n\n    def call(self, symbol_inds):\n        symbol_bits = tf.gather(self._template, symbol_inds, axis=0)\n        return symbol_bits\n```  \n  "
"Here is the detailed information of TimeChannel:   \n  \nLink of the source code: https://nvlabs.github.io/sionna/_modules/sionna/channel/time_channel.html#TimeChannel    \n  \nsource code:\n```python\n#\n# SPDX-FileCopyrightText: Copyright (c) 2021-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n\"Layer for implementing the channel in the time domain\"\n\nimport tensorflow as tf\n\nfrom . import GenerateTimeChannel\nfrom .channel_model import ChannelModel\n\nclass TimeChannel(tf.keras.layers.Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"TimeChannel\n\n    Generate channel responses and apply channel in the time domain\n\n    This class inherits from the `Layer` class and can be used as layer\n    in a Keras model.\n\n    The channel output :math:`y` for an input signal :math:`x` and a\n    time-varying channel realization :math:`\\mathbf{h}=(h_0,\\dots,h_{N_t-1})`,\n    is computed as follows:\n\n    .. math::\n        y_b = \\sum_{\\ell = L_{\\text{min}}}^{L_{\\text{max}}} x_{b-\\ell} \\bar{h}_{b,\\ell} + w_b\n\n    where :math:`L_{\\text{min}}` and  :math:`L_{\\text{max}}` are the minimum and\n    maximum\n    delays, :math:`L_{\\text{max}} - L_{\\text{min}} + 1 = N_t`, :math:`\\bar{h}_{b,\\ell}`\n    is the\n    :math:`\\ell^{th}` tap of the :math:`b^{th}` channel sample,\n    and :math:`w_b` the additive noise.\n\n    For the implementation of the channel, we assume a\n    baseband system, i.e., the channel transfer function\n    is assumed to filter some white Gaussian noise :math:`(w_b)`.\n    The noise power is per complex dimension.\n\n    In the case of a multi-path channel, the channel filter\n    ```\n    h_{b,\\ell} = a_{m}(b) \\bar{h}_{m,\\ell}\n    ```\n    is the composition of a time varying channel realization\n    (see :class:`~sionna.channel.ChannelModel`) and the  tap\n    gains. The tap gains  :math:`a_{m}(b)` are generated by this class\n    and can be  used to scale the channel\n    (Single Tap Equalization). They are computed so that the total energy\n    per  channel coefficient is equal on average :math:`\\left(\\mathbb{E}\\left[ \\sum_{m=0}^{M-1} \\left| a_{m}(b) \\right|^2 \\right] = 1\\right)`.\n\n    If requested, the channel model can be made trainable, in which case\n    the channel impulse response is\n    computed using bi-orthogonality principle and the K-Factor, as done in [S. Sun,\n    M. Shitz, M. Rodwell, \u201cCapacity K-Factors and Measurement\n    Techniques for MIMO Wireless LAN Systems,\u201d Eleventh IEEE\n    International Symposium on Personal, Indoor and Mobile Radio Communications\n    (PIMRC), 2000.]. Training the channel requires to over-sample the\n    simulation in the time domain, which can be done using the\n    :class:`~sionna.channel.Oversampling` layer.\n\n    When adding this layer to a Keras model, the layer\n    can be used as is, or the parameters  of the ``call`` method\n    can be specified:\n\n    *Example:*\n    ```python\n    channel = TimeChannel(channel_model = my_channel_model,\n                          bandwidth = 1.4e6,\n                          num_time_samples = 7,\n                          l_min = -40,\n                          l_max = 40)\n    x = tf.complex([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7])\n    y = channel(x)\n    ```\n\n    Input\n    ------\n        (x, no) or x:\n            Tuple or Tensor: Channel input (x) and noise power (no)\n            The channel input is of shape\n            `[..., num_time_samples, num_tx_ant]`, tf.complex.\n            The noise power ``no`` is optional and can be a scalar or of\n            any shape that can be broadcasted to the shape of the channel\n            output, `[...,num_time_samples, num_rx_ant]`.\n            It must be given if 'x' is provided as a tuple.\n\n    Output\n    -------\n        : Channel output, of shape\n            `[..., num_time_samples + l_max - l_min, num_rx_ant]`, tf.complex.\n\n        (x, h, no):\n            Tuple of (x, channel responses, noise power), where:\n            `x` is the channel input of shape\n            `[..., num_time_samples, num_tx_ant]`, tf.complex.\n            `h` is the channel responses of shape\n            `[..., num_rx_ant, num_tx_ant, num_time_samples + l_max - l_min]`, tf.complex.\n            `no` is the noise power, which is a scalar\n            `[]` (broadcastable to `x.shape`), tf.float.\n            It is only returned if ``return_channel=True``..\n\n    Note\n    ----\n        For numerical stability and to ensure that the total energy per\n        channel coefficient is as close\n        as possible to one, the channel impulse response is scaled by\n        ``1/num_rx_ant`` when given as input.\n    \"\"\"\n    def __init__(self, channel_model, trainable = False, transmit_spatial_rep = None, return_channel = False):\n        super().__init__()\n        self._channel_model = channel_model\n        self._gen_channel = GenerateTimeChannel(channel_model = channel_model,\n                                                trainable = trainable,\n                                                transmit_spatial_rep=transmit_spatial_rep)\n        self._return_channel = return_channel\n        # if we are supposed to return the channel, we need to annotate the\n        # function such that the TFGPUProfiler can be used.\n        if self._return_channel:\n            from tensorflow.python.eager import def_function\n            self.call = def_function.DynamicAnnotationHookFunction(self.call)\n\n    @property\n    def channel_model(self):\n        \"\"\"The :class:`~sionna.channel.ChannelModel` used to generate\n         channel realizations.\"\"\"\n        return self._channel_model\n\n    @property\n    def bandwidth(self):\n        \"\"\"The bandwidth of the channel impulse response in Hz\"\"\"\n        return self._bandwidth\n\n    @bandwidth.setter\n    def bandwidth(self, value):\n        self._gen_channel.bandwidth = value\n        self._bandwidth = value\n\n    @property\n    def delay_spread(self):\n        \"\"\"The delay spread [s]\"\"\"\n        return self._delay_spread\n\n    @property\n    def l_min(self):\n        r\"\"\"The smallest time-lag for the discrete complex baseband channel\n        ($L_{\\text{min}}$)\"\"\"\n        return self._l_min\n\n    @l_min.setter\n    def l_min(self, value):\n        self._gen_channel.l_min = value\n        self._l_min = self._gen_channel.l_min\n\n    @property\n    def l_max(self):\n        r\"\"\"The largest time-lag for the discrete complex baseband channel\n        ($L_{\\text{max}}$)\"\"\"\n        return self._l_max\n\n    @l_max.setter\n    def l_max(self, value):\n        self._gen_channel.l_max = value\n        self._l_max = self._gen_channel.l_max\n\n    @property\n    def transmit_spatial_rep(self):\n        r\"\"\"\n        Representation of the transmit antenna port.\n        None (default) implies a single  Port.\n        \"\"\"\n        return self._transmit_spatial_rep\n\n    @transmit_spatial_rep.setter\n    def transmit_spatial_rep(self, value):\n        self._transmit_spatial_rep = value\n        self._gen_channel.transmit_spatial_rep = value\n        # We set transmit spatial rep also in the  h_time() as we\n        # dont access the channel inputs in call().\n        if value is not None:\n            zeros = tf.zeros( shape=(tf.shape(value)[:-1] + [1],\n                                1),\n                            dtype = value.dtype)\n            self._x_rep = tf.concat([ value,  zeros] , axis=-1)\n\n\n    def call(self, inputs):\n        if self._return_channel:\n            x, no = inputs\n        else:\n            x = inputs\n\n        # We over-sample the channel in the time domain, we\n        # dont need this value but the code will throw an error if\n        # it is not set\n        sample_frequency = 1.0\n\n        # Delays are always relative to the first (or zero) sample of the\n        # input. We only need to update this offset if a new input is provided.\n        # Note that this does not change the channel. It is done to make\n        # the channel \"appear\" to be discrete complex baseband.\n        update_offset = True\n\n        if self._return_channel:\n            h, no = self._gen_channel([x, no, sample_frequency, update_offset])\n        else:\n            h = self._gen_channel([x, sample_frequency, update_offset])\n\n        # Apply the channel\n        x = tf.pad(x, [[0,0], [self.l_min, 0]], constant_values=0)\n        x = tf.nn.conv1d(x, tf.transpose(h, (1,2,0)), padding='full')\n\n        if self._return_channel:\n            return x, h, no\n        else:\n            return x\n```"
"This is the class representing an Antenna in a radio propagation simulation. The Antenna class defines the antenna's characteristics like pattern, polarization, polarization_model, and datatype. The antenna pattern can be predefined (like \"iso\", \"dipole\", \"hw_dipole\", \"tr38901\") or custom if a callable function is provided. The class can be single or dual polarized and for dual polarization, the directions can be cross-coupled. This class is part of the Sionna module and is typically used as a parameter in classes like Transmitter and Receiver for simulating propagation phenomena. \n\nAttributes of the 'Antenna' class include:\n\n1. pattern: The antenna pattern. \n2. polarization: Type of polarization (\"V\" for vertical, \"H\" for horizontal, \"VH\" for cross-polarized in the \"V\" direction and \"cross\" for cross-polarized with adjustable coupling). \n3. polarization_model: The polarization model to be used. \n4. polarization_model_1: Callable that defines the first polarization pattern. \n5. polarization_model_2: Callable that defines the second polarization pattern, if applicable. \n6. dtype: Datatype for computations. \n\nProperties of the 'Antenna' class include:\n\n1. patterns: List of antenna patterns. \n2. polarization: List of polarization for which to return the antenna patterns. \n3. polarization_model: Antenna polarization model. \n4. polarization_model_1: First antenna polarization pattern. \n5. polarization_model_2: Second antenna polarization pattern. \n\nMethods of this class include:\n\n1. show: Show the antenna pattern. \n2. visualize: Visualize the antenna patterns. \n3. radiation_pattern: Compute the radiation field for one or two polarization directions. \n\nPlease note that the class can be single or dual polarized and can have an adjustable coupling. The class datatype is typically complex float for the computations. The antenna patterns is generally defined in terms of spherical coordinates representing zenith and azimuth angles. The antenna polarization_model_1 and polarization_model_2 defines the antenna's radiation pattern in the first and second polarization direction respectively."
"Here is the source code of the class  [TurboTermination](https://nvlabs.github.io/sionna/_modules/sionna/fec/turbo/utils.html#TurboTermination):  \n\n```python\nclass TurboTermination():\n    r\"\"\"\n    Termination object, handles the transformation of termination\n    bits from the convolutional encoders to a Turbo codeword. Similarly,\n    it handles the transformation of channel symbols corresponding to the\n    termination of a Turbo codeword to the underlying convolutional codewords.\n\n    Parameters\n    ----------\n    constraint_length : int\n        Constraint length of the convolutional encoder used in the Turbo code.\n        Note that the memory of the encoder is :math:`\\mu` .\"\"\"\n    def __init__(self, constraint_length, conv_n=2, num_conv_encs=2,\n                 num_bit_streams=3):\n\n        super().__init__()\n        self._mu = constraint_length - 1\n        self._conv_n = conv_n\n        self._num_conv_encs = num_conv_encs\n        self._num_bit_streams = num_bit_streams\n\n        # Number of termination symbols for the convolutional codes.\n        self._num_term_syms = math.ceil(2*self._conv_n*self._mu /\n                                               self._num_bit_streams)\n\n        # # Precompute masks for bit and symbol streams\n        # masks = []\n        # for j in range(num_bit_streams):\n        #     bit_stream = tf.one_h(2**j, num_state)\n        #     symbol_stream = tf.bitwise.bitwise_or.reduce(\n        #                 tf.bitwise.bitwise_or(conv_ni, tf.expand_dims(bit_stream, -1)),\n        #                                             axis=-1)\n        #     symbol_stream = tf.expand_dims(symbol_stream, 0)\n        #     masks.append(symbol_stream)\n        # self._symbol_masks = masks\n\n        # masks = []\n        # for j in range(num_bit_streams):\n        #     bit_stream = tf.one_h(2**j, num_state)\n        #     symbol_stream = tf.floordiv(\n        #             tf.expand_dims(tf.reduce_sum(conv_ni*bit_stream, axis=-1), 0),\n        #                         1+conv_ni)\n        #     masks.append(symbol_stream)\n        # self._bit_masks = masks\n\n        convenc = ConvolutionalEncoder(gen=PolynomialEncoder(\n                                                gen=conv_gen.primitive_polynomial(\n                                                                self._mu)),\n                                        rate=1/self._conv_n,\n                                        constraint_length=self._mu)\n        self._conv_n_bits = convenc._n\n\n    @property\n    def num_conv_syms(self):\n        r\"\"\"Number of termination symbols for the convolutional codes.\n        One symbol equals ``_conv_n`` bits.\"\"\"\n        return self._num_term_syms\n\n    def get_num_term_syms(self):\n        r\"\"\"\n        Computes the number of termination symbols for the Turbo\n        code based on the underlying convolutional code parameters,\n        primarily the memory :math:`\\mu` .\n\n        Note\n        ----\n        It is assumed that one Turbo symbol implies ``_num_bit_streams`` bits.\n        \"\"\"\n        return self._num_term_syms\n\n    def term_bits_turbo2conv(self, term_bits):\n        r\"\"\"\n        This method splits the termination symbols from a Turbo codeword\n        to the termination symbols corresponding to the two convolutional\n        encoders, respectively.\n\n        Lets assume :math:`\\mu=4` and the underlying convolutional encoders\n        are systematic and rate-1/2, for demonstration purposes.\n\n        Let ``term_bits`` tensor, corresponding to the terminations symbols of\n        the Turbo codeword be as following:\n\n        .. math::\n            y = [x_1(K), z_1(K), x_1(K+1), z_1(K+1), x_1(K+2, z_1(K+2), x_1(K+3), z_1(K+3),\n            x_2(K), z_2(K), x_2(K+1), z_2(K+1), x_2(K+2), z_2(K+2), x_2(K+3), z_2(K+3), 0, 0]\n\n        The two termination tensors corresponding to the convolutional encoders\n        are:\n\n        .. math::\n            y[0,..., 2\\mu] = [x_1(K), z_1(K), x_1(K+1), z_1(K+1)]\n\n        .. math::\n            y[2\\mu,..., 4\\mu] = [x_1(K+2), z_1(K+2), x_1(K+3), z_1(K+3), x_2(K), z_2(K),\n                               x_2(K+1), z_2(K+1)]\n\n        The output from this method is a tuple of two tensors, each of\n        size :math:'2\\mu' and shape :math:`[\\mu,2]`.\n\n        :math:`[[x_1(K), z_1(K)]`\n\n        :math:`[x_1(K+1), z_1(K+1)]`\n\n        :math:`[x_1(K+2, z_1(K+2), x_1(K+3), z_1(K+3)]`\n\n        :math:`[x_2(K),z_2(K), x_2(K+1), z_2(K+1)]`\n\n        :math:`[x_2(K+2), z_2(K+2), x_2(K+3), z_2(K+3)]`\n\n        :math:`[0, 0)]`\n\n        Therefore, the output from this method is a single dimension vector\n        where all Turbo symbols are concatenated together.\n\n        :math:`[x_1(K), z_1(K), x_1(K+1), z_1(K+1), x_1(K+2, z_1(K+2), x_1(K+3),`\n\n        :math:`z_1(K+3), x_2(K),z_2(K), x_2(K+1), z_2(K+1), x_2(K+2),`\n\n        :math:`z_2(K+2, x_2(K+3), z_2(K+3), 0, 0]`\n\n        Input\n        -----\n        term_bits : tf.int32\n            2+D Tensor containing termination bits from the Turbo codeword\n\n        Output\n        ------\n        : tf.int32\n            Two tensors of channel outputs, corresponding to encoders 1 and 2,\n            respectively.\n        \"\"\"\n        # For zero termination, the code 0 padding is done such that\n        # the total length is divisible by num_bit_streams\n        conv_n = self._conv_n_bits*tf.cast(self._mu, dtype=tf.int32)\n        x_len_turbo = tf.shape(term_bits)[-1]\n        x_len_t1 = tf.floordiv(x_len_turbo, self._num_bit_streams)\n        x_len_1 = x_len_t1 + 2*self._conv_n\n        z_len_1 = x_len_1 - self._conv_n\n        # Also compute number of t1 symbols to handle zero termination\n        num_t1_syms = tf.floordiv(x_len_turbo, tf.cast(self._conv_n, tf.int32))\n\n        t_bits1 = term_bits[0:x_len_turbo : self._num_bit_streams]\n        t_bits2 = term_bits[0:x_len_turbo :\n                                          self._num_bit_streams*num_convencoders]\n        t_bits2 = term_bits[num_t1_syms*self._num_bit_streams :\n                           num_t1_syms*self._num_bit_streams +\n                                              z_len_1*num_convencoders:z_len_1]\n        return (t_bits1, t_bits2)\n\n    def __call__(self, term_bits_t1, term_bits_t2, zero_termination=True):\n        \"\"\"\n        This method implements the termination, i.e., it takes the termination\n        bit streams of the two convolutional encoders and multiplexes them\n        to form the Turbo codeword termination.\n\n        Let ``term_bits_t1`` and ``term_bits_t2`` be the termination bit streams\n        from the convolutional component 1 and 2, resectively.\n\n        For demonstration, we we assume :math:`\\mu=4` and zero termination\n        such that ``term_bits_t1`` and ``term_bits_t2``  are aas following:\n\n        ``term_bits_t1``:\n\n        .. code-block:: python\n\n            [x1(K), z1(K), x1(K+1), z1(K+1), x1(K+2, z1(K+2), x1(K+3), z1(K+3)]\n\n        ``term_bits_t2``:\n\n        .. code-block:: python\n\n            [x2(K), z2(K), x2(K+1), z2(K+1), x2(K+2), z2(K+2),  x2(K+3), z2(K+3)]\n\n        Input\n        -----\n        term_bits_t1 : tf.int32\n            2+D Tensor containing termination bits from turbo codeword, or\n            output of :meth:`TurboEncoder` where ``output_bits`` =` False`.\n\n        term_bits_t2 : tf.int32\n            2+D Tensor containing termination bit from turbo codeword, or\n            output of :meth:`TurboEncoder` where ``output_bits`` =` False`.\n\n        zero_termination: bool\n            If `True`, it is ensured that the Turbo codeword ends with a\n            zero bit. Defaults to `True`.\n\n        Output\n        ------\n        : tf.int32\n            1+D tensor of termination bits corresponding to the Turbo codeword.\n        \"\"\"\n        if zero_termination:\n            term_bits_t1 = tf.concat(\n                        [term_bits_t1, tf.zeros([1, self._conv_n], tf.int32)], 1)\n            term_bits_t2 = tf.concat(\n                        [term_bits_t2, tf.zeros([1, self._conv_n], tf.int32)], 1)\n\n        turbo_term_bits = tf.stack([term_bits_t1, term_bits_t2], axis=-1)\n        turbo_term_bits = tf.reshape(turbo_term_bits, shape=[-1, 2])\n\n        # Prepare term bit streams for output\n        t1_len = tf.shape(term_bits_t1)[-1] + 1\n        t2_len = tf.shape(term_bits_t2)[-1] + 1\n        t1_bits = turbo_term_bits[0, :t1_len]\n        t2_bits = turbo_term_bits[0, t1_len:t1_len+t2_len]\n\n        # Precompute masks for bit and symbol streams\n        masks = []\n        for j in range(self._num_bit_streams):\n            bit_stream = np.hstack( [np.zeros(self._mu-1),\n                                    np.ones(self._conv_n),\n                                    np.zeros(j) ])\n            symbol_stream = np.mod(\n                        np.dot(np.hstack([self._conv_ni, bit_stream]), self._gen) , 2)\n            symbol_stream = np.hstack([np.zeros(self._mu-1),\n                                        symbol_stream[j:self._mu + j])\n                                for j in range(2**self._mu)])\n            masks.append(symbol_stream)\n        mask = np.vstack(masks)\n\n        # termination to systematic bits\n        sys_bit_stream = tf.gather(mask, turbo_term_bits, axis=0,\n                                                    batch_dims=1,  validate_indices=False)\n\n        # systematic to 1st encoder systematic and parity bits\n        sys_cvi_1 = 2**np.reshape(np.arange(self._mu+1), [1, -1])\n        sys_bits_1 = tf.gather(sys_bit_stream, sys_cvi_1, axis=-1)\n\n        # 1st encoder systematic and parity bits to bit interleaved systematic\n        # and parity bits\n        sys_bits_1 = tf.transpose(sys_bits_1, perm=[1,0])\n        code_bits_1 = tf.reshape(sys_bits_1, [-1, self._conv_n])\n        code_bits_1i = tf.gather(code_bits_1, self._pi, axis=-1)\n\n        # 2nd encoder systematic and parity bits\n        sys_cvi_2  = tf.expand_dims(\n                        tf.expand_dims(2**np.reshape(\n                                            np.arange(2*self._mu), [1, -1]), 1), 0)\n                                                                , 0)\n        sys_bits_2 = tf.gather(mask, sys_cvi_2, axis=-1)\n        sys_bits_2 = tf.squeeze(sys_bits_2, 0)\n\n        # 2nd encoder systematic and parity bits to bit interleaved systematic\n        # and parity bits\n        sys_bits_2 = tf.transpose(sys_bits_2, perm=[1,0])\n        code_bits_2 = tf.reshape(sys_bits_2, [-1, self._conv_n])\n        code_bits_2i = tf.gather(code_bits_2, self._pi, axis=-1)\n\n        # gather all termination symbols\n        term_bits_turbo = tf.scatter_nd(self._pi,\n                                     code_bits_1i, shape=[2**self._mu])\n        term_bits_turbo = tf.stack([tf.squeeze(code_bits_1i),\n                                     tf.squeeze(term_bits_turbo)], axis=-1)\n        term_bits_turbo = tf.reshape(term_bits_turbo, shape=[-1])\n\n        # gather all channel symbols\n        channel_syms_turbo = tf.scatter_nd(self._pi,\n                                     code_bits_2i, shape=[2**self._mu])\n        channel_syms_turbo = tf.stack([tf.squeeze(channel_syms_turbo),\n                                        tf.zeros([2**self._mu], tf.int32)],\n                                        axis=-1)\n        channel_syms_turbo = tf.reshape(channel_syms_turbo, shape=[-1])\n\n        # ensure that Turbo codeword ends with a zero bit\n        if zero_termination:\n            term_bits_turbo = tf.concat(\n                        [term_bits_turbo, tf.constant([0], tf.int32)], 0)\n\n        output = tf.concat([term_bits_turbo, channel_syms_turbo], 0)\n\n        output = tf.cast(output, dtype=tf.int32)\n        return output\n```"
"To wrap everything in a Keras model for convenient Bit Error Rate (BER) simulations and comparison of model parameters, you can follow these steps:\n\n1. Instantiate a SpatialCorrelation object, which is part of the 'sionna.channel' module. This object models the spatial correlation in the channel.\n\n2. Instantiate a Model object, which is a class that inherits from tf.keras.Model. In the __init__() method of this class, you can initialize all the components of the system model. This includes the binary source, encoder, mapper, demapper, decoder, and channel components. You can also set the parameters for the system such as the code rate and the number of bits per symbol.\n\n3. Define the __call__() method of the model. This method should take as input a batch size and an Eb/No value (in dB). It should generate a batch of random bit vectors, encode them, map them to constellation symbols, transmit them over the channel at the specified Eb/No value, and then demap and decode them. The method should return the original bits and the decoded bits.\n\n4. After defining the model, you can simulate the system over a range of Eb/No values, calculate the BER for each Eb/No value, and then plot the BER. This can be done using the helper functions plot_ber() and simulate_ber().\n\nHere is a simplified version of the code:\n\n```python\nimport sionna\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Instantiate a SpatialCorrelation object\nspatial_corr = sionna.channel.RayleighBlockFading(num_rx=1, num_rx_ant=32, num_tx=4, num_tx_ant=1, correlation=sionna.channel.KroneckerCorrelation)\n    \n# Define the system model\nclass Model(tf.keras.Model):\n    # Initialization code here...\n\n    def __call__(self, batch_size, ebno_db):\n        # Generate random bits, encode them, map them to symbols, and transmit them over the channel\n        # ...\n\n        # Demap the received symbols and decode them\n        # ...\n\n        return b, b_hat\n\n# Simulate the BER over a range of Eb/No values\nebno_dbs = np.linspace(-5.0, 20.0, 20)  # dB\nber = simulate_ber(model, ebno_dbs, batch_size=10000, num_it=100, max_num_bit_err=10000)\n```\n\nPlease note that this is a simplified version of the code and you may need to adapt it to your specific needs. For example, you may need to add more parameters to the initialize the components of the system model, and you may need to define additional methods to encode and decode the data."
"The `build_type_grid` method of the `ResourceGrid` class is used to get a tensor indicating the type of each resource element in the grid. Here, a resource element refers to a carrier frequency and time or, more simply, a slot on a subcarrier. \n\nInput parameters to the `build_type_grid` method are:\n- **None**\n\nThe output of this method is a  `[num_tx, num_streams_per_tx, num_ofdm_symbols, fft_size]` tf.int32 tensor, which essentially indicates for each transmitter and respective stream the type of the resource element at a given position on the resource grid. \n\nIn the context, for instance, we see that for a 4-FFT resource grid with `num_guard_carriers = (5, 0)`, the `build_type_grid` method would return a tensor indicating for each transmitter and stream the type of the resource elements in this grid. This would look something like: \n\n`[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2]`\n\nHere, 0s indicate regular pilot symbols, 1s indicate pilot symbols with a prefixed trainable variable, 2s indicate nulled subcarriers, and 3s indicate the DC carrier. \n\nIt's important to note that the actual code snippet usage of this method is within the context of the Sionna package but might vary depending on the specific class implementation. For the exact function interpretation, one would need to refer to the Sionna's `ResourceGrid` class documentation. \n\nCould not extract original code for: build_type_grid  in the given context. \n  "
"The source code link of SymbolLogits2LLRs is [SymbolLogits2LLRs](https://nvlabs.github.io/sionna/_modules/sionna/mapping.html#SymbolLogits2LLRs).  \n  \nThe channel parameters of the class are:  \n- **method** (str): One of [\"app\",\"maxlog\"]. Specifies the method for computing the LLRs.\n- **num_bits_per_symbol** (int): The number of bits per constellation symbol, e.g., 4 for QAM16.\n- **hard_out** (bool): If True, the layer provides hard-decided bits instead of soft-values. Defaults to False.\n- **with_prior** (bool): If True, it is assumed that prior knowledge on the bits is available. This prior information is given as LLRs as an additional input to the layer. Defaults to False.\n- **dtype** (One of [tf.float32, tf.float64]): The dtype for the input and output. Defaults to tf.float32.\n\nThe input to the class SymbolLogits2LLRs is expected to be :\n- **logits or (logits, prior)** \u2013 Tuple:  \n  - **logits** ([,n], or [,n, num_points], tf.float) or (logits, prior) \u2013 Logits on constellation points.  \n  - **prior** ([,num_bits_per_symbol] or [,n, num_bits_per_symbol], tf.float) \u2013 Prior for every bit as LLRs. It can be provided either as a tensor of shape [num_bits_per_symbol] for the entire input batch, or as a tensor that is broadcastable to [,n, num_bits_per_symbol]. Only required if the with_prior flag is True.\n\nAnd the output of SymbolLogits2LLRs is :  \n- **LLRs** ([,n], tf.float32) or ([,n, num_bits_per_symbol], tf.float32) \u2013 LLRs or hard-decisions for every bit.\n  "
"Here is the detailed information of MaximumLikelihoodDetector:   \n  \n[sionna.ofdm.MaximumLikelihoodDetector(output, det_meth, resource_grid, stream_management, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, dtype=tf.complex64, **kwargs)](https://nvlabs.github.io/sionna/_modules/sionna/ofdm/detection.html#MaximumLikelihoodDetector)  \n\nThis class is a layer that wraps the MIMO maximum-likelihood (ML) detector for use in the OFDM waveform. It is assumed that the received signal is the output of the :class:`~sionna.channel.OFDMChannel` channel model. The detection algorithm and constellation and bit-wise mapper are configured through the OFDMResourceGrid, StreamManagement, Constellation, and Mapper classes.\n\n**Parameters**\n\n- `output` (One of [\"bit\", \"symbol\"], str): Type of output, either LLRs for bits or logits for constellation symbols.\n- `det_method` (One of [\"lmmse\", \"kbest\"], str): Detection method to be used.\n- `resource_grid` (ResourceGrid): Instance of the ResourceGrid.\n- `stream_management` (StreamManagement): Instance of the StreamManagement.\n- `constellation_type` (One of [\"qam\", \"pam\", \"custom\"], str): For \"custom\", an instance of Constellation must be provided.\n- `num_bits_per_symbol` (int): Number of bits per constellation symbol, e.g., 4 for QAM16. Required only for constellation types [\"qam\", \"pam\"].\n- `constellation` (Constellation): Instance of Constellation or None. If None, both `constellation_type` and `num_bits_per_symbol` must be specified.\n- `hard_out` (bool): If true, computes hard-decided bit values or constellation point indices instead of soft-values. Defaults to False.\n- `dtype` (One of [tf.complex64, tf.complex128] tf.DType (dtype)): The dtype of x. Defaults to tf.complex64. The output dtype is the corresponding real dtype (tf.float32 or tf.float64).\n\n**Input**\n\n- `(y, h_hat, err_var, no)` \u2013 Tuple:\n  - `y` ([batch_size, num_rx, num_rx_ant, num_ofdm_symbols, fft_size], tf.complex): Received OFDM resource grid after cyclic prefix removal and FFT.\n  - `h_hat` ([batch_size, num_rx, num_rx_ant, num_tx, num_streams_per_tx, num_ofdm_symbols, num_effective_subcarriers], tf.complex): Channel estimates for all streams from all transmitters.\n  - `err_var` ([Broadcastable to shape of h_hat], tf.float): Variance of the channel estimation error.\n  - `no` ([batch_size, num_rx, num_rx_ant] or only the first n dimensions, tf.float): Variance of the AWGN.\n\n**Output**\n\n- Depending on the `output` parameter:\n  - If `output` equals \"bit\":\n    - [batch_size, num_tx, num_streams, num_data_symbols * num_bits_per_symbol], tf.float: LLRs or hard-decisions for every bit of every stream.\n  - If `output` equals \"symbol\":\n    - [batch_size, num_tx, num_streams, num_data_symbols, num_points], tf.float or [batch_size, num_tx, num_streams, num_data_symbols], tf.int: Logits or hard-decisions for constellation symbols for every stream. Hard-decisions correspond to the symbol indices."
"To initialize a `TurboEncoder` with custom generator polynomials, the polynomials should be in a binary format. The binary sequence \"10011\" represents the generator polynomial $1 + D^4 + D^5$. In this notation, the coefficients of the polynomial are represented as binary numbers. The terms of the polynomial are in descending order of power of the variable $D$.\n\nHere's the breakdown of \"10011\":\n\n- The leftmost digit \"1\" signifies that $D^0$ term is present in the polynomial.\n- The second digit from the left \"0\" signifies that there is no $D^1$ term in the polynomial.\n- The next digit \"0\" signifies that there is no $D^2$ term.\n- The next digit \"1\" signifies that there is a $D^3$ term.\n- The rightmost digit \"1\" signifies that there is a $D^4$ term in the polynomial.\n\nSo, the binary sequence \"10011\" completely defines the generator polynomial $g(D)=1 + D^4 + D^5$.\n\nWhen using this to initialize a `TurboEncoder`, the `gen_poly` parameter should be assigned this binary string. For example:\n\n```python\nencoder = TurboEncoder(gen_poly=\"10011\", rate=1/2)  # Use your custom binary string here\n```\n\nPlease ensure the string is correct for your application. The `gen_poly` string should match the specifications of Turbo codes and the constraints of the `TurboEncoder` component. Confirm the specific format from the documentation or source code of the `TurboEncoder` you are using. The above format is a common representation for the generator polynomial of a convolutional code in binary form, where each digit of the string represents a coefficient of the polynomial."
"To sample the first few items of the dataset using the generator, you can simply loop over the generator and break out of the loop once you have sampled the required number of items. \n\nHere's how you can do it, following the code from the context:\n\n```python\ni = 0\nfor (a,tau) in generator():\n    print(a.shape)\n    print(tau.shape)\n    i = i + 1\n    if i == 5:\n        break\n```\n\nIn this code, the generator is looped over. The `i` variable is used to count the number of iterations, which is used to sample the first 5 items. The `if` statement is used to break out of the loop after the 5th iteration. \n\nWhen using a generator, it's important to ensure that the generator is suitable for the specific task at hand. If the generator is not properly configured or if the iteration process is not correctly implemented, it could lead to an infinite loop or a loop that doesn't terminate within the expected number of iterations."
"To save the estimated matrices for future use, you can use the `np.save` function from the numpy library. In the provided context, the estimated frequency, time, and space covariance matrices (FREQ_COV_MAT, TIME_COV_MAT, and SPACE_COV_MAT) are saved as numpy arrays using this function. The first argument is the file name and the second argument is the array to be saved. \n\nHere is how it is done in the context:\n\n```python\nnp.save('freq_cov_mat', FREQ_COV_MAT.numpy())\nnp.save('time_cov_mat', TIME_COV_MAT.numpy())\nnp.save('space_cov_mat', SPACE_COV_MAT.numpy())\n```\n\nThese lines of code will save the frequency covariance matrix as an array in a file named 'freq_cov_mat.npy', the time covariance matrix as an array in a file named 'time_cov_mat.npy', and the space covariance matrix as an array in a file named 'space_cov_mat.npy'. \n\nTo load these matrices for future use, you can use the `np.load` function, which loads the saved matrices from a file into an array. Here is how you can do it:\n\n```python\nFREQ_COV_MAT = np.load('freq_cov_mat.npy')\nTIME_COV_MAT = np.load('time_cov_mat.npy')\nSPACE_COV_MAT = np.load('space_cov_mat.npy')\n``` \n\nThese lines of code will load the frequency covariance matrix from the file 'freq_cov_mat.npy' into an array named FREQ_COV_MAT, the time covariance matrix from the file 'time_cov_mat.npy' into an array named TIME_COV_MAT, and the space covariance matrix from the file 'space_cov_mat.npy' into an array named SPACE_COV_MAT. \n\nTo use these matrices in a simulation, you need to pass them as arguments to the `MIMOOFDMChannel` function. For example:\n\n```python\nchannel = MIMOOFDMChannel( # Load saved covariance matrices\n                         freq_cov_mat=FREQ_COV_MAT,\n                         time_cov_mat=TIME_COV_MAT,\n                         space_cov_mat=SPACE_COV_MAT,\n```"
"Concept Explanation:\n\nThe Iterative Detection and Decoding (IDD) is a method used in MIMO (Multiple Input, Multiple Output) simulations. It involves the process of data detection and decoding, which are essential parts of many communication systems. \n\nIn the context of IDD, the MIMO receiver iteratively exchanges soft-information between the data detector and the channel decoder. This process is based on the mathematical model that represents the flow of information in a noisy channel. The model includes the transmitted information (input), the received noisy signal, and the estimated output (transmitter or decoder output). \n\nThe *a priori* information (represented by log-likelihood ratios, LLRs) and the *a posteriori* information (L^D) are two types of soft-information that are exchanged iteratively. The *a posteriori* information is the updated version of the *a priori* information after taking into account the received signal. \n\nIn the original IDD, a resetting (Turbo) decoder was used. However, a non-resetting decoder has been found to be more effective in some scenarios. The decoder state, which represents the check node to variable node messages from each IDD iteration to the next, is forwarded in each iteration. \n\nThe concept of IDD is particularly relevant in simulations of wireless communication using multiple antennas, where the data detected and decoded is more complex compared to single-antenna systems. The IDD method has been found to improve the performance of MIMO systems, particularly in noisy environments."
"   taking the instruction as it is, doesn't provide any more details about how to use it. The context provide is mostly about the usage of Scrambler. \n\nAs for the details of using Descrambler, here is what you might want to know: \n`Descrambler` is a class in the `sionna.fec.scrambling` and it is used to descramble a sequence that was scrambled with `Scrambler`.\n\nYou should first create a Scrambler, and then you can create a Descrambler instance with the Scrambler instance.  \n  \nHere is an example:\n```python\nfrom sionna.fec.scrambling import Scrambler, Descrambler\nimport tensorflow as tf\n# Parameters\nbsc_p = 0.046 # bit flipping probability of the bit wise flipping i.e. the bsc\nseed = 1234 # internal seed, can be chosen randomly\n\n# Define and connect components\nscrambler = Scrambler(seed=1234,binary=True,channel_type=\"AWGN\")\ndescrambler = Descrambler(scrambler=scrambler)\n\n# For the list of inputs, bit_seed is in [x,seed] to realize random scrambler/\n# descrambler pairs\nbsc_p = tf.constant(0.046,tf.float32)\nc = tf.constant(0,tf.float32) # as we only do B2B simulation c is not required.\nx = tf.constant([[0,1,1,0],[1,0,1,0]],tf.float32)\n# seed_int can be set as in (xk,sk) or directly as an int.\ny = scrambler([x,bsc_p])\nx_rec = descrambler([y,bsc_p])```\n  "
"Here is the definition of KBestDetector: sionna.ofdm.KBestDetector(output, num_streams, k, resource_grid, stream_management, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, use_real_rep=False, list2llr=None, dtype=tf.complex64, **kwargs)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/ofdm/detection.html#KBestDetector)  \n\n```python\nclass KBestDetector(OFDMDetector):\n    # pylint: disable=line-too-long\n    r\"\"\"KBestDetector(output, num_streams, k, resource_grid, stream_management, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, use_real_rep=False, list2llr=None, dtype=tf.complex64, **kwargs)\n\n    This layer wraps the MIMO K-BestDetector for use with the OFDM waveform.\n\n    Both detection of symbols or bits with either\n    soft- or hard-decisions are supported. The OFDM and stream configuration are provided\n    by a :class:`~sionna.ofdm.ResourceGrid` and :class:`~sionna.mimo.StreamManagement`\n    instance, respectively. The actual detector is an instance of\n    :class:`~sionna.mimo.KBestDetector`. The default\n    value for ``constellation_type`` is \"qam\" and ``num_bits_per_symbol``\n    is 2, which corresponds to :class:`~sionna.mapping.Constellation`\n    : First, ``constellation_type``\n    is used to create a :class:`~sionna.mapping.Constellation`,\n    or ``constellation`` is directly provided. Then,\n    :class:`~sionna.mimo.KBestDetector` is instantiated with\n    the given ``output`` and the constellation.\n\n    Parameters\n    ----------\n    output : One of [\"bit\", \"symbol\"], str\n        Type of output, either bits or symbols. Whether soft- or\n        hard-decisions are returned can be configured with the\n        ``hard_out`` flag.\n\n    num_streams : tf.int\n        Number of transmitted streams\n\n    k : tf.int\n        Number of paths to keep\n\n    resource_grid : ResourceGrid\n        Instance of :class:`~sionna.ofdm.ResourceGrid`\n\n    stream_management : StreamManagement\n        Instance of :class:`~sionna.mimo.StreamManagement`\n\n    constellation_type : One of [\"qam\", \"pam\", \"custom\"], str\n        For custom, an instance of :class:`~sionna.mapping.Constellation`\n        must be provided.\n\n    num_bits_per_symbol : int\n        Number of bits per constellation symbol, e.g., 4 for QAM16.\n        Only required for ``constellation_type`` in [\"qam\", \"pam\"].\n        Defaults to 4.\n\n    constellation : Constellation\n        Instance of :class:`~sionna.mapping.Constellation` or `None`.\n        In the latter case, ``constellation_type``\n        and ``num_bits_per_symbol`` must be provided.\n\n    hard_out : bool\n        If `True`, the layer provides hard-decided bit values or\n        constellation point indices instead of soft-values.\n        Defaults to `False`.\n\n    use_real_rep : bool\n        If `True`, the detector uses the real-valued equivalent\n        representation of the channel. Note that this only works with\n        a QAM constellation. Defaults to `False`.\n\n    list2llr: One of [None, \"neurallattice\"], str\n        The method used to compute LLRs from a list of candidates.\n        If `None`, the (non differentiable) KBest algorithm is used.\n        Currently, only \"neurallattice\" is implemented. Defaults to `None`.\n\n    dtype : One of [tf.complex64, tf.complex128] tf.DType (dtype)\n        The dtype of ``y``. Defaults to tf.complex64.\n        The output dtype is the corresponding real dtype (tf.float32 or tf.float64).\n\n    Input\n    ------\n    (y, h_hat, err_var, no) :\n        Tuple:\n\n    y : [batch_size, num_rx, num_rx_ant, num_ofdm_symbols, fft_size], tf.complex\n        Received OFDM resource grid after cyclic prefix removal and FFT\n\n    h_hat : [batch_size, num_rx, num_rx_ant, num_tx, num_streams_per_tx, num_ofdm_symbols, num_effective_subcarriers], tf.complex\n        Channel estimates for all streams from all transmitters\n\n    err_var : [Broadcastable to shape of ``h_hat``], tf.float\n        Variance of the channel estimation error\n\n    no : [batch_size, num_rx, num_rx_ant] (or only the first n dims), tf.float\n        Variance of the AWGN\n\n    Output\n    ------\n    One of:\n\n    : [batch_size, num_tx, num_streams, num_data_symbols*num_bits_per_symbol], tf.float\n        LLRs or hard-decisions for every bit of every stream, if ``output`` equals `\"bit\"`.\n\n    : [batch_size, num_tx, num_streams, num_data_symbols, num_points], tf.float or [batch_size, num_tx, num_streams, num_data_symbols], tf.int\n        Logits or hard-decisions for constellation symbols for every stream, if ``output`` equals `\"symbol\"`.\n        Hard-decisions correspond to the symbol indices.\n    \"\"\"\n    def __init__(self,\n                 output,\n                 num_streams,\n                 k,\n                 resource_grid,\n                 stream_management,\n                 constellation_type=None,\n                 num_bits_per_symbol=None,\n                 constellation=None,\n                 hard_out=False,\n                 use_real_rep=False,\n                 list2llr=None,\n                 dtype=tf.complex64, **kwargs):\n        super().__init__(output=output,\n                         num_streams=num_streams,\n                         resource_grid=resource_grid,\n                         stream_management=stream_management,\n                         detection_algorithm=\"kbest\",\n                         constellation_type=constellation_type,\n                         num_bits_per_symbol=num_bits_per_symbol,\n                         constellation=constellation,\n                         hard_out=hard_out,\n                         use_real_rep=use_real_rep,\n                         list2llr=list2llr,\n                         k=k,\n                         dtype=dtype, **kwargs)\n```"
"Answer:\nThe `position` and `orientation` properties are used in Sionna to move scene objects. These properties are part of every scene object in Sionna's ray tracing module, and they are used to define the position and orientation of the object in the scene.\n\nThe `position` property is used to define the position of the object in the scene. It is a three-dimensional vector that points to the position of the object.\n\nThe `orientation` property is used to define the orientation of the object. It is expressed as a quaternion (a four-dimensional vector). The quaternion represents a rotation, and can be used to rotate the object as needed.\n\nBy manipulating the `position` and `orientation` properties of an object, you can move and rotate the object in the scene as needed. This can be useful for creating dynamic scenes or for simulating movement of objects. Note that the `position` and `orientation` properties are not the only way to move and rotate objects in the scene. You can also move and rotate objects by manipulating their velocity or by applying forces to them.\n\nIt's important to note that while you can use the `position` and `orientation` properties to move and rotate objects, you need to be careful that the objects are not moved into invalid positions (for example, inside a building or underground). Also, be aware that rotating an object can change the direction of its antenna if it has an antenna attached to it."
"To run the final evaluation for different system configurations in Sionna RT, you can follow these steps:\n\n1. Define the system configurations you want to evaluate. This could include different transmitters, receivers, antenna arrays, different numbers of antennas, different modulation schemes, different channel models, etc.\n\n2. Create a batch of system configurations and their respective channel impulse responses (CIRs). You can do this by using a for loop to iterate over the different system configurations. For each configuration, set up the transmitter and receiver, and then compute the CIR using the `compute_cir` function from the Sionna RT module.\n\n3. After computing the CIRs, you can evaluate the mean square error (MSE) for each of the system configurations. The MSE can be used as a metric to compare the performance of the different system configurations.\n\n4. Finally, you can run a deep learning model (or any other kind of model) for the different system configurations. The input to the model could be the CIRs and the output could be the equalizer weights or the demodulated bits. The model should be trained using the MSE as the loss function and the weights could be initialized using a Keras layer or provided externally.\n\n5. After training the model, you can compute the bit error rate (BER) for each of the system configurations. The BER can be used as a metric to evaluate the quality of the transmission over the given channel model.\n\nPlease note that the actual code to implement these steps will depend on the specific requirements of your evaluation and the exact Sionna RT APIs that you are using. The steps outlined above are general and may need to be adjusted or extended to fit your needs."
"To compute the exact Doppler shifts for reflected and diffracted paths in a multipath channel, we can use the equations provided in the Background Information:\n\n\\[\n\\Delta f = \\frac{\\mathbf{v} \\cdot \\hat{\\mathbf{k}}}{\\lVert \\hat{\\mathbf{k}} \\rVert}\n\\]\n\nwhere \\(\\Delta f\\) is the Doppler shift we want to find, \\(\\mathbf{v} = [v_x, v_y, v_z]\\) is the movement vector of the transmitter, and \\(\\hat{\\mathbf{k}} = \\frac{\\mathbf{k}}{||\\mathbf{k}||}\\) is the unit norm of the direction of the path.\n\n\\[\n\\lVert \\hat{\\mathbf{k}} \\rVert = \\sqrt{1 + \\left( \\frac{\\partial \\phi}{\\partial x} \\right)^2 + \\left( \\frac{\\partial \\phi}{\\partial y} \\right)^2}\n\\]\n\nThe above equation can be understood through the following code snippet. The function `r_hat` computes the unit norm of the direction of the path and the second line calculates the magnitude of the velocity vector of the transmitter. In the third line, the dot product is computed and divided by the magnitude of the direction to get the Doppler shift:\n\n```python\ndef r_hat(theta, phi):\n    return sn.scene.rt.raytracing.Ray().r_hat(theta, phi)\n\ntheta, phi = ..., ...\nk = sn.scene.rt.raytracing.Ray().k(theta, phi)\nx, y = ...\nk = sn.scene.rt.raytracing.Ray().x_deriv(k, x, y)\nl = sn.scene.rt.raytracing.Ray().y_deriv(k, x, y)\nmagnitude = np.sqrt(1 + np.sum(k**2))\ndoppler = np.dot(velocity, k) / magnitude\n```\n\nNext, we need to take into account the rotation of the scene. The movement of all scene objects is computed relative to a common rotation center. The orientation of an object is described by the Euler angles \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\). The rotation matrix \\(\\mathbf{R}\\) and its time derivative \\(\\dot{\\mathbf{R}}\\) are used to rotate the scene.\n\nThe movement vector \\(\\mathbf{v}'\\) of an object with a relative velocity vector \\(\\mathbf{v}\\) is then given as:\n\n\\[\n\\mathbf{v}' = \\dot{\\mathbf{R}} \\mathbf{r}(\\alpha, \\beta, \\gamma) \\mathbf{v}\n\\]\n\nwhere \\(\\mathbf{r}\\) is the vector from the rotation center to the object. The Doppler shift experienced by a path can be obtained using the above equation. The function of the above computations is provided below:\n\n```python\ndef compute_rot Doppler_shift( position, rotation, velocity, time_derivative_of_rotation, direction_of_path ):\n    # Transposition for correct mat mul\n    position = tf.expand_dims(position, axis=-1)\n\n    # Rotation matrix\n    r = sn.scene.rt.raytracing.rotation_matrix(position, rotation)\n    r_inv = sn.scene.rt.raytracing.inverse_rotation_matrix(position, rotation)\n    r_dot = time_derivative_of_rotation\n    # Time derivative of the rotation matrix\n    drdt = sn.utils.tensor_matmul(r, r_dot)\n\n    # Velocity vector of the scene object\n    v = velocity\n    v = tf.expand_dims(v, axis=-1)\n\n    # Compute (v,dr/dt)\n    # (v,dr/dt) = (v,r_dot) + (dv/dt,r)\n    dot_prod = tf.squeeze(tf.reduce_sum(v*drdt, axis=-1))\n    doppler_shift = dot_prod/tf.norm(direction_of_path)\n\n    # r_inv\n    # is used to rotate the Poynting vector that defines the direction of the path into\n    # the coordinate system of the scene\n    return doppler_shift\n```\n\nPlease note that the exact code implementation may vary based on the specific requirements of the application. The above code is provided as a general guide."
"Here is the definition of cir_to_time_channel: sionna.channel.cir_to_time_channel(bandwidth, a, tau, l_min, l_max, normalize=False)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/channel/utils.html#cir_to_time_channel)  \n\nsource code:\n```python\ndef cir_to_time_channel(bandwidth, a, tau, l_min, l_max,\n                        normalize=False):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    Compute the channel taps forming the discrete complex-baseband\n    representation of the channel from the channel impulse response\n    (a, tau).\n\n    This function assumes that a sinc filter is used for pulse shaping and receive\n    filtering. Therefore, given a channel impulse response\n    :math:`(a_{m}(t), \\tau_{m}), 0 \\leq m \\leq M-1`, the channel taps\n    are computed as follows:\n\n    .. math::\n        \\bar{h}_{b, \\ell}\n        = \\sum_{m=0}^{M-1} a_{m}\\left(\\frac{b}{W}\\right)\n            \\text{sinc}\\left( \\ell - W\\tau_{m} \\right)\n\n    for :math:`\\ell` ranging from `l_min` to `l_max`, and where :math:`W`\n    is the `bandwidth`.\n\n    Input\n    ------\n    bandwidth : float\n        Bandwidth [Hz]\n\n    a : `[batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps]`, `tf.complex`\n        Path coefficients\n\n    tau : `[batch size, num_rx, num_tx, num_paths]` or\n           `[batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths]`, `tf.float`\n        Path delays [s]\n\n    l_min : int\n        Smallest time-lag for the discrete complex baseband channel (:math:`L_{\\text{min}}`)\n\n    l_max : int\n        Largest time-lag for the discrete complex baseband channel (:math:`L_{\\text{max}}`)\n\n    normalize : bool\n        If set to `True`, the channel is normalized over the block size\n        to ensure unit average energy per time step.\n        Defaults to `False`.\n\n    Output\n    ------\n    h : `[batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_time_steps, l_max - l_min + 1]`, `tf.complex`\n        Channel taps coefficients\n    \"\"\"\n\n    # Remove dimensions with size 1. This is done to support broadcasting, be we\n    # need to be careful to not remove the broadcast dimension, but only the\n    # singletons.\n    tau = tf.squeeze(tau, axis=-1)\n\n    # Scale the delays with the bandwidth\n    h = tau * bandwidth\n\n    # Generate time-lags for the complex baseband channel\n    # h is the set of time-lag for the first and the last tap of a sinc filter\n    # of length  N (N = 32). For example, if N=32, h = [0,32] in the complex\n    # baseband. Then, the discrete complex baseband channel becomes\n    # (with L ranging from l_min to l_max as initialization):\n    # y_b = \\sum_{m=0}^{M-1} a_{m}\\left(\\frac{b}{W}\\right) x_n, 32) + w_b)\n    # with\n    # x_n the transmitted symbols,\n    # w_b the discrete noise\n    # and with the filter tap  position h (time step) ranging from\n    # L-l_max (0) to L-l_min (N=32) . For the first tap, this corresponds\n    # to the the first N samples of the the received signal, and to the\n    # filter position h. For the last tap, this corresponds to the last\n    # N samples of the the received signal.\n    h = tf.cast(h, dtype=tf.int32)\n    first_taps = h + l_min\n    last_taps = h + l_max\n    num_time_steps = last_taps - first_taps + 1\n    in_baseband = 1/bandwidth * tf.range(num_time_steps)\n    h = tf.stack([in_baseband, tf.fill([num_time_steps], 0.)], -1)\n    h = tf.cast(h, dtype=tau.dtype)\n\n    a = tf.complex(0.,0.) + tf.squeeze(a, axis=-2)\n    a = tf.pad(a, [[0,0], [0,0], [0,0], [0,0], [0],\n                  [0, l_max - l_min + 1]])\n    a = tf.signal.sinc_filter(a, h, 1.)  # sinc_filter is from the receive filter\n                                       # library\n    a = a[..., first_taps:first_taps+num_time_steps]\n\n    h = tf.reduce_sum(a, axis=-2)\n    h = tf.squeeze(h, axis=-1)\n\n    if normalize:\n        # We do not normalize the first and the last tap to ensure that the\n        #  total energy is not scaled, as it is done in the filter.\n        n = tf.reduce_sum(tf.pow(tf.abs(h), 2), axis=[5,6], keepdims=True)\n        n = tf.math.reciprocal(tf.maximum(n, 1e-12))\n        h = tf.complex(tf.math.sqrt(n), tf.constant(0., h.dtype)) * h\n\n    return h\n```"
"To generate the Channel Frequency Response, you need to create a channel impulse response first. This is done by using a channel model function from the Sionna package. In the provided context, a `cdl` (Clustered Delay Line) model is used. This model simulates a MIMO (Multiple-Input Multiple-Output) channel based on the CDL (Call, Doppler, and Delay Spread) wireless channel model from 3rd Generation Partnership Project (3GPP) specifications.\n\nHere is the code snippet from the context that creates the impulse response:\n\n```python\na, tau = cdl(batch_size=2, num_time_steps=rg.num_time_samples+l_tot-1, sampling_frequency=rg.bandwidth)\n```\n\nIn this snippet, `cdl` is the channel model function. The parameters `batch_size`, `num_time_steps`, and `sampling_frequency` are specific values needed by the chosen model to generate the impulse response. `batch_size` is often the number of examples processed by Sionna in parallel. `num_time_steps` is the number of time steps for the simulation, which can be the total number of time samples for the channel impulse response (`rg.num_time_samples`) plus the length of the impulse response (`l_tot`). `sampling_frequency` is the rate at which the channel is sampled in Hz.\n\nOnce you have the impulse response, you can convert it to the frequency domain to get the Channel Frequency Response. This is done using the `cir_to_frequency_channel` function which takes the impulse response as input. The Channel Frequency Response is necessary for channel equalization or channel demapping in a communication system.\n\nTo generate the Discrete-Time Channel Impulse Response, you can use the `a` and `tau` variables that were generated when creating the channel impulse response. These variables represent the complex amplitudes and time delays of the paths in the channel. You can use them directly in the z-domain (the domain for Discrete-Time Signals) to represent the channel in the equation:\n\n![h](https://latex.codecogs.com/svg.latex?h_b%28n%29%20=%20%5Csum_%7Bm%3D1%7D%5E-M%7D%20a_m%28b%29%20e%5E%7D%7B%20%2%5Ctau_m%28b%29%2C%20%2A%20%5Ctau_m%28b%29%7D%20%5Cdelta%28n%29%20%5C%5C%20%28b,%20n%20=%201,%20...,%20N_B%20-%201%29)\n\nWhere ![h_b](https://latex.codecogs.com/svg.latex?h_b) represents the complex amplitude of the bth path and ![2tau_b](https://latex.codecogs.com/svg.latex?2%5Ctau_b) the bth path delay. You would generate this for each example in your batch, if you are doing batch processing.\n\nPlease note that the exact functions and methods to be used may vary depending on the specific package or library you are using for your channel model. The instructions provided here are specifically for the Sionna package, as per the context provided."
"import matplotlib.pyplot as plt\nimport numpy as np\nimport sionna\nfrom sionna.mapping import Constellation, Mapper, Demapper\nfrom sionna.fec.ldpc import LDPC5GEncoder, LDPC5GDecoder\nfrom sionna.fec.interleaving import RandomInterleaver, Deinterleaver\nfrom sionna.fec.scrambling import Scrambler, Descrambler\nfrom sionna.utils import BinarySource, ebnodb2no\nimport os\n# Import QAM source\nfrom sionna.utils import QamSource\n\n# Setup\npath = os.path.expanduser(\"~/nvidia-xavier1-rs93dr/models/fec/constellation\");\nsymbol_os = 2;\nnum_bits_per_symbol = 4;\n\n# Create and load constellation object\nconstellation = Constellation(\"qam\", int(num_bits_per_symbol/2), oversampling=symbol_os)\nconstellation.load(path, num_bits_per_symbol)\nconstellation._show_weights = False\nconstellation.show();\n\n# Generate QAM with weights\nsource = QamSource(int(num_bits_per_symbol/2), num_bits_per_symbol//symbol_os, normalize=False, dtype=tf.complex64)\nbits = source([100, int(k/2**symbol_os)], snr_db=10.0)\nx = mapper([bits_q.reshape([-1, n])])\nprint(\"x.shape:\", x.shape)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# QAM\nnum_bits_per_symbol = 4\noversampling = 2\nsymbol_os = oversampling\n# Create and load constellation object\nconstellation = sn.mapping.Constellation(\"qam\", int(num_bits_per_symbol/2), oversampling=symbol_os)\nconstellation.qam_source(int(num_bits_per_symbol/2), num_bits_per_symbol//symbol_os, snr_db=8.0)\nbits = constellation.qam_source([128, num_bits_per_symbol//symbol_os])\nPoints = constellation.points\nplt.figure()\nplt.scatter(Points.real, Points.imag)\nplt.show()\nbits_q = sn.utils.BinarySource(num_samples=[128, num_bits_per_symbol], dtype=tf.complex64)\nplt bits_q\nsource = sn.utils.QamSource(num_bits_per_symbol, num_bits_per_symbol//oversampling, num_samples=128)\nx = source([bits_q])\nplt.figure()\nplt.scatter(Points.real, Points.imag)\nplt.scatter(x[:,0], x[:,1], c=\"r\")\nplt.show()\nx.shape"
"To demonstrate how to handle a stateful situation with the LDPCBPDecoder class from sionna's LDPC module, we need to understand the role of the `msg_vn` parameter. This parameter is expected to contain the VN messages of the LDPC BP Decoder.\n\nIn a stateful LDPC decoder scenario, the decoding process is maintained across multiple calls, which can be useful in iterative decoding processes where the intermediate information from previous iterations is needed. \n\nHere's how to handle the situation:\n\n1. Import the necessary classes from sionna's LDPC module.\n\n```python\nfrom sionna.fec.ldpc.decoding import LDPCBPDecoder, load_parity_check_matrix, PCMDecoder\n```\n\n2. Load the parity-check matrix. In this example, a pre-built binary parity-check matrix is loaded from a file.\n\n```python\npcm, k, n , coderate = load_parity_check_matrix(pcm_file) \n```\n\n3. Initialize the LDPC Decoder, specifying that it should track EXIT information during decoding. This is a requirement whenever the decoder is used to estimate the channel EXIT (or bit) EXIT characteristics.\n\n```python\ndecoder=LDPCBPDecoder(pcm,track_exit=True) \n```\n\n4. For the first decoding iteration, `msg_vn=None` as it will be used as the initialization. \n\n5. After each decoding iteration, the `msg_vn` parameter contains the VN messages of the decoder. One must not overwrite this parameter with None.\n\n6. The final `hard_out` indicates whether the final output is hard decisions (True) or soft-decisions (False). \n\nIn the case of a multi-iterative decoder, you would typically call this LDPC BP Decoder in a loop, with `msg_vn` from the previous iteration used as an input for the next iteration. \n\nRemember, for each call of the decoder, you need to pass your messages and llr's in a none (Tensor) to Tensor flow, and the datatype of the variables you pass as arguments should be tf.float32 (if there's a different required datatype, as in the example). \n\nPlease note that the detailed error handling and iteration stopping criteria have been omitted in this explanation for simplicity. In a real-world application, you would need to implement those as well. \n\nThe use of `msg_vn` is critical in maintaining the statefulness of the decoder. If you were to pass `msg_vn` as None in each iteration, the decoder would not maintain the iterative process' state, and the final output would not be meaningful in the context of iterative belief propagation decoding. \n\nBe careful to reassign `msg_vn` in each iteration, making sure not to nullify the state that it carries from the previous iterations. Again, note that you should handle potential errors and decoding stop criteria in a practical use case, which are not detailed here to address the stateful handling. \n\nThis method is based on the provided LDPCBPDecoder class documentation and is intended for use in such stateful processing scenarios. It applies to any LDPC decoder that requires the preservation of decoding state across multiple calls, which is facilitated through the `msg_vn` parameter."
"Here is the detailed information of subcarrier_frequencies:   \n\nLink of the source code: https://nvlabs.github.io/sionna/_modules/sionna/channel/wireless/ofdm.html#subcarrier_frequencies\n\nsource code:\n```python\ndef subcarrier_frequencies(self, normalized=False):\n        r\"\"\"\n        Returns the subcarrier frequencies.\n\n        For a given number of FFT points :math:`N`, the subcarrier\n        frequencies are computed through the FFT normalization, as:\n        :math:`\\frac{1}{\\sqrt{N}}\\mathtt{F}_N\\lbrack{n}\\rbrack,\n        where :math:`0\\leq n<N`.\n\n        The subcarrier spacing is defind as\n\n        .. math::\n            \\Delta_f = \\frac{1}{N}\\text{.}\n\n        Parameters\n        ----------\n        fft_size : int\n            The FFT size :math:`N`.\n\n        normalized : bool\n            If `True`, the subcarrier frequencies are normalized\n            to the interval :math:`[0, 1]`. Defaults to `False`.\n\n        Output\n        ------\n        : [...,num_tx_ant,:num_streams_per_tx,:fft_size], tf.complex\n            The `fft_size` normalized subcarrier frequencies.\n        \"\"\"\n        num = self.fft_size\n        n = tf.reshape(tf.range(num, dtype=tf.int32), 1)\n        n = tf.cast(n, self._dtype)\n        exp = tf.cast(2, self._dtype) * tf.cast(np.pi, self._dtype) \\\n            * tf.cast(1/num, self._dtype) \\\n            * tf.cast(n, self._dtype)\n        if normalized:\n            exp *= tf.cast(1/np.sqrt(2), self._dtype)\n        return tf.cast(1/tf.sqrt(tf.cast(num, self._dtype)), self._dtype) \\\n            * tf.exp(tf.cast(0, self._dtype) + 1j*exp)\n```"
"SIMULATION OF GAUSSIAN IMPULSE OVER OPTICAL CHANNEL WITH LUMPED AMPLIFICATION:\n\n1. **Set Simulation Parameters:**\n   Define simulation parameters including the number of symbol to be simulated, symbol duration, impulse generation, simulation accuracy (i.e., the number of samples per symbol), carrier frequency, bit rate, fiber length, etc.\n\n2. **Create an Instance of the Optical Lumped Amplification Channel:**\n   Initialize the EDFA (Erbium Doped Fiber Amplifier) with the noise figure (NF) and gain (G). The channel model consists of multiple EDFAs, each with its own gain and noise figure.\n\n3. **Prepare Gaussian Impulse:**\n   The transmitted impulse is prepared as a Gaussian pulse with a peak power $P_0$ which is equivalent to sending a Gaussian minimum shift keying (MSK) signal at bit rate $R$ over an (ideal) non-dispersive channel. The impulse is generated over a time span of one symbol duration and with a specified sample spacing.\n\n4. **Simulate Transmission:**\n   The Gaussian impulse is then transmitted over the optical channel. The simulation will compute the signal at the output of the channel after transmission.\n\n5. **Visualize Results:**\n   The simulation results can be visualized by plotting the transmitted and received signals in the time and frequency domains.\n\n6. **Add More Complex Impairments:**\n   You can further modify the simulation to model more complex impairments such as attenuation, noise, dispersion, and non-linearity. Each of these impairments can be individually switched on or off. For example, you can enable attenuation by setting a non-zero value for `alpha`, or enable non-linearity by choosing a nonlinearity coefficient higher than zero.\n\nDo note that the exact implementation details would vary based on the specific tools and software you are using for simulation. The above steps are based on the provided Python context which focuses on using the Sionna package for simulating optical communications. The specific API calls and methods mentioned are from Sionna and may not be applicable in a general context."
"PARAMETERS:\n\n- **crc_encoder**: An instance of `CRCEncoder` to which the CRCDecoder is associated.\n\n- **dtype**: (Optional) tf.DType. It's used to define the datatype for internal calculations and the output datatype. If no explicit dtype is provided, the dtype from the associated interleaver is used. \n\n- ***kwargs**: allows any additional keyword arguments to be passed, but would be ignored in this context. \n      \nInput: \n-  **x** : Inputs to the CRC Decoder. Must have at least rank two. Shape is `[...,k]`, where `k` is the information bits length.\n  \nOutput: \n-  **(x, crc_valid)** : Tuple:\n   -  **x** : Contains the information bit sequence without CRC parity bits. It is a 2+D tensor of `tf.float32` with shape `[,k]`.\n   -  **crc_valid** : Contains the result of the CRC per codeword. It is a 2+D tensor of `tf.bool` with shape `[,1]`.\n\nRaises:\n\n- **AssertionError** : If `crc_encoder` is not an instance of `CRCEncoder`.\n- **InvalidArgumentError** : When Rank(`x`)<2. It's thrown by TensorFlow, for arguments that are not from expected type or shape. \n\nCOMMON ERRORS:\n- You may encounter an `InvalidArgumentError` if the rank of your input `x` is less than 2. \n- An `AssertionError` would be thrown if `crc_encoder` is not a `CRCEncoder` instance. \n\nA CRC (Cyclic Redundancy Check) is a type of hash function that detects errors in transmission. The CRCDecoder class works in association with the CRCEncoder for a specified CRC polynomial to verify and remove parity bits from the input sequence. This operation is particularly used in the context of Bit- or Burst-Errors in data transmission processes.\n\nBy the way, the use of `interleaver` in this context typically means a component used in a channel coding scheme that changes the order of bits, which is the inverse operation of a deinterleaver. This can be used to minimize the impact of bit errors during transmission."
"The output of the `Constellation` class is a handle to a matplotlib figure object. This constellation object can be used to visualize the constellation, which is a pattern of symbols in a digital signal. This can be helpful in understanding and analyzing the characteristics of the signal. Symbols in the constellation represent bits in the signal or, in the case of analog signals, different levels or values the signal can take on. The constellation is a key part of digital modulation schemes, where bits are mapped to constellation points to be transmitted over a communication channel. The `Constellation` class provides a way to create, manipulate, and visualize the constellation used in the signal mapping and demapping processes."
"The `LinearEncoder` class takes a `dtype` parameter, which is a `tf.DType` that specifies the datatype for the LinearEncoder output. The provided context shows the parameter definition in the `def __init__(...)` section:\n\n```python\ndef __init__(self, enc_mat, is_pcm=False, dtype=tf.float32, **kwargs):\n```\n\nHere, the `dtype` parameter is defaulted to `tf.float32`, which is a 32-bit floating point `tf.DType`. The explicit instruction is to use this datatype for the output of the `LinearEncoder`. Therefore, the datatype for the `LinearEncoder` output is not to be defined by the user and should be understood as `tf.float32`."
"Here is the detailed information of sionna.utils.matrix_pinv:  \n\n[sionna.utils.matrix_pinv(tensor)](https://nvlabs.github.io/sionna/_modules/sionna/utils/tensors.html#matrix_pinv)\n\nComputes pseudo-inverse of a matrix.\n\nGiven a matrix $\\mathbf{A}\\in\\mathbb{C}^{M\\times N}$, the pseudo-inverse $\\mathbf{A}^{\\dagger}\\in\\mathbb{C}^{N\\times M}$ is $\\text{Matrix Pinv}$ such that\n$\\mathbf{A}\\mathbf{A}^{\\dagger}=\\mathbf{A}^{\\dagger}\\mathbf{A} =: \\mathbf{I}_{\\min(N,M)}$. The function returns $\\mathbf{A}^{\\dagger}$ for general $\\mathbf{A}$, and the null-space indicating dimensions if $\\mathbf{A}$ is rank deficient.\n\n**Note:** \nResult is not unique if $\\mathbf{A}$ is rank deficient, but any solution must nullify the columns corresponding to vanishing singular values.\n\n### Parameters\n\n- **tensor** ([..., M, K], tf.complex)): Tensor of rank greater than or equal to two. The inner-most dimensions must be MxN matrices.\n\n### Returns\n\n- **[..., M, K]**): Pseudo-inverse of $\\mathbf{A}$.\n- **[2, \u2026, M]**, tf.int64): Tuple of integers describing the nullspace dimensions. These are only returned if $\\text{rank}(\\mathbf{A})<\\min(N,M)$.\n\n### Note\n- If you want to use this function in Graph mode with XLA, i.e., within a function that is decorated with @tf.function(jit_compile=True), you must set sionna.Config.xla_compat=true. See xla_compat.\n\nsource code:\n```python\ndef matrix_pinv(tensor):\n    r\"\"\"Computes pseudo-inverse of a matrix.\n\n    Given a matrix :math:`\\mathbf{A}\\in\\mathbb{C}^{M\\times N}`, the\n    pseudo-inverse :math:`\\mathbf{A}^{\\dagger}\\in\\mathbb{C}^{N\\times M}`\n    is :math:`\\text{Matrix Pinv}` such that\n\n    .. math::\n\n        \\mathbf{A}\\mathbf{A}^{\\dagger}=\\mathbf{A}^{\\dagger}\\mathbf{A} =: \\mathbf{I}_{\\min(N,M)}`.\n\n    The function returns :math:`\\mathbf{A}^{\\dagger}` for general\n    :math:`\\mathbf{A}`, and the null-space indicating dimensions if\n    :math:`\\mathbf{A}` is rank deficient.\n\n    Note: Result is not unique if :math:`\\mathbf{A}` is rank deficient, but any\n    solution must nullify the columns corresponding to vanishing singular\n    values.\n\n    Parameters\n    ----------\n    tensor : [..., M, K], tf.complex))\n        Tensor of rank greater than or equal to two. The inner-most\n        dimensions must be MxN matrices.\n\n    Returns\n    -------\n    [..., M, K]**): Pseudo-inverse of :math:`\\mathbf{A}`.\n    [2, \u2026, M], tf.int64): Tuple of integers describing the nullspace\n        dimensions. These are only returned if :math:`\\text{rank}(\\mathbf{A})<\\min(N,M)`.\n\n    Note\n    ----\n    If you want to use this function in Graph mode with XLA, i.e., within\n    a function that is decorated with ``@tf.function(jit_compile=True)``,\n    you must set ``sionna.Config.xla_compat=true``.\n    See :py:attr:`~sionna.Config.xla_compat`.\n    \"\"\"\n\n    if Config.xla_compat and not t.is_compat_vector():\n        # The pseudo-inverse of A is also the hermitian-conjugate of A.\n        # We compute the hermitian-conjugate with the last two axes\n        # being M and N, and then we multiply the last tensor with\n        # the unitary filter with the correct ordering of dimensions.\n        # This is more performant that concatenating the tensor with\n        # unitary filters and summing over all filters, as only one\n        # filter is applied at a time.\n\n        # Compute the hermitian-conjugate\n        AHC = hermitian_conjugate(tensor)\n\n        # Bring tensor in matrix-factorized form\n        tf.debugging.assert_equal(\n            tf.shape(AHC)[-2],\n            tf.shape(tensor)[-2],\n            \"The inner-most two dimensions of the input tensor must be \" \\\n            \"a matrix of full rank.\"\n        )\n        s, u, _ = matrix_svd(tensor)\n\n        # Compute the pseudoinv\n        c = tf.math.divide_no_nan(1, s)\n        pseudoinv = tf.einsum(\"...ki, ...k, ...ij->...kj\", c, u, u)\n\n        return pseudoinv\n\n    else:\n        s, u, _ = tensor_svd(tensor)\n\n        # Compute the pseudo-inverse\n        c = tf.math.divide_no_nan(1, s)\n        pseudoinv = tf.einsum(\"...ki, ...k, ...ij->...kj\", c, u, u)\n\n        return pseudoinv, tf.shape(pseudoinv)[:-2]\n```"
"Here is the detailed information of [sionna.mapping.Demapper(demapping_method, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, with_prior=False, dtype=tf.complex64, **kwargs)](https://nvlabs.github.io/sionna/_modules/sionna/mapping.html#Demapper):  \n  \nComputes log-likelihood ratios (LLRs) or hard-decisions on bits for a tensor of received symbols.\nIf the flag with_prior is set, prior knowledge on the bits is assumed to be available.\n\nThis class defines a layer implementing different demapping functions. All demapping functions are fully differentiable when soft-decisions are computed.\n\n### Parameters\n\n- **demapping_method** (str): One of [\"app\", \"maxlog\"], where \"app\" stands for the demapper based on the [A Posteriori Probability (APP)](https://en.wikipedia.org/wiki/A_posteriori_probability) and \"maxlog\" for the [Maximum Likelihood (MLP)](https://en.wikipedia.org/wiki/Maximum_likelihood) demapper.\n\n- **constellation_type** (str): One of [\"qam\", \"pam\", \"custom\"], ignored if constellation is not None. For \"custom\", an instance of Constellation must be provided.\n\n- **num_bits_per_symbol** (int): The number of bits per constellation symbol, e.g., 4 for QAM16. Only required for constellation_type in [\"qam\", \"pam\"].\n\n- **constellation** (Constellation): An instance of Constellation or None. In the latter case, both constellation_type and num_bits_per_symbol must be provided.\n\n- **hard_out** (bool): If true, the demapper provides hard-decided bits instead of soft-values. Defaults to False.\n\n- **with_prior** (bool): If true, it is assumed that prior knowledge on the bits is available. This prior information is given as LLRs as an additional input to the layer. Defaults to False.\n\n- **dtype** (One of [tf.complex64, tf.complex128] tf.DType): The dtype of `y`. Defaults to tf.complex64. The output dtype is the corresponding real dtype (tf.float32 or tf.float64).\n\n### Input\n\n- **y** ([..., n], tf.complex): The received symbols.\n\n- **prior** (None or [\u2026, n, num_bits_per_symbol], tf.float): Prior for every bit as LLRs. It can be provided either as a tensor of shape `[num_bits_per_symbol]` for the entire input batch, or as a tensor that is broadcastable to `[..., n, num_bits_per_symbol]`. Only required if the `with_prior` flag is set.\n\n- **no** (None or [\u2026, n], tf.float): The noise variance estimate. It can be provided either as a scalar for the entire input batch or as a tensor that is broadcastable to `y`.\n\n### Output\n\n- **[\u2026,n*num_bits_per_symbol]**, tf.float: LLRs or hard-decisions for every bit.\n  \n**Note: **With the app demapping method, the LLR for the $i\\text{th}$ bit is computed according to $LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=1\\lvert y,\\mathbf{p}\\right)}{\\Pr\\left(b_i=0\\lvert y,\\mathbf{p}\\right)}\\right) =\\ln\\left(\\frac{\n        \\sum_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n        \\exp\\left(-\\frac{1}{N_o}\\left|y-c\\right|^2\\right)\n        }{\n        \\sum_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n        \\exp\\left(-\\frac{1}{N_o}\\left|y-c\\right|^2\\right)\n        }\\right)$  \n  \nwhere $\\mathcal{C}_{i,1}$ and $\\mathcal{C}_{i,0}$ are the \nsets of constellation points for which the $i\\text{th}$ bit is equal to 1 and 0, respectively. $\\mathbf{p} = \\left[p_0,\\dots,p_{K-1}\\right]$ \nis the vector of LLRs that serves as prior knowledge on the $K$ bits that are mapped to a constellation point and is set to $\\mathbf{0}$ if no prior knowledge is assumed to be available, and $\\Pr(c\\lvert\\mathbf{p})$ is the prior probability on the constellation symbol $c$:\n     $\\Pr\\left(c\\lvert\\mathbf{p}\\right) = \\prod_{k=0}^{K-1} \\text{sigmoid}\\left(p_k \\ell(c)_k\\right)$\n  \nwhere $\\ell(c)_k$ is the $k^{th}$ bit label of $c$, where 0 is \nreplaced by -1.\nThe definition of the LLR has been\nchosen such that it is equivalent with that of logits. This is\ndifferent from many textbooks in communications, where the LLR is\ndefined as $LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=0\\lvert y\\right)}{\\Pr\\left(b_i=1\\lvert y\\right)}\\right)$.\n\nWith the maxlog demapping method, LLRs for the $i\\text{th}$ bit\nare approximated like\n$\\begin{align}\n    LLR(i) &\\approx\\ln\\left(\\frac{\n        \\max_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n            \\exp\\left(-\\frac{1}{N_o}\\left|y-c\\right|^2\\right)\n        }{\n        \\max_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n            \\exp\\left(-\\frac{1}{N_o}\\left|y-c\\right|^2\\right)\n        }\\right)\\\\\n        &= \\max_{c\\in\\mathcal{C}_{i,0}}\n            \\left(\\ln\\left(\\Pr\\left(c\\lvert\\mathbf{p}\\right)\\right)-\\frac{|y-c|^2}{N_o}\\right) -\n         \\max_{c\\in\\mathcal{C}_{i,1}}\\left( \\ln\\left(\\Pr\\left(c\\lvert\\mathbf{p}\\right)\\right) - \\frac{|y-c|^2}{N_o}\\right)\n        .\n\\end{align}$  \n  "
"Here is the definition of CDL: sionna.channel.tr38901.CDL(model, delay_spread, carrier_frequency, ut_array, bs_array, direction, min_speed=0.0, max_speed=None, dtype=tf.complex64)\n\nHere is the source code of CDL:   \n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/channel/tr38901/cdl.html#CDL)  \n\nsource code:\n```python\n#\n# SPDX-FileCopyrightText: Copyright (c) 2021-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n\"Clustered delay line (CDL) channel model from 3GPP TR38.901\"\n\nimport sys\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer\nimport sionna\nfrom sionna.utils import generate_circular_ue_trajectory\n\nimport importlib\ntr38901 = importlib.import_module('.tr38901', 'sionna.channel')\n\nPI = 3.141592653589793\n\n# All the delays models have a nominal delay spread of 100ns\n# The value of the RMS delay spread in meters is obtained from the\n# Rappaport model by the SS and carrier frequency which provide\n# a worst case scenario approximation\nNOMINAL_DELAY_SPREAD = TR38901.nominal_delay_spread = tf.constant(3.42,\n                                                                  'float32')\n\nclass CDL(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"CDL(model, delay_spread, carrier_frequency, ut_array, bs_array, direction, min_speed=0., max_speed=None, dtype=tf.complex64)\n\n    Clustered delay line (CDL) channel model from 3GPP [TR38901]_.\n\n    The :class:`~sionna.channel.tr38901.CDL` model generates :class:`~sionna.channel.MIMOCDL` objects\n    which can be used to generate channel realizations and/or  channel impulse responses\n    in either the time domain or the frequency domain, and then\n    apply the channel to some (transmitted) signal.\n\n    This model is particularly suited to the simulation of wide-band\n    communications as it allows to simulate the channel at a carrier\n    frequency for which the nominal delay spread is very small\n    (as compared to the carrier period).\n\n    Parameters\n    -----------\n\n    model : str\n        CDL model to use. Must be one of \"A\", \"B\", \"C\", \"D\", or \"E\".\n\n    delay_spread : float\n        Nominal delay spread [s].\n        The energy of the channel impulse response ideally\n        should decay to to $0$ within a time window of\n        ``delay_spread`` seconds.\n        This can also be seen as the maximum expected delay of\n        the very first path (line of sight) in the power delay profile.\n\n    carrier_frequency : float\n        Carrier frequency [Hz]\n\n    ut_array : PanelArray\n        Panel array used by the transmitters. The panel array\n        is composed of one or more :class:`~sionna.channel.antenna.Panel` .\n        The panel are polarized in the CDL model, i.e., in the \"A\" model\n        a single panel is used which can be dual polarized.\n\n    bs_array : PanelArray\n        Panel array used by the receivers. The panel array\n        is composed of one or more :class:`~sionna.channel.antenna.Panel`\n\n    direction : str\n        Link direction. Must be either \"uplink\" or \"downlink\".\n\n    ut_array : PanelArray\n        Panel array used by the transmitters. The panel array\n        is composed of one or more :class:`~sionna.channel.antenna.Panel` .\n        The panel are polarized in the CDL model, i.e., in the \"A\" model\n        a single panel is used which can be dual polarized.\n\n    min_speed : float\n        Minimum speed of the UT [m/s].\n        Defaults to `0.`.\n\n    max_speed : None or float\n        Maximum speed of the UT [m/s].\n        If `None`, then `max_speed` assumes the value of\n        :math:`c \\approx 3\\times 10^8 \\,\\text{m/s}` (i.e., no\n        Doppler effect by default).\n        This is also the value to use when the UT is\n        non-moving (i.e., the mobility is not implemented). This\n        is done to ensure that the channel models are transparent\n        to the effect of mobility. If one needs to disable this\n        feature, one can set ``max_speed`` to a very large value\n        (e.g., :math:`10^3`).\n\n    dtype : Complex tf.DType\n        Defines the datatype for internal calculations and the output.\n        Defaults to `tf.complex64`.\n\n    Input\n    -----\n\n    num_time_steps : int\n        Number of time steps\n\n    sampling_frequency : float\n        Sampling frequency [Hz]\n\n    Output\n    -------\n        (h_freq, a_rx, a_tx)\n            Tuple:\n\n    h_freq : [batch size, num_rx, num_rx, num_tx, num_tx, num_paths, num_time_steps], tf.complex\n        Frequency-domain channel responses\n\n    a_rx : [batch size, num_rx, num_rx, num_time_steps, num_rx_ant, num_tx_ant], tf.complex\n        Frequency-domain channel coefficient matrices, that map the\n        channel inputs to the channel outputs\n\n    a_tx : [batch size, num_tx, num_tx, num_time_steps, num_rx_ant, num_tx_ant], tf.complex\n        Frequency-domain channel coefficient matrices, that map the\n        channel inputs to the channel outputs\n    \"\"\"\n    def __init__(self, model, delay_spread, carrier_frequency, ut_array,\n                 bs_array, direction, min_speed=0., max_speed=None,\n                 dtype=tf.complex64):\n\n        assert dtype.is_compatible_with(ut_array.dtype),\\\n            \"ut_array and antenna disctype must match\"\n        assert dtype.is_compatible_with(bs_array.dtype),\\\n            \"bs_array and antenna disctype must match\"\n\n        if direction not in (\"uplink\", \"downlink\"):\n            raise ValueError(\"Invalid link direction\")\n        self.direction = direction\n\n        # If max_speed=None, set to the largest possible value\n        if max_speed is None:\n            max_speed = 1e3\n        self.max_speed = max_speed\n        self.min_speed = min_speed\n\n        # Set the nominal delay spread\n        self.delay_spread = delay_spread\n        self.carrier_frequency = carrier_frequency\n        self.dtype = dtype\n\n        # Antenna arrays\n        self.ut_array = ut_array\n        self.bs_array = bs_array\n\n        self.beta_rms = tf.sqrt(NOMINAL_DELAY_SPREAD)*0.314\n        if self.delay_spread > 1e-6:\n            self.alpha = 0.0469 * \\\n                (self.delay_spread*1e9)**0.6844 + 0.2813 * \\\n                tf.math.log(self.delay_spread*1e9 + 1e-6)\n            self.beta = 17.55 * \\\n                (self.delay_spread*1e9)**0.6424 + 1.1762 * \\\n                tf.math.log(self.delay_spread*1e9 + 1e-6)\n        else:  # for the deterministic scenario\n            self.alpha = 0.\n            self.beta = self.beta_rms\n\n        self.l_v = 7\n        self.w_v = 3.5\n\n        # Depending on the scenario set r_x and r_y to the panel\n        # dimensions\n        # r_x is the half-power (-3dB) beamwidth in the vertical direction\n        if model in (\"A\", \"B\", \"C\"):\n            self.r_x = self.w_v\n            self.r_y = self.l_v\n        elif model == \"D\":\n            self.r_x = 75.\n            self.r_y = 75.\n        elif model == \"E\":\n            self.r_x = 175.\n            self.r_y = 7.\n\n        # Set the UT velocity vector\n        if self.direction == \"uplink\":\n            self.ut_v = tf.stack(generate_circular_ue_trajectory(ut_array,\n                                            self.delay_spread,\n                                            self.min_speed,\n                                            self.max_speed,\n                                            batch_size=1,\n                                            r_min=self.r_x), 0)\n        else:   # direction == \"downlink\"\n            self.ut_v = tf.zeros([1, 3], tf.float32)\n\n        # Regularization parameter for r_n\n        self.epsilon = 1e-6\n\n        super().__init__()\n\n    def build(self, input_shape):\n        del_shape = tf.concat([input_shape[0], tf.constant([2, 2], tf.int32)], 0)\n        self._del = tf.random.uniform(del_shape, 0, 2, tf.int32)\n\n    def call(self, num_time_steps, sampling_frequency):\n\n        if self.direction == \"uplink\":\n            phi = tf.constant(PI/2., self.dtype)\n        else:\n            phi = tf.constant(0., self.dtype)\n\n        # alpha, beta in Eq.(7.5.1)-24\n        a = self.alpha\n        b = self.beta\n\n        # Slant street angle (Eq.(7.3.1)-2)\n        if self.ut_v[0, 0] == 0. and self.ut_v[0, 1] == 0.:\n            theta_s = self.ut_v[0, 2]\n        else:\n            theta_s = tf.atan(self.ut_v[0, 1]/self.ut_v[0, 0])\n\n        # Panel vertical angles of departure (Eq. (7.2.3)-5)\n        if self.ut_array.num_rows_per_panel == 1:\n            panel = self.ut_array.panel_array[0]\n            panel.downtilt = theta_s - phi\n        else:\n            r = self.r_y\n            for i in (-panel.num_rows_per_panel*r, -panel.num_rows_per_panel*(r+1), 1):\n                downtilt = tf.acos(tf.exp(-(i - 0.5)*panel.num_rows_per_panel*r))\n                downtilt += theta_s - phi\n                panel = self.ut_array[(i, 0)]\n                panel.downtilt = downtilt\n\n        # Using the DFT size as number of samples (which is, in fact, the\n        # sampling frequency), as the subcarrier spacing, and as the\n        # carrier frequency, we do not need to scale or frequency shift.\n        f_norm = tf.cast(1./self.ut_array.dft_size, self.ut_array.dtype)\n        w = self.ut_array.elementary_separation * f_norm\n        r = self.r_x * f_norm\n        t = self.ut_array.dft_size * w/(2.*PI)\n\n        # 3GPP TR 38.901, Section 7.7.1\n        theta_p = tf.asin(r/t)\n        theta_c = PI/12. + 3./38.*tf.atan((self.delay_spread-theta_p)/r)\n        theta_r = 3./38.*tf.atan((self.delay_spread)/r)\n        theta_d = 7./38. *tf.atan(2.* (r-0.5* self.delay_spread)/r)\n        g = (3./2.) * (1. + tf.math.erf((r - self.delay_spread)/self.beta_rms))\n        g_r = (3./2.) * (1. + tf.math.erf(r/self.beta_rms))\n        gamma = g + g_r\n        r_n = 0.5 * (1. - gamma)\n        gamma = 1. + tf.math.erf(r/self.beta)\n        r_n = r_n + 0.5 * gamma\n        gamma = 1. + tf.math.erf((r-0.5*self.delay_spread)/self.beta)\n        r_n = r_n + 0.5 * gamma\n\n        # Eq.(7.5.1)-1\n        r_n = self.r_x * self.r_y * r_n\n        r_n = tf.maximum(r_n, self.epsilon)\n        # Eq.(7.5.1)-11\n        p = 1./ r_n * tf.linalg.diag_part(self.ut_array.gain(r)) + tf.complex(0., 0.)\n        # Eq.(7.5.1)-10\n        e_r = -0.5 * tf.math.log(r_n)\n        # Eq.(7.2.3)-3\n        # The received power is the sum of the powers from which the reflected\n        # signal arrives and the line-of-sight signal\n        # This is a simplification of Eq.(7.2.3)-3, since the\n        # directin and reflected path have the same coefficient in rician\n        # scenarios. However, this is only aopproximation is only valid\n        # for the specular reflection model (as is done here).\n        # Otherwise, one would need to compute the coefficients\n        # for the different multipath components.\n        # We do not compute the power in dB here, since this would\n        # introduce an additional branch, which is not wanted for CDP.\n        # path = 0 is the LoS path\n        p_0 = p + e_r\n        # path = 12 is the reflected path\n        p_12 = p + e_r - self.alpha - 2.*self.beta\n        # Eq.(7.2.3)-1\n        a = tf.complex( tf.sqrt(p_0), tf.sqrt(p_0) *  tf.math.exp(-1j*theta_r))\n        # Eq.(7.2.3)-8\n        a = tf.complex(tf.sqrt(0.5) * (1 + a), 0) * tf.sqrt(2)\n        p_avg = (tf.abs(a)**2)/2\n        # Eq.(7.2.3)-9\n        p_ri = 2 * (p_12 - p_avg)\n        c = tf.complex(tf.sqrt(p_ri), tf.sqrt(p_ri) * tf.math.exp(-1j*theta_r))\n        # Eq.(7.2.3)-12\n        a_f = tf.sqrt(2 * p_avg * (1 + r_n))\n        # Discrete scattering function S_m (Eq.(7.3.1)-6)\n        s_m = tf.complex( r_n * tf.exp(1j*m*w*t), tf.zeros_like(m, dtype=self.dtype))\n        # Doppler spectrum, obtained by FFT of the scattering function\n        # S_m\n        # We do not compute the square root here, to avoid branches\n        # (i.e., to handle the complex-valued Doppler spectrum).\n        # This is done in the signal domain by the DopplerSpectrum class.\n        l = tf.cast(12, self.dtype)\n        l_m = tf.range(-l, l+1, dtype=self.dtype)\n        f = tf.cast(1/self.ut_array.dft_size, self.dtype) * tf.range(\n                0, self.ut_array.dft_size, dtype=self.dtype)\n        f = tf.math.minimum(f, 1. - f) # avoid numerical issues at F=0 and 1\n        doppler_spectrum = tf.experimental.DtypeSettings( self.dtype\n                ).fft(tf.cast(s_m, tf.complex128)) #, norm='ortho')\n        doppler_spectrum = doppler_spectrum[:,\n            tf.cast(self.ut_array.dft_size/2, 'int32')]\n        doppler_spectrum = tf.complex(\n            tf.math.sqrt(tf.cast(p_ri, 'float64')) * tf.math.abs(doppler_spectrum),\n            tf.math.atan2(tf.math.imag(doppler_spectrum),\n                          tf.math.real(doppler_spectrum)))\n        doppler_spectrum = tf.cast(doppler_spectrum, self.dtype)\n\n        # Eq.(7.3.1)-16\n        # The received power is the sum of the powers from which the\n        # scattered (and reflected) paths arrive and the power of the\n        # LoS path\n        p_scat = tf.math.reduce_sum(2 * p_ri)\n        p_tot = 2 * (p_0 + p_12) + p_scat\n        # LoS  component\n        # Eq.(7.2.3)-1 or Eq.(7.5.1)-1\n        a_los = tf.complex(tf.sqrt(p_0), tf.sqrt(p_0))\n        # Scattered component\n        # Eq.(7.5.1)-1\n        r_eff = self.r_x * self.r_y\n        a_scat = tf.complex( tf.sqrt(p_ri), tf.sqrt(p_ri) * tf.math.exp(\n                    -1j*theta_r))\n        # Eq. (7.3.1)-5\n        # x = r*cos(theta_s) is the vertical displacement\n        # y = r*sin(theta_s) is the horizontal displacement\n        # z = r_eff is the longitudinal displacement\n        t_x = 2. * self.l_v * (0.5 * (1. + tf.math.cos(self.r_x * theta_s)))\n        t_y = 2. * self.w_v * (0.5 * (1. + tf.math.cos(self.r_y * phi)))\n        t_z = self.delay_spread - self.alpha / self.beta\n        a = a_scat * tf.math.exp( - (t_x/t_xa)**l_v\n                    - (t_y/t_ya)**l_v\n                    - (t_z/r_n)**l)\n        a = tf.cast(a, dtype)\n\n        # Generate angular frequency for each diffraction order\n        omega_m = f * tf.math.abs(tf.cast(1., self.dtype) -\n                             tf.math.pow( tf.cast(10., self.dtype),\n                                        - l\\\n                                            * tf.cast(5. + self.alpha, self.dtype)))\n        # Compute amplitude for each diffraction order\n        a = tf.math.real(a)\n        a = tf.where(omega_m > 1./(2.*PI)*r_n, a*tf.math.exp(-2.*PI*omega_m*t_s), a)\n        a = tf.complex(a, tf.zeros_like(a))\n\n        # Eq.(7.3.1)-4\n        phi_m = - self.beta / self.beta_rms * tf.math.log(\n                tf.cast(1.,\n                self.dtype) - l * r_n / self.beta_rms)  - theta_r\n        k_hat = tf.stack([tf.cos(phi_m)*tf.sin(theta_s),\n                         -tf.sin(phi_m)*tf.sin(theta_s),\n                         tf.ones_like(phi_m)], axis=-1)\n        k = tf.sqrt(1. -  tf.math.reduce_sum( k_hat**2))\n        # Eq.(7.3.1)-3\n        theta_dif = 16 * PI * r * r * \\\n            tf.math.pow(tf.cast(1., self.dtype) - l * r / self.beta_rms, 0.5)\\\n                / self.c\n        f"
"To handle an `AssertionError` when the input encoding matrix is not valid in the `LinearEncoder` class, you can use a `try/except` block to catch the error. In the `try` block, you can attempt to create an instance of the `LinearEncoder` with the encoding matrix that might cause the `AssertionError`. In the `except` block, you can catch the `AssertionError` and handle it in a way that doesn't cause the program to stop or fail.\n\nHere's how you can do it:\n\n```python\ntry:\n    # Attempt to create an instance of LinearEncoder with the encoding matrix\n    encoder = LinearEncoder(enc_mat=your_encoding_matrix, is_pcm=False, dtype=tf.float32)\nexcept AssertionError as e:\n    # Handle the AssertionError here. Print an error message and do not create the encoder.\n    print(\"AssertionError: The encoding matrix is not valid. Details: \", str(e))\n    # You can also choose to re-raise the AssertionError to the calling function\n    # This will stop the execution of the current block and the control will move to the calling function.\n    raise e\n```\n\nIn this code: \n\n- If the `LinearEncoder` is instantiated with a valid encoding matrix, the code will continue to execute normally.\n- If the encoding matrix is not valid and an `AssertionError` is raised, the code in the `except` block will be executed.\n- The error message is printed and the program doesn't stop.\n- The `LinearEncoder` instance is not created in case of the `AssertionError`.\n\nPlease replace `your_encoding_matrix` with the actual encoding matrix you are using. The `AssertionError` message will be printed with the details from the `str(e)`, which is the `AssertionError` instance. You can also choose to re-raise the `AssertionError` to the calling function if you want to stop the program immediately when the matrix is not valid. \n\nNote: The context doesn't provide the details of the LinearEncoder implementation, so the actual creation of the `enc_mat` is not included in this answer. The actual encoding matrix should be provided in the `your_encoding_matrix`. If the `AssertionError` is caused by a non-valid encoding matrix, the details of the matrix that caused the error should be included in the error message."
"2021-02-22   17:10\n   \nBy Interactions, Universit\u00e9 Paris-Saclay, France\n2021-02-10   13:01\n\nTimo Walter, Lars Klett, Fabian H\u00fcger, Johannes Pamp, and Marco Mehlano,\nDeepRX: A Deep Learning Based Ray Tracer,\nin OSA Continuum, Vol. 3, No. 2, pp. 293-304.\n  "
"To add a transmitter and receiver to the scene and compute the propagation paths in Sionna, follow the steps below:\n\n1. **Import the necessary modules and set up the environment**:\n\n    ```python\n    from sionna.rt import load_scene  # Load the scene function from the Sionna Ray Tracing module\n\n    # Load the scene\n   scene = load_scene(sionna.rt.scene.munich)\n\n    ```\n    This code imports the `load_scene` function from the Sionna RT module and loads a scene called 'munich'.\n\n2. **Add a Transmitter**:\n\n    ```python\n    tx = Transmitter(name=\"tx\", position=[8.5,21,27], orientation=[0,0,0]) #[Tx=Transmitter(\"tx\", [30,40,1.5], [0,0,0])]\n    scene.add(tx)\n    ```\n\n    In this code, a `Transmitter` object is created with a name of \"tx\", a position at (8.5,21,27), and an orientation of (0,0,0). The transmitter is then added to the scene using the `add` method.\n\n3. **Add a Receiver**:\n\n    ```python\n    rx = Receiver(name=\"rx\", position=[44,77,1.5], orientation=[0,0,0])\n    scene.add(rx)\n    ```\n\n    In this code, a `Receiver` object is created with a name of \"rx\", a position at (44,77,1.5), and an orientation of (0,0,0). The receiver is then added to the scene using the `add` method.\n\n4. **Compute Propagation Paths**:\n\n    ```python\n    paths = scene.compute_paths() #,los=True, reflection=True, diffraction=True, scattering=False, num_samples=1000000, max_depth=5\n    ```\n\n    In this code, the `compute_paths` method of the scene is called to compute the propagation paths. The parameters to this method control which propagation phenomena are taken into account (line-of-sight, reflection, diffraction, and scattering), as well as the number of samples and the maximum depth of the paths.\n\n5. **Visualize the Scene and the Propagation Paths**:\n\n    ```python\n    scene.preview(paths=paths) #tx=tx,rx=rx,\n    ```\n\n    In this code, the `preview` method of the scene is called to visualize the scene and the computed propagation paths. The `paths` parameter passed to this method contains the computed paths as returned by the `compute_paths` method. The `preview` method also accepts parameters to specify the transmitters and receivers used for the paths visualization.\n\n6. **Compute Field Coverage**:\n\n    ```python\n    coverage_map = rx.compute_coverage_map(tx, num_samples=524288)\n    ```\n\n    In this code, the `compute_coverage_map` method of the receiver is called to compute a coverage map, which describes the received power level at different locations in the scene. The `tx` parameter specifies the transmitter and `num_samples` specifies the number of random rays to be traced for each location in the coverage map.\n\nPlease note that the above code snippets are based on the provided context and may need to be adjusted based on your specific requirements, such as the scene geometry, antenna configurations, and the specific parameters for path computation and visualization."
"Using the FastFadingChannel class in Sionna requires three steps:\n\n1. **Set up a model**: This includes specifying parameters for the different channel models. Parameters include the model type (like OFDM, RayleighBlockFading, etc.) and waveform-specific parameters such as `fft_size` and `subcarrier_spacing` for the 'resource_grid' `carrier` type.\n\n2. **Define and link the other necessary components**: You need to define a channel model, an uplink or downlink transmission scenario, and the corresponding antenna arrays. For example, in an OFDM channel model, you'd need to define a `ResourceGrid` and a `DMRS`.\n\n3. **Instantiate a FastFadingChannel layer and call it with a batch size and number of samples**: Based on the setup, you then create an instance of the `FastFadingChannel` class. This instance is callable, meaning you can use it to generate channel responses.\n\nHere's a simplified example of how you might do this:\n\n```python\nfrom sionna.channel import ResourceGrid, DMRS, FastFadingChannel, OFDMChannel\n\n# Setup a model\nCHANNEL_MODEL = \"Rayleigh\"\nCARRIER_MODEL = \"OFDM\"\n\n# Parameters for the channel model\nparams = {\n        \"channel_model\": CHANNEL_MODEL,\n        \"carrier_model\": CARRIER_MODEL,\n        \"speed\": 3.0,              # m/s\n        \"carrier_frequency\": 3.5e9, # Hz\n        \"direction\": \"downlink\",\n        \"ofdm_num_effective_subcarriers\": 12,  # FFT size\n        \"ofdm_cyclic_prefix_length\": 0,  # Length of cyclic prefix\n        \"enable_fast_fading\": True,    # Use Rayleigh block fading\n        \"enable_channel_discrete_process\": False,\n        \"num_tx\": 1,\n        \"num_tx_ant\": 1,\n        \"num_rx\": 1,\n        \"num_rx_ant\": 1\n}\n\n# Link the necessary components\nresource_grid = ResourceGrid(params[\"ofdm_num_effective_subcarriers\"], fft_size=params[\"ofdm_cyclic_prefix_length\"], subcarrier_spacing=15e3)\ndmrs = DMRS(resource_grid, num_cyclic_prefix=params[\"dmrs_cyclic_prefix_length\"])\nchannel_model = RayleighBlockFading(num_tx=params[\"num_tx\"], num_tx_ant=params[\"num_tx_ant\"], num_rx=params[\"num_rx\"], num_rx_ant=params[\"num_rx_ant\"])\n\n# Instantiate a FastFadingChannel with the required KerasLayer parameters\nff_channel = FastFadingChannel(channel_model=channel_model, num_samples_per_symbol=params[\"ofdm_num_effective_subcarriers\"],  # For OFDM, this is the FFT size\n                               delay_spread=params[\"delay_spread\"],\n                               carrier_frequency=params[\"carrier_frequency\"])\n\n# Call the layer with a complex-valued input tensor\n# x should be the channel input (i.e., the transmit signal)\ny = ff_channel(x)\n```\n\nIn this example, `x` is a complex-valued tensor that represents the transmit signal. The tensor can have any shape that can be broadcast to the channel input shape. The output `y` is also a complex-valued tensor and its shape is determined by `x` and the parameters of the channel model. This output `y` represents the channel response over which the channel symbols are scattered. \n\nRemember, this is a simplified example and the actual parameters and setup would depend on the specific requirements of your system."
"Here is the detailed information of ChannelModel from Sionna:   \n  \n[sionna.rt.LambertianPattern(alpha_r, dtype=tf.complex64)](https://nvlabs.github.io/sionna/_modules/sionna/rt/scattering_pattern.html#LambertianPattern)  \n\nLambertian scattering pattern.\n\nIn this model, the scattered field follows a Lambertian pattern, i.e., the reflected ray is equally likely in any direction of the plane of reflection. This model is not differentiable.\n\nThe definition of this model can be found in Section 4.1.2 of [F. Runia, P. Schmi<a href=\"https://www.sionna.rt.ScatteringPattern\">[sionna.rt.ScatteringPattern</a>]</a>.    \n  \n### Parameters\n\n- `alpha_r` (float)\nThe parameter alpha_r of the Lambertian model, related to the roughness of the scattering surface. Valid values are in the range $[0,1]$. A value of $0$ corresponds to purely diffuse reflection, and a value of $1$ corresponds to mirror-like reflection.\n\n- `dtype` (tf.complex64 or tf.complex128)\nThe dtype of the return value. The default is tf.complex64.\n\n### Input\n\n- `k_i` ([3], dtype)\nThe wave vectors of the incoming fields.\n\n- `k_s` ([3], dtype)\nThe desired scattered (or reflected) wave vector. For true random scattering, this parameter is not needed and set to None.\n\n### Output\n\n- `pattern` (dtype)\nThe scattering pattern for every element of `k_s`. The output dtype can be either tf.complex64 or tf.complex128, as specified by the `dtype` parameter."
"Here is the definition of DemapperWithPrior: sionna.mapping.DemapperWithPrior(demapping_method, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, dtype=tf.complex64, **kwargs)  \nThe link of the source code of [DemapperWithPrior](https://nvlabs.github.io/sionna/_modules/sionna/mapping.html#DemapperWithPrior)  \n\nDemaps a (noisy) constellation point to bit-wise Log-Likelihood Ratios (LLRs) or decisions, assuming that prior knowledge on the bits is available.\nThis is deprecated as the fully differentiable implementation has been integrated into Mapping.  \n  \n### Input\n\n- **demapping_method** (str): One of [\"app\",\"maxlog\" \"bayes\"], where \"app\" stands for the a posteriori demapping, \"maxlog\" for the \n  maximum-approximation of the a posteriori demapping, and \"bayes\" for the\n  optimal demapping.\n- **constellation_type** (str): One of [\"qam\", \"pam\", \"custom\"]. For \"custom\", an instance of Constellation must be provided.\n- **num_bits_per_symbol** (int): The number of bits per constellation symbol, e.g., 4 for QAM16. Only required for \"qam\" and \"pam\".\n- **constellation** (Constellation): An instance of Constellation or None. If None, `constellation_type`\n  and `num_bits_per_symbol` must be provided.\n- **hard_out** (bool): If True, the demapper provides hard-decided bits instead of soft values. Defaults to False.\n- **dtype** (tf.DType): One of [tf.complex64, tf.complex128]. The dtype of `y`. Defaults to tf.complex64.\n- **kwargs**: Additional keyword arguments, mainly for backward compatibility.\n\n### Output\n\n- **[, n], tf.float or [, n/num_bits_per_symbol], tf.int** \u2013 LLRs or hard-decisions for every bit. If hard_out is True, this is the output. Otherwise, this tensor needs to be separated into the individual bits to get the original bit-wise LLRs."
"To understand the characteristics of the TB5GScrambler class, follow these steps:\n\n1. **Import the class**: In your Python environment, first import the `TB5GScrambler` class from the Sionna library.\n\n```python\nfrom sionna.fec.scrambling import TB5GScrambler\n```\n\n2. **Initialize an instance of the class**: Define a `TB5GScrambler` object, specifying relevant parameters that the scrambler will use. You can either set parameters to default values or customize them based on your specific use case.\n\nHere's an example of creating a `TB5GScrambler` instance with some custom parameters.\n\n```python\ntb5g_scr = TB5GScrambler(\n    n_rnti=1234,           # RNTI identifier\n    n_id=17,               # Scrambling ID related to cell ID\n    binary=True,           # Whether to use bit flipping or signs flipping\n    channel_type=\"PUSCH\",  # Radio channel type\n    codeword_index=1,      # Index of the codeword to be scrambled\n    dtype=tf.complex64    # Datatype for internal calculations and the output\n)\n```\n\n3. **Explore the class properties and methods**:`TB5GScrambler` provides various properties and methods that are utilized in the scrambling process. Once the object is initialized, you can explore its characteristics. Remember that a property can be accessed like an attribute and a method can be called with parentheses.\n\nHere are some key properties of `TB5GScrambler`:\n- `binary`: Indicates whether bit sequence should be flipped (i.e., binary operations are performed) or the signs should be flipped (i.e., soft-value/LLR domain-based). This property can be overridden when providing input, if needed.\n- `keep_state`: This property is required for descrambler and is always set to True for the TB5GScrambler.\n- `n_id`: Datascrambling ID (n_id). This property can be assigned to multiple values in a list to enable independent scrambling for multiple sequences.\n- `n_rnti`: The radio network temporary identifier (RNTI). This property can also be a list, allowing independent scrambling for multiple streams.\n- `seed`: Scrambler's seed, which determines the initial state of the LFSR (Linear Feedback Shift Register). If it's set to -1, a random seed is generated. This property can be assigned to multiple values in a list to enable independent scrambling for multiple sequences.\n\nAnd here is a key method:\n- `generate_l()`[Method] -  This method is used to generate the scrambling sequence. It takes as input the parameters `length` and `binary`, which allow for overriding the instance's property `length` and `binary`, if needed.\n\n4. **Apply the scrambling function**: Use the scrambling function to apply the scrambling to a tensor. You can use this in-built function to ensure that the implementation is in a \"use-case\" appropriate manner.\n\nThe implementation also features an inverse unbiasing function, although we won't detail the implementation of this part. \n\n5. **Understand the working**: Take the time to understand how the scrambler manipulates the input based on the parameters you provided. This will help you understand the details of the 5G NR compliant scrambling and descrambling.\n\n6. **Verify the results**: Run a quick test to scramble a known bit sequence and verify the results. Be sure to run both the scrambling and descrambling functions to see if they produce the original bit sequence.\n\nRemember, for the TB5GScrambler to work properly for a given bit sequence, the binary bit sequence should be provided as an input along with the initialized parameters of the TB5GScrambler. The bit sequence should be properly formatted according to the provided context, ensuring that the TB5GScrambler flips the bits either in binary domain or soft-value/LLR domain.\n\nAs the last step, make sure that you can call the methods and properties of the TB5GScrambler class. For example, you can test whether the `generate_l` method can be called using the following code:\n\n```python\ntb5g_scr.generate_l()\n``` \n\nBy following the steps above, you should be able to understand the working of the TB5GScrambler class and its relevant methods and properties."
" To encode random input bits using the Sionna package, follow these steps:\n\n1. Define the parameters for the LDPC code. In the provided context, a 5G compliant LDPC encoder is created with specific parameters including `k` (the number of information bits per codeword), `n` (the codeword length), and `num_bits_per_symbol` (the modulation order, e.g., 2 for BPSK).\n\n```python\nfrom sionna.fec.ldpc import LDPC5GEncoder\n\nk = 100  # Number of information bits per codeword\nn = 200  # Codeword length\nnum_bits_per_symbol = 2  # Modulation order (i.e., 2 for BPSK)\nencoder = LDPC5GEncoder(k=k, n=n, num_bits_per_symbol=num_bits_per_symbol)\n```\n\n2. Generate a batch of random bit vectors. The LDPC encoder expects the input data `u` to be a 2+D tensor, where the first dimension is the number of codewords to be encoded.\n\n```python\nimport tensorflow as tf\n\nbatch_size = 1  # Simulate 1 codeword\nu = tf.random.uniform((batch_size, k), minval=0, maxval=2, dtype=tf.float32)\n```\n\n3. Encode the bits using the LDPC encoder. The encoder returns an LDPC encoded bit-representation `c`.\n\n```python\nc = encoder(u)\n```\n\nThe variable `c` now holds the LDPC encoded bits for the random input bit vector `u`.\n\nRemember to ensure that the parameters (e.g., `k`, `n`, `num_bits_per_symbol`) are properly set according to your application's requirements. For verification, you may also check the output shapes corresponding to the input shapes, which are typically documented for each encoding layer in the Sionna package."
"The `LMMSEEqualizer` class is used for performing Linear Minimum Mean Squared Error (LMMSE) equalization in Orthogonal Frequency-Division Multiplexing (OFDM) Multiple-Input Multiple-Output (MIMO) systems. It is particularly useful in scenarios where the transmitted signal is affected by noise and interference in the channel.\n\nFor OFDM MIMO transmissions, the `LMMSEEqualizer` class takes a tuple as an input. This tuple contains the received OFDM resource grid `y`, the channel estimates `h_hat`, the error variance `err_var`, and the noise `no`. The class uses these parameters to compute LMMSE equalization. Additionally, for each transmitter and stream, you need to configure a `resource_grid` and indicate which streams are detected by which receiver according to the `stream_management`.\n\nThe output of the `LMMSEEqualizer` class is a tuple. It includes the estimated symbols `x_hat` and the effective noise variance `no_eff`. The estimated symbols represent the data symbols of the transmitted frame, and the effective noise variance provides the effective noise power after the equalization, which can be used for further processing like demapping to obtain Log-Likelihood Ratios (LLRs).\n\nTo use this class, you need to configure the suitting [`ResourceGrid`](../ofdm.html#sionna.ofdm.ResourceGrid) and [`StreamManagement`](../mimo.html#sionna.mimo.StreamManagement) instances for the number of transmitters and streams as well as the number of guard carriers, cyclic prefix, etc. Then, you utilize these instances to create an `LMMSEEqualizer` instance.\n\n```python\nlmmse_eq = LMMSEEqualizer(\n    resource_grid=rg, \n    stream_management=sm,\n    whiten_interference=True, \n    dtype=tf.complex64\n)\n```\nwhere `rg` is the `ResourceGrid` instance and `sm` is the `StreamManagement` instance. \n\nYou might also need to set `xla_compat=true` to use this class within a TensorFlow graph function with XLA.\n\n```python\nsionna.Config.xla_compat = True\n```"
"To get started with Sionna, you need to set up the environment, import necessary libraries, and configure the GPU settings if you are using one. \n\nHere are the steps involved:\n\n1. Set up the environment:\n   ```python\n   import os\n   os.environ[\"SIONNA_DEVICES\"] = \"CPU\"  # Use \"GPU:<i>\" to use a specific GPU\n   os.environ[\"SIONNA_AUTOTUNE\"] = \"0\"  # Set to \"1\" to use extensions that can be conditionally compiled\n   os.environ[\"SIONNA_EXPERIMENTAL\"] = \"1\"  # Set to \"1\" to run components in eager mode\n   ```\n\n2. Import necessary libraries:\n   ```python\n   import sionna\n   import tensorflow as tf\n   import numpy as np\n   import matplotlib.pyplot as plt\n   ```\n\n3. Configure the GPU settings:\n   ```python\n   gpu_num = 0  # Use \"\" to use the CPU\n   os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n   os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Avoid warnings from TensorFlow\n   ```\n\nSionna provides several layers and sub-layers to implement a wide range of channel and sub-layer functions. For example, you can implement a 5G NR compliant code, a 3GPP channel model, a mapper, a demapper, a LDPC encoder, a LDPC 5G encoder, a LDPC 3GPP encoder, a polar encoder, a polar 5G encoder, a polar 3GPP encoder, a polar decoder, a convolutional encoder, a Turbo encoder, a convolutional encoder, a 3GPP encoder, a binary memoryless channel, an analog models, an AWGN channel, an OFDM channel, a RayleighBlockFading channel, a JointTimeFreqChannel, a KBestDetector, or a NonUniformQamSource.\n\nThe tutorial is divided into four parts and will guide you through:\n\n- Part 1: Getting started with Sionna\n- Part 2: Differentiable Communication Systems\n- Part 3: Advanced Link-level Simulations\n- Part 4: Toward Learned Receivers\n\nEach part contains a structured tutorial with multiple sections that explain the usage of different components of Sionna and how to implement them. It also includes examples to validate the implementation. \n\nRemember to run the tutorial sections in sequential order to ensure all the dependencies are correctly loaded. If you want to run the tutorial in a Jupyter notebook, you can open the individual notebook for each part and run all the sections sequentially. \n\nAt the end of the tutorial, there are exercises and simulations that you can run to further test your understanding of the material."
"To import the necessary modules from the Sionna package, you can follow these steps:\n\n1. First, import the os module and set the GPU number. If you want to use the CPU, you can set the GPU number to an empty string.\n\n```python\nimport os\ngpu_num = 0 # Use \"\" to use the CPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n```\n\n2. Next, import the Sionna package. If it's not already installed, the code will install it.\n\n```python\ntry:\n    import sionna\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna\n```\n\n3. After successfully importing the Sionna package, you can import the necessary modules. For wireless channel implementation, you would typically need to import `AWGN` (for simulating the Additive White Gaussian Noise channel), `BinarySource` (for generating random data bits), `ebnodb2no` (for converting Eb/No in dB to noise power spectral density), `log10` (for computing the common logarithm), `expand_to_rank` and `reduce_sum_squares` (for computing the noise energy), and `simulate_ber` (for computing the Bit Error Rate).\n\n```python\nfrom sionna.utils import BinarySource, ebnodb2no, log10, expand_to_rank, reduce_sum_squares, simulate_ber\nfrom sionna.channel import AWGN\n```\n\nRemember to replace the necessary modules with the ones you actually need depending on the specific use case."
"To apply the gradient to the weights using an optimizer, you first need to obtain the gradient. In the given context, this is done using the `tape.gradient` method. Here is the relevant code:\n\n```python\nwith tf.GradientTape() as tape:\n    # ... (compute the loss)\ngradient = tape.gradient(loss, tape.watched_variables()\n```\n\nIn this code, `loss` is the loss function of your model, and `tape.watched_variables()` returns the list of all trainable weights being watched. The `with` statement is used to ensure that the operations are recorded with the tape.\n\nOnce you have obtained the gradient, you can apply it to the weights using the `apply_gradients` method of the optimizer. The optimizer must be an instance of the `tf.keras.optimizers` class, such as `Adam`, as shown in the context. Here is the code:\n\n```python\noptimizer = tf.keras.optimizers.Adam(1e-2)\noptimizer.apply_gradients(zip(gradient, tape.watched_variables()))\n```\n\nIn this code, `1e-2` is the learning rate, which is the first argument of the `Adam` class. The `zip` function is used to combine the gradient and the list of weights into a single argument for the `apply_gradients` method.\n\nWhen you run this code, the `apply_gradients` method will update the weights according to the gradient of the loss function. This is the basic process for applying gradients to the weights using an optimizer in TensorFlow."
"Here is the detailed information of the function: \n\n[sionna.mapping.qam(num_bits_per_symbol, normalize=True)](https://nvlabs.github.io/sionna/_modules/sionna/mapping.html#qam)\n\nGenerates a QAM constellation.  \nThis function generates a complex-valued vector, where each element is a constellation point of an M-ary QAM constellation. The bit label of the n th point is given by the length- num_bits_per_symbol binary represenation of n.\n\n#### Input\n- `num_bits_per_symbol` (int) \u2013 The number of bits per constellation point. Must be a multiple of two, e.g., 2, 4, 6, 8, etc.\n- `normalize` (bool) \u2013 If True, the constellation is normalized to have unit power. Defaults to True.\n\n#### Output\n- [2^num_bits_per_symbol], np.complex64 \u2013 The QAM constellation.\n\n**Note**\nThe bit label of the nth constellation point is given by the binary representation of its position within the array and can be obtained through `np.binary_repr(n, num_bits_per_symbol)`.\n\nThe normalization factor of a QAM constellation is given in closed-form as:  \n$\\sqrt{\\frac{1}{2^{n-2}}\\sum_{i=1}^{2^{n-1}}(2i-1)^2}$  \nwhere $n= \\text{num_bits_per_symbol}/2$ is the number of bits per dimension.\n\nThis algorithm is a recursive implementation of the expressions found in Section 5.1 of 3GPP TS 38.211. It is used in the 5G standard."
"Here is the definition of RandomInterleaver:    \n  \n[sionna.fec.interleaving.RandomInterleaver(seed=None, keep_batch_constant=True, inverse=False, keep_state=True, axis=- 1, dtype=tf.float32, **kwargs)](https://nvlabs.github.io/sionna/_modules/sionna/fec/interleaving.html#RandomInterleaver)  \n  \nRandom interleaver permuting a sequence of input symbols.\n\nThe class inherits from the Keras layer class and can be used as layer in a Keras model.  \n  \nParameters\n\n    seed (int) : Defines the random seed used if the \u2018keep_state\u2019 is set to True.\n        Defaults to None.\n\n    keep_batch_constant (bool) : Defaults to True. If True, each sample in the batch uses the same permutation. Otherwise, for each sample in the batch a unique permutation is generated, which is slower.\n        Note: This option is only used if \u2018keep_state\u2019 is set to True.\n\n    inverse (bool) : Defaults to False. If True, the inverse permutation is performed.\n\n    keep_state (bool) : Defaults to True. If True, the permutation is fixed for multiple calls, which is controlled by the seed.\n        Note: This option is only used if \u2018keep_state\u2019 is set to True.\n\n    axis (int) : Defaults to -1. The dimension that should be interleaved. Cannot be the first dimension if \u2018inverse\u2019 is set to True.\n\n    dtype (tf.DType) : Defines the datatype for internal calculations and the output dtype. Defaults to tf.float32.\n\nInput\nx (tf.DType) \u2013 2+D tensor of arbitrary shape and dtype.\n\nOutput\n    - : tf.DType. 2+D tensor of same shape and dtype as the input \u2018x\u2019.\n\nRaises\n\n    AssertionError \u2013 If axis is not an int.\n\n    AssertionError \u2013 If seed is not int.  \n  \nsource code:   \n  \n```python\nclass RandomInterleaver(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"RandomInterleaver(seed=None, keep_batch_constant=True, inverse=False, keep_state=True, axis=- 1, dtype=tf.float32, **kwargs)\n\n    Random interleaver permuting a sequence of input symbols.\n\n    The class inherits from the Keras layer class and can be used as layer in a\n    Keras model.\n\n    Parameters\n    ----------\n        seed : int\n            Defines the random seed used if the ``keep_state`` is set to\n            True. Defaults to None.\n\n        keep_batch_constant : bool\n            Defaults to True. If True, each sample in the batch uses the same\n            permutation. Otherwise, for each sample in the batch a unique\n            permutation is generated, which is slower.\n            Note: This option is only used if the ``keep_state`` is set to True.\n\n        inverse : bool\n            Defaults to False. If True, the inverse permutation is performed.\n\n        keep_state : bool\n            Defaults to True. If True, the permutation is fixed for multiple\n            calls, which is controlled by the seed.\n            Note: This option is only used if the ``keep_state`` is set to True.\n\n        axis : int\n            Defaults to -1. The dimension that should be interleaved.\n            Cannot be the first dimension if ``inverse`` is set to True.\n\n        dtype : tf.DType\n            Defines the datatype for internal calculations and the output\n            dtype. Defaults to `tf.float32`.\n\n    Input\n    -----\n        x : tf.DType\n            2+D tensor of arbitrary shape and dtype.\n\n    Output\n    ------\n        : tf.DType\n            2+D tensor of same shape and dtype as the input ``x``.\n\n    Raises\n    ------\n        AssertionError\n            If ``axis`` is not an int.\n\n        AssertionError\n            If ``seed`` is not int.\n    \"\"\"\n    # pylint: enable=line-too-long\n\n    def __init__(self,\n                 seed=None,\n                 keep_batch_constant=True,\n                 inverse=False,\n                 keep_state=True,\n                 axis=-1,\n                 dtype=tf.float32,\n                 **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n\n        if not isinstance(axis, int):\n            raise AssertionError(\"axis must be int.\")\n        self._axis = axis\n\n        if seed is not None and not isinstance(seed, int):\n            raise AssertionError(\"seed must be int.\")\n        self._seed = seed\n\n        if not isinstance(keep_batch_constant, bool):\n            raise TypeError(\"keep_batch_constant must be bool.\")\n        self._keep_batch_constant = keep_batch_constant\n\n        if not isinstance(inverse, bool):\n            raise TypeError(\"inverse must be bool.\")\n        self._inverse = inverse\n\n        if not isinstance(keep_state, bool):\n            raise TypeError(\"keep_state must be bool.\")\n        self._keep_state = keep_state\n\n    @property\n    def axis(self):\n        \"\"\"Axis to be permuted.\"\"\"\n        return self._axis\n\n    def call(self, inputs):\n        \"\"\"interleaving function.\n\n        This function returns the permuted version of inputs.\n\n        ``axis`` denotes the dimension that should be interleaved.\n        Cannot be the first dimension if ``inverse`` is `True`.\n\n        ``keep_batch_constant`` if `True`, the same permutation is used for\n        the entire batch. Otherwise, permutatations are generated\n        per sample in the batch, which is slower.\n\n        \"\"\"\n        if self._axis < 0:\n            self._axis = tf.shape(inputs)[self._axis]\n\n        # prepare indices\n        input_shape = tf.shape(inputs)\n        range_ = tf.range(input_shape[self._axis], dtype=tf.int32)\n        idxs = tf.reshape(tf.random.shuffle(range_),\n                          tf.shape(inputs)[self._axis])\n\n        # tile indices along the other dimensions\n        perm_shape = tf.concat((tf.expand_dims(1, axis=0),\n                                tf.gather(input_shape, range_),\n                                tf.expand_dims(1, axis=-1)), axis=0)\n        idxs = tf.tile(tf.expand_dims(idxs, axis=-1), perm_shape)\n        x_out = tf.gather(inputs, idxs, axis=self._axis)\n\n        return x_out\n```"
"INPUT:\n\n-   `num_bits_per_symbol` (int): The number of bits per constellation point.\n-   `normalize` (bool): If `True`, the constellation is normalized to have unit power. Defaults to `True`.\n\nOUTPUT:\n\n-   `[2^num_bits_per_symbol], np.float32`: The PAM constellation.\n  \nsource code:  \n```python\ndef pam(num_bits_per_symbol, normalize=True):\n    r\"\"\"\n    Generates a PAM constellation.\n\n    This function generates a real-valued vector, where each element is\n    a constellation point of an M-ary PAM constellation. The bit\n    label of the `n` th point is given by the length-`num_bits_per_symbol`\n    binary represenation of `n`.\n\n    Input\n    -----\n    num_bits_per_symbol : int\n        The number of bits per constellation point. Must be positive.\n\n    normalize : bool\n        If `True`, the constellation is normalized to have unit power.\n        Defaults to `True`.\n\n    Output\n    ------\n    : [2^num_bits_per_symbol], np.float32\n        The PAM constellation.\n\n    Note\n    ----\n    The bit label of the nth constellation point is given by the binary\n    representation of its position within the array and can be obtained\n    through ``np.binary_repr(n, num_bits_per_symbol)``.\n\n    The normalization factor of a PAM constellation is given in\n    closed-form as:\n\n    .. math::\n\n        \\sqrt{\\frac{1}{2^{n-1}}\\sum_{i=1}^{2^{n-1}}(2i-1)^2}\n\n    where :math:`n= ` ``num_bits_per_symbol`` is the number of bits\n    per symbol.\n\n    This algorithm is a recursive implementation of the expressions found in\n    Section 5.1 of [3GPPTS38211]_. It is used in the 5G standard.\n    \"\"\"\n    assert num_bits_per_symbol > 0, \"Minimum value is 1.\"\n\n    if num_bits_per_symbol == 1:\n        return np.array([-1.0, 1.0], dtype=tf.as_dtype(DTYPE))\n\n    two_n = 2*num_bits_per_symbol\n    x = np.arange(2**num_bits_per_symbol) # Note PAM is initialized to log2M zeros...\n    x = x - (2**num_bits_per_symbol-1)/2 # ... if normalize is False,\n                                          # this will be reset.\n    x = x.astype(PREC_TYPE) # Adjust the dtype.\n    x = tf.as_dtype(DTYPE)(x) # to handle the promotion\n\n    # recursion, the base case is two_n =2, i.e., num_bits_per_symbol=1\n    c = pam(two_n, False)\n\n    # normalize and return\n    if normalize:\n        c = normalize(c)\n    return c\n```  \n  "
"The `return_crc_status` and `output_dtype` are two parameters used in the `Polar5GDecoder` class in Sionna for the purpose of modifying the behavior of the decoder when it returns outputs.\n\nThe `return_crc_status` parameter is a boolean type. When it's set to `True`, the decoder will return the CRC (Cyclic Redundancy Check) status along with the information bits. This is useful because the CRC is a type of check that can detect errors in the transmitted data. Including the CRC status in the decoder's output allows the user to know whether the decoded data was likely to be correct or not. An 'AssertionError' will be raised if the `return_crc_status` is not bool.\n\nThe `output_dtype` is a parameter of type `tf.DType`. It defines the output datatype of the decoder. The internal precision of the calculations in the decoder might be different from the output precision. By default, the `output_dtype` is set to `tf.float32`. It means that the decoder will use 32-bit floating point format for the output. The `output_dtype` parameter allows the user to choose the datatype for the output of the decoder. If the `output_dtype` parameter is not `tf.DType`, an 'AssertionError' will be raised.\n\nHere is an example of how to use these parameters in the `Polar5GDecoder`:\n\n```python\nfrom sionna.fec.polar.decoding import Polar5GDecoder\n\ndecoder = Polar5GDecoder(enc_polar=encoder,\n                         dec_type='SC',\n                         return_crc_status=True,\n                         output_dtype='tf.float32')\n```\n\nIn this example, a `Polar5GDecoder` is instantiated with an encoder instance (`encoder`) that is a `Polar5GEncoder`. The `dec_type` is set to 'SC', `return_crc_status` is set to `True`, and `output_dtype` is set to `tf.float32`. These parameters will modify the behavior of the decoder as described above."
"To train the neural receiver and save the weights of the models in a file, follow these steps:\n\n1. Set up a training loop with a fixed number of iterations. In the provided context, the number of iterations is 100,000, as specified by `NUM_TRAINING_ITERATIONS`.\n\n2. Use TensorFlow's `tf.keras.optimizers.Adam` optimizer for training. This is a variant of the Adam optimizer, which is used to perform stochastic gradient descent. The `Adam` optimizer is initialized with default settings, but you can also specify custom settings if needed.\n\n3. Inside the training loop, sample a batch of SNRs (signal-to-noise ratios) for the training. This is done using TensorFlow's `tf.random.uniform` function, which samples random values from a uniform distribution. The `ebno_db` variable is used to store the sampled SNRs.\n\n4. Perform a forward pass through the end-to-end system using the neural receiver. The `model` is assumed to be an instance of the `OFDMSystemNeuralReceiver` class, which is designed to work with the `LDMC` layer.\n\n5. Compute the gradients of the loss with respect to the weights using TensorFlow's `tape.gradient` function. The `tape` parameter is an instance of `tf.GradientTape` that was used to record the operations during the forward pass.\n\n6. Apply the gradients to the weights using the `optimizer.apply_gradients` function. This function requires a list of gradient-weight pairs, which can be generated using the `zip` function.\n\n7. Periodically print the progress of the training. In the provided context, the progress is printed every 100 iterations.\n\n8. After the training loop, save the weights of the models in a file. This is done by calling the `model.get_weights` function to get the current weights of the model, and then using the `pickle.dump` function to write the weights to a file. The file is opened in binary write mode (`'wb'`).\n\nPlease note that the exact implementation may vary depending on the specific requirements of your project, the architecture of your models, and the data you are working with."
"Here are the links to it:\n\n[Run in Google Colab](https://colab.research.google.com/github/nvlabs/sionna/blob/main/examples/Sionna_tutorial_part1.ipynb)\n\n[View on GitHub](https://github.com/nvlabs/sionna/blob/main/examples/Sionna_tutorial_part1.ipynb)\n\n[Download notebook](https://nvlabs.github.io/sionna/examples/Sionna_tutorial_part1.ipynb)"
"To run throughput tests in Graph Mode using the Sionna Python package, follow the steps below:\n\n1. Define the model: Create a class for the model you want to test. The model should inherit from `tf.keras.Model` and include the `@tf.function` decorator. This allows the model to be run in graph mode. \n\n2. Instantiate the model: Create an instance of the model. The model should be compiled to ensure that it is ready for evaluation.\n\n3. Set the batch size: The batch size determines how many examples are processed at the same time. \n\n4. Run the throughput test: Use the `sionna.utils.run_throughput` function to run the throughput test. This function will evaluate the model over a range of Eb/No values and measure the average number of examples processed per second.\n\n5. (Optional) Visualize the results: You can use the `PlotBER` class to plot the results of the throughput test. This class provides a convenient way to visualize the Bit Error Rate (BER) performance of a model.\n\nHere is an example of how to define a simple throughput test using the Sionna package:\n\n```python\nfrom sionna.utils import run_throughput\nfrom sionna.utils import PlotBER\n\n# define the model class\nclass MyModel(tf.keras.Model):\n    # ... (define the model here) ...\n\n# instantiate the model\nmodel = MyModel()\n\n# set the batch size\nbatch_size = 128\n\n# run the throughput test\nber = run_throughput(model, \n                     ebno_dbs=ebno_db, \n                     batch_size=batch_size, \n                     num_target_block_errors=100, \n                     num_bits_per_symbol=2, \n                     min_mc_iter=100, \n                     max_mc_iter=1000, \n                     num_coders=1, \n                     coder=ldpc_encoder, \n                     spatial_corr=False, \n                     num_parallel=8, \n                     fast_ber=True, \n                     graph_mode=True, \n                     max_bler=0.1)\n\n# visualize the results\nber_plot = PlotBER(\"LDPC BP Decoding\")\nber_plot.show()\n```\n\nPlease note that this is a simplified example and you may need to adjust the parameters and the model according to your specific needs. \n\nAlso, please be aware that the Sionna package is still in an early development stage and the API might change in the future. As the package is also highly parallellizable, you might want to adjust the `num_parallel` parameter based on the number of available CPU threads."
"The task requires demonstrating the use of the `ConvEncoder` class from Sionna's convolutional coding module to instantiate a rate-1/2, constraint-length-3 convolutional code encoder in two ways: using the `rate` and `constraint_length` parameters and using the `gen_poly` parameter.\n\nTo create a `ConvEncoder` using the `rate` and `constraint_length` parameters, you can simply import the class and then instantiate it by specifying the desired rate and constraint length:\n\n```python\nfrom sionna.fec.conv.encoding import ConvEncoder\n\n# Create a rate-1/2, constraint-length-3 convolutional code encoder\nencoder_1 = ConvEncoder(rate=1/2, constraint_length=3)\n```\n\nThis method directly initializes a convolutional encoder with the desired properties. The `gen_poly` method provides another way to create a `ConvEncoder`. You can either specify the generator polynomials for the encoder or let the function generate random generator polynomials for the given rate and constraint length.\n\nHence, to use `gen_poly` to create a `ConvEncoder` instance at a rate of 1/2 and constraint length of 3, you would do:\n\n```python\nfrom sionna.fec.conv.encoding import gen_poly, ConvEncoder\n\n# Create rate-1/2, constraint-length-3 convolutional code encoder using gen_poly\nencoder_2 = ConvEncoder(gen_poly=gen_poly(rate=1/2, constraint_length=3))\n```\n\nThe `gen_poly` function returns the generator polynomials based on the input parameters. Here, `gen_poly` is used to create an instance of `ConvEncoder` with random generator polynomials; however, you can also pass predetermined generator polynomials to `gen_poly` to create an encoder with those specified polynomials.\n\nTo demonstrate the usage of either encoder, you would typically supply input data to the encoder in the form of a binary sequence and receive the encoded output. The binary sequence should match the `k` number of information bits per codeword expected by the encoder. The encoder itself is stateless and does not perform interleaving or any recursive encoding operations, which are characteristic features of turbo and rsc encoders. Interleaving and termination are examples of operations that the `ConvEncoder` class does not handle, as it is designed to specifically encode bits from a tail-biting convolutional code, which is a principal component in systematic recoding.\n\nNote: The example code snippets provided create instances of the convolutional encoder. To actually see the encoder in action, you would need to feed it input data and observe the encoded output, which is beyond the scope of the given context. Remember also to handle import statements correctly and ensure that the Sionna package is installed and configured in your Python environment."
" \n\nModel Parameters:\n\nThe RMa model from the 3GPP specification is utilized to create a channel object in conjunction with the 'set_topology' function to establish the network configuration. The instantiation of the 'RMa' model takes several input parameters as shown in the code snippet in the given context:\n\n- `carrier_frequency`: Frequency of the carrier in Hertz. This represents the carrier frequency which corresponds to the channel model.\n- `ut_array` and `bs_array`: Instance of the PanelArray, representing the panel array used by the user terminals (UT) and base stations (BS) respectively.\n- `direction`: Set to either \"uplink\" or \"downlink\", representing the direction of the transmission.\n- `enable_pathloss`: A boolean parameter which, when set to `True`, enables path loss following the model of the environment in which the wireless devices are transmitting and receiving.\n- `enable_shadow_fading`: Another boolean parameter which, when set to `True`, enables the wireless channel's time variability via so-called fast fading processes.\n- `always_generate_lsp`: A boolean parameter. If set to `True`, the model always generates line-of-sight (LoS) and non-LoS scenario independently of the network configuration. If set to `False`, it follows the configuration set by the 'set_topology' method.\n- `dtype`: Defines the datatype for internal calculations and the output datatype. Defaults to `tf.complex64`. \n\nMethod: set_topology\n\nThe `set_topology` method establishes the network configuration. The parameter `ut_loc` represents the locations of the user terminals, `bs_loc` represents the locations of the base stations, `ut_orientations` represents the orientations of the user terminals, `bs_orientations` represents the orientations of the base stations, `ut_velocities` represents the velocities of the user terminals, `in_state` represents the indoor/outdoor state of user terminals.\n\nPrinciples:\n\n1. Instantiation of the RMa model: The RMa model is used to simulate the wireless channel. It uses the PanelArray for the representation of the antenna arrays used by the base stations and user terminals.\n\n2. Network Topology Configuration with set_topology: The `set_topology` method is used to set up the network configuration. The method follows the batch example principle, where the size of the batch examples is used when setting up the network topology.\n\n3. Data Transmission: After the network configuration is set, the RMa model can be used to simulate the transmission of data from the base station to the user terminal.\n\nInputs and Outputs:\n\nInputs to the set_topology method (`ut_loc`, `bs_loc`, `ut_orientations`, `bs_orientations`, `ut_velocities`, `in_state`) represent the network parameters. These parameters are provided to model the network configuration.\n\nAn output from this method is the pathloss model which is an instance of the Pathloss class from Sionna. This output can be used as an input in the creation of a channel model using the instance of LayerModel.\n\nThe method used for the creation of the channel model after the network topology is set is not explicitly mentioned in the context. It could be any of the methods - time_channel, ofdm_channel, or frequency_channel. The choice of the method would depend on the waveform used and the requirement of the user. For example, it could be time-channel for simulating the channel at the sub-carrier level in an OFDM system, or it could be a frequency domain model if the user wants to simulate the channel's effect on the usage of the resource grid. \n\nOutput of the channel model represents the channel's effect on the transmitted signal, which can then be input into a layer model to simulate the channel's impact on the signal's transmission. This will give an output which is then used in the reverse process in the LayerModel when decoding the transmitted signal."
"The `Trellis` structure from the `sionna.fec.conv.utils` module is a finite state machine used in the encoding process of convolutional codes. The Trellis structure provides a way to visualize the encoding process and is also utilized in the decoding process in Viterbi and BCJR decoders for Recursive Systematic Convolutional (RSC) codes.\n\nThe `gen_poly` parameter is a tuple that represents the generator polynomials in a convolutional encoder. These polynomials define the connections between the encoder's internal shift registers. If Trellis is recursive and systematic, the first polynomial in the `gen_poly` tuple is taken to be the feedback polynomial, which is used to close the encoder's feedback loop. \n\nAn important feature is that the Trellis structure is also used to define the encoder's output. At each time step, the encoder's output is determined by the state transition and the input bit (or, in the case of a systematic encoder, the corresponding information bit). This means the Trellis is used not only to guide the encoding process in the recursive encoder, but also to produce the code symbols for the encoded data.\n\nThe `rsc` parameter is a boolean value that indicates whether the Trellis is recursive systematic. In a recursive systematic Trellis, the feedback polynomial is such that the encoder is guaranteed to output systematic binary sequences, given any binary input sequence as long as there are at least `constraint_length` time steps remaining. This ensures that the original information bits are easily recognizable when decoded and is a key characteristic of RSC codes.\n\nWhen `gen_poly` is provided, the `Trellis` object is created with the given generator polynomial, and if `rsc` is true, the first polynomial in `gen_poly` is taken to be the feedback polynomial. If `gen_poly` is null, i.e., when `gen_poly` is set to None, then `rate` and `constraint_length` must be provided instead, which are the rate of the code and the constraint length of the code, respectively.\n\nTo summarize, the `gen_poly` and `rsc` parameters in the `Trellis` structure are essential parts of configuring the convolutional codes used in the recursive systematic encoder, and they define the encoders connection and operation during the encoding process. Implementing a Trellis structure with the given generator polynomials allows simulation of coders and decoders using the Trellis as a fundamental part of the process.\n\nIt's important to note that the module provides a disclaimer that the `Trellis` class is not implemented for non-recursive generator matrices or for encoders that are not systematic, as it focuses on code simulations."
"Here is the detailed information of CarrierConfig:   \n  \n[sionna.nr.CarrierConfig](https://nvlabs.github.io/sionna/_modules/sionna/nr/carrier_config.html#CarrierConfig)  \n  \nCarrier configuration for a (possible dual) pair of PHY layer Protocols - currently only the 5G NR one.\n\nThis class defines parameters at the physical layer such as the subcarrier spacing, the number of PRBs, the FFT size, the duration of a slot, etc. A configurable property in this class can be automatically shared by multiple components (e.g., the subcarrier spacing can be configured in the CarrierConfig and accessed in the PUSCH Transmitter).\n\n**5G NR Dual Connectivity**\n\nIn the 5G NR, two types of dual connectivity (DC) are defined [3GPP TS 38.337 \u201cStudy on Dual Connectivity\u201d]:\n-\t**Type I** DC: In this type of DC, the UE is configured for simultaneous communication with the 4G evolved NodeB (MeNB) and the 5G gNodeB (gNB). Data and control information are transmitted via the 4G RAN or the 5G RAN. It is assumed that the core network (CN) has been enhanced to support DC. Type I DC is also referred to as the data plane (DP) or user plane (UP) DC.\n-\t**Type II** DC: This type of DC requires no data duplication and is controlled by the 5G gNB. It involves the addition of a secondary 5G carrier, i.e., the PDCCH and PDSCH are operated for the secondary carrier, while a primary 5G NR carrier is used for control signaling. Type II DC is also called the control plane (CP) DC.\n\nThis class supports the configuration of both type I and type II DC. The 4G and 5G RAN are referred to as \u201cprimary\u201d and \u201csecondary\u201d, respectively. The carrier that is used to send PUSCH and PDSCH is defined as \u201ccarrier 0\u201d and \u201ccarrier 1\u201d in 5G NR, respectively.\n\nFor DC, it is assumed that a `secondary_pusch_config` has been created. The `secondary_pusch_config` is either used to simulate type II DC using the sionna.nr.PUSCHTransmitter and sionna.nr.PUSCHReceiver, or to simulate the 5G NR PUSCH transmissions over the 4G RAN using the sionna.nr.PUSCHTransmitter and sionna.nr.PUSCHReceiver with the sionna.nr.ETSI5GChannelModel and the 3GPP PUSCH resources from the sionna.nr.RAN.\n\n**Example**\n```python\ncarrier_config = CarrierConfig(num_ofdm_symbols=14,\n                               subcarrier_spacing=30,\n                               configuration=CarrierConfigType.FR1)\n```\n\n**Properties**\n\n- `cyclic_prefix`  \n  Cyclic prefix length.  \n  Type: `str` ([\"normal\", \"extended\"])  \n  Read only: `True`  \n  Only relevant if `fft_size` and `subcarrier_spacing` are configured through the `configuration`.\n- `cyclic_prefix_length`  \n  Cyclic prefix length in seconds.  \n  Type: `float`  \n  Read only: `True`  \n- `configuration`  \n  OFDM configuration.  \n  Type: `CarrierConfigType`  \n- `configuration_type`  \n  Type: `str` ([\"legacy\", \"fr1\", \"fr2\"])  \n  Read only: `True`  \n- `cp_dl`  \n  Downlink cyclic prefix length.  \n  Type: `str` ([\"normal\", \"extended\"])  \n  Read only: `True`  \n  Only relevant if `fft_size` and `subcarrier_spacing` are configured through the `configuration`.\n- `cp_dl_length`  \n  Downlink cyclic prefix length in seconds.  \n  Type: `float`  \n  Read only: `True`  \n- `cp_ul`  \n  Uplink cyclic prefix length.  \n  Type: `str` ([\"normal\", \"extended\"])  \n  Read only: `True`  \n  Only relevant if `fft_size` and `subcarrier_spacing` are configured through the `configuration`.\n- `cp_ul_length`  \n  Uplink cyclic prefix length in seconds.  \n  Type: `float`  \n  Read only: `True`  \n- `delta_f`  \n  Subcarrier spacing $\\Delta f$ in Hertz.  \n  Type: `float`  \n  Read only: `True`  \n- `effective_subcarrier_spacing`  \n  Effective subcarrier spacing in Hz.  \n  Type: `float`  \n  Read only: `True`  \n- `fft_size`  \n  FFT size.  \n  Type: `int`  \n  Read only: `True`  \n- `frame_number`  \n  System frame number.  \n  Type: `int`  \n- `kappa`  \n  Ratio of sampling times for 60 kHz and 480 kHz subcarrier spacing.  \n  Type: `float`  \n  Read only: `True`  \n- `mu`  \n  Subcarrier spacing configuration ($2^{\\mu} 15$ kHz).  \n  Type: `int`  \n  Read only: `True`  \n- `n_cell_id`  \n  Physical layer cell identity $N_\\text{ID}^\\text{cell}$.  \n  Type: `int`  \n- `n_dmrs`  \n  Number of (additional) DMRS symbols.  \n  Type: `int`, (0, 1, 2, 3])  \n- `n_size_grid`  \n  Number of resource blocks in the carrier resource grid  $N^{\\text{size},\\mu}_{\\text{grid},x}$.  \n  Type: `int`  \n- `n_start_grid`   \n  Start of the resource grid relative to common resource block (CRB) 0 $N^{\\text{start},\\mu}_{\\text{grid},x}$.  \n  Type: `int`  \n- `num_cyclic_prefixes`  \n  Number of cyclic prefixes.  \n  Type: `int`  \n  Read only: `True`  \n- `num_dci_symbols`  \n  Number of symbols reserved for DCI.  \n  Type: `int`, (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  \n  Read only: `True`  \n- `num_effective_subcarriers`  \n  Number of effective subcarriers used for data transmission.  \n  Type: `float`  \n  Read only: `True`  \n- `num_dmrs_groups`  \n  Number of DMRS groups.  \n  Type: `int`, (1, 2, 4)  \n  Read only: `True`  \n- `num_dmrs_ports`  \n  Number of DMRS ports.  \n  Type: `int`, (0, 1, 2)  \n  Read only: `True`  \n- `num_pusch_dmrs_symbols`  \n  Total number of DMRS symbols.  \n  Type: `int`, read only (`True`)  \n- `num_res_per_prb`  \n  Number of resource elements per PRB.  \n  Type: `int`  \n  Read only: `True`  \n- `num_slots_per_frame`  \n  Number of slots per frame.  \n  Type: `int`  \n  Read only: `True`  \n- `num_symbols_per_slot`  \n  Number of OFDM symbols per slot.  \n  Type: `int`  \n  Read only: `True`  \n- `num_syms`  \n  Number of allocated OFDM symbols for the numerology.  \n  Type: `int`, read only (`True`)\n- `num_tx_ant`  \n  Number of transmit antennas.  \n  Type: `int`  \n- `num_ue`  \n  Number of UEs.  \n  Type: `int`  \n- `num_vv`  \n  Number of virtual resource blocks configuring the carrier $N_\\text{VV}$.  \n  Type: `float`  \n  Read only: `True`  \n- `pdcch_config`  \n  Configuration of the PDCCH.  \n  Type: `PDCCHConfig`  \n- `pdsch_config`  \n  Configuration of the PDSCH.  \n  Type: `PDSCHConfig`  \n- `radio_service_type`  \n  Radio service type as defined in RIT.  \n  Type: `str` (\"data\", \"iot\", \"emin\")  \n- `sampling`  \n  Sub-carrier spacing configuration relative to 15 kHz  $\\mu`[15kHz/.\\,.kHz]  \n  Type: `SamplingConfig`  \n- `slot_config`  \n  Configuration of the slot format.  \n  Type: `SlotConfig`  \n- `subcarrier_spacing`  \n  Sub-carrier spacing $\\Delta f$[kHz].  \n  Type: `float`  \n- `subframe_config`  \n  Configuration of the subframe format.  \n  Type: `SubframeConfig`  \n- `t_c`  \n  System Sampling time.  \n  Type: `float`  \n  Read only: `True`  \n- `ue_configs`  \n  Dictionary of configured UEs.  \n  Type: `UeConfig`  \n- `num_resource_blocks`  \n  Number of configured resource blocks.  \n  Type: `int`  \n  Read only: `True`  \n- `slot_number`  \n  System slot number $n_\\text{s}`.  \n  Type: `int`  \n- `slot_number_offset`  \n  Slot number offset used for DMRS set 1.  \n  Type: `int`  \n- `tpcf_config`  \n  TPC commands configuration.  \n  Type: `TPCFConfig`  \n- `transform_precoding_config`  \n  Transform precoding configuration.  \n  Type: `str` (\"off\", \"on\")  \n- `transform_precoding`  \n  Indicator if transform precoding is used.  \n  Type: `bool` (\"on\" is `True`)  \n- `csi_meas_config`  \n  CSI measurements configuration.  \n  Type: `CsimMeasConfig`  \n- `dmrs_config`  \n  DMRS configuration.  \n  Type: `DmrsConfig`  \n- `dmrs_grid`  \n  DMRS configuration for all REs.  \n  Type: `DMRSGrid`  \n- `dmrs_ports`  \n  List of DMRS port as used in the numerology.  \n  Type: `[0,..., 11]`, list of ints  \n- `dmrs_additional_position`  \n  Additional DMRS position relative to \"dmrs_type_a_position\"  for DMRS port 0.  \n  Type: `DmrsAdditionalPosition`  \n- `dmrs_type_a_position`  \n  DMRS position relative to the \"pilot_start\" for DMRS port 0.  \n  Type: `DmrsTypeAPosition`  \n- `num_dmrs_symbols`  \n  Number of DMRS symbols configured for transmission.  \n  Type: `int`  \n- `num_cdm_groups_without_data`  \n  Number of CDM groups without data.  \n  Type: `int`  \n- `num_vdm_groups_without_data`  \n  Number of VDM groups without data.  \n  Type: `int`  \n- `cdm_groups`  \n  Complex value for every PRB and OFDM symbol of all DMRS-carrying resource elements of all DMRS ports\n  for every pilot symbol of all layers for all CDM groups without data. \n  This corresponds to the punctured OFDM channel in e.g. [TR38.901, sec. 6.4.4.2].  \n  Type: `NumTx` $\\times$ `NumCdmGroupsWithoutData` $\\times$ `NumSubcarriers` $\\times$ `NumO` $\\times$ [`NumPilots`], tf.complex  \n- `vdm_groups`  \n  Same as `cdm_groups` but for VDM groups without data.  \n  Type: `NumTx` $\\times$ `NumVdmGroupsWithoutData` $\\times$ `NumSubcarriers` $\\times$ `NumStreamsPerTx` $\\times$ `NumPilots`, tf.complex  \n- `w_f`  \n  List of precoding matrices for all DMRS-carrying resource elements of all DMRS ports\n  for every pilot symbol of all layers for all streams..\n  Type: `NumStreamsPerTx` x [`NumPilots`], tf.complex  \n- `w_f`  \n  List of precoding matrices for all DMRS-carrying resource elements of all DMRS ports\n  for every pilot symbol of all layers for all streams..\n  Type: `NumStreamsPerTx` x [`NumPilots`], tf.complex  \n- `w_f`  \n  List of precoding matrices for all DMRS-carrying resource elements of all DMRS ports\n  for every pilot symbol of all layers for all streams..\n  Type: `NumStreamsPerTx` x [`NumPilots`], tf.complex  \n- `w_f`  \n  List of precoding matrices for all DMRS-carrying resource elements of all DMRS ports\n  for every pilot symbol of all layers for all streams..\n  Type: `NumStreamsPerTx` x [`NumPilots`], tf.complex  \n- `w_f`  \n  List of precoding matrices for all DMRS-carrying resource elements of all DMRS ports\n  for every pilot symbol of all layers for all streams..\n  Type: `NumStreamsPerTx` x [`NumPilots`], tf.complex  \n- `w_f`  \n  List of precoding matrices for all DMRS-carrying resource elements of all DMRS ports\n  for every pilot symbol of all layers for all streams..\n  Type: `NumStreamsPerTx` x [`NumPilots`], tf.complex  \n- `ze_id`  \n  Zoned resource element mapper for inter-layer DMRS.  \n  Type: `ZeResourceGrid`  \n- `ze_id`  \n  Zoned resource element mapper for all DMRS-related channels.  \n  Type: `ZeResourceGrid`  \n\n**Configurable PDCCH**  \nThe PDCCH is configured through an instance of the PDCCHConfig class.\n\n**Configurable PDSCH**  \nThe PDSCH is configured through an instance of the PDSCHConfig class.\n\n**Read-only Attributes**\nThe following attributes are derived from the primary PDCCH and PDSCH, and are therefore read-only.\n\n- `data_carrying_resource`  \n  Indicates the resource elements that carry data.  \n  Type: `tf.complex`, read-only  \n- `data_carrying_resource_mask`  \n  Masked version of `data_carrying_resource`.  \n  Type: `tf.complex`, read-only  \n- `dmrs_grid`  \n  DMRS grid.  \n  Type: `DMRSGrid`, read-only  \n- `pdcch`  \n  Configurable instance of PDCCH.  \n  Type: `PDCCH`, read-only  \n- `pdsch`  \n  Configurable instance of PDSCH.  \n  Type: `PDSCH`, read-only  \n- `pilots`  \n  List of pilot sequences for all transmitters and streams.  \n  Type: `PilotPattern`, read-only  \n- `tx_kappa`  \n  List of TX precoding factor.  \n  Type: `float`, read-only  \n- `w_f`  \n  List of precoding matrices for all streams.  \n  Type: `tf.complex`, read-only  \n- `ze_id`  \n  Zoned resource element mapper for precoding.  \n  Type: `ZeResourceGrid`, read-only  \n\n**Methods**\n\n- `bandwidth()`\n  Get the channel bandwidth in Hz.  \n  Returns: `float` \u2013 Channel bandwidth [Hz]\n\n- `duration()`\n  Subframe duration.  \n  Returns: `1e4*`, float \u2013 Subframe duration [ms]\n\n- `num_cyclic_prefixes()`\n  Number of cyclic prefixes.  \n  Returns: `int` \u2013 Number of cyclic prefixes\n\n- `sampling_frequency()`\n  Get the OFDM symbol duration in Hz.  \n  Returns: `float` \u2013 OFDM symbol duration [Hz]\n  "
"Here is the definition of Receiver: sionna.Receiver(receiver, transmitters, channel, codec=None, return_nli=False, bipolar_input=True, llr_max=2.5, dtype=tf.complex64, **kwargs)  \n  \nThe link of the source code of Receiver is [sionna.Receiver](https://nvlabs.github.io/sionna/_modules/sionna/receiver.html#Receiver)  \n  \n```python\n#\n# SPDX-2.0 License\n# Copyright 2021-2024 Sionna Project Authors\n# Copyright 2021-2024 NVIDIA Corporation. All rights reserved.\n#\n# This source code is licensed under the  SPDX-2.0 License.\n# https://opensource.org/licenses/Simple-2.0\n#\n\"\"\"\nThis module defines a receiver layer\n\"\"\"\n\nimport numpy as np\nimport tensorflow as tf\nimport sionna\n\n\nclass Receiver(tf.keras.layers.Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    Receiver(receptor, transmitters, channel, codec=None, return_nli=False, bipolar_input=True, llr_max=2.5, dtype=tf.complex64, **kwargs)\n\n    Neural network based receiver, which includes\n    1) a :class:`~sionna.channel.SplittingAndDemapping` layer,\n    2) an optional custom trainable decoder (``codec``).\n\n    The layer computes log-likelihood ratios (LLRs) or hard-decisions (if\n    ``return_llrs`` is `False`) on the bits of a transmitted codeword, from\n    received noisy codeword samples.\n\n    Parameters\n    ----------\n    receiver : str, one of [\"linear\", \"nonlinear1\", \"nonlinear2\"]\n        The demapper to use. Options \"linear\" :class:`~sionna.Receiver`\n        and :class:`~sionna.MLReceiver` are available for custom\n        non-likelihood-based receivers.\n\n    channel : :class:`~sionna.channel AWGN\n        Layer implementing the channel model\n\n    list_decoder : :class:`~sionna.Decoder`\n        An instance of the :class:`~sionna.Decoder` to be\n        considered, such as :class:`~sionna.Polar5GDecoder`.\n        If `None`, no list decoding is applied.\n\n    return_nli : bool\n        If `True`, the noise plus intereference is returned.\n        Defaults to `False`.\n\n    bipolar_input : bool, True\n        Expects channel model to have used a bipolar input. If `False`,\n        the receiver expects soft-symbol LLR inputs. Defaults to `True`.\n\n    llr_max : tf.float\n        The (estimated) maximum value of an LLR, Defaults to 2.5.\n        The LLRs are clipped to this value.\n\n    dtype : One of [tf.complex64, tf.complex128]tf.complex64\n        The dtype of `y`. Defaults to tf.complex64.\n\n    Input\n    -----\n\n    (y, no) or (y, n, i) :\n        Tuple:\n\n    y : [...,n], tf.complex\n        2+D Tensor containing the (noisy) channel output symbols\n\n    no : Tensor of same shape as ``y``, tf.float\n        2+D Tensor containing the (noisy) channel variances\n\n    i : Tensor of same shape as ``y``, tf.float\n        2+D Tensor containing the (noisy) channel interferences\n\n    Output\n    -------\n    x_hat : [...,k], tf.float\n        2+D Tensor containing hard-decided estimations of the ``k`` information\n        bits\n\n    llr : [...,k], tf.float\n        2+D Tensor containing LLR for every ``k`` bit\n\n    nli : [...,k], tf.float\n        2+D Tensor containing the (noisy) channel log-likelihood ratio\n        for the ``k`` bit when ``return_nli`` = `True`.\n        Only returned if ``return_nli`` = `True`.\n    \"\"\"\n\n    def __init__(self,\n                 receiver,\n                 transmitters,\n                 channel,\n                 codec=None,\n                 return_nli=False,\n                 bipolar_input=True,\n                 llr_max=2.5,\n                 dtype=tf.complex64,\n                 **kwargs):\n        super().__init__(trainable=False, dtype=dtype, **kwargs)\n\n        assert dtype in [tf.complex64, tf.complex128], \\\n            \"dtype must be tf.complex64 or tf.complex128\"\n\n        self._rdtype = dtype\n\n        # Set all subcomponents of the layer\n        self._constellation_transmitter = transmitters.constellation\n        self._constellation_receiver = Constellation=sn\n        if encoder is not None:\n            assert encoder is not None\n            enc_t, _, _ = encoder(None)\n            self._encoder = enc_t\n        else:\n            self._encoder = None\n\n        # Set constellation. If not explicitly set, the one of the encoder is used.\n        if constellation is not None:\n            self.constellation = tf.cast(constellation, dtype.real_dtype)\n        else:\n            self.constellation = tf.cast(self._constellation_transmitter,\n                                          dtype.real_dtype)\n\n        if receiver==\"linear\":\n            self._receiver = LinearReceiver(constellation=self.constellation,\n                                             encoder=self._encoder,\n                                             llr_max=llr_max,\n                                             dtype=dtype)\n        elif receiver==\"nonlinear1\":\n            self._receiver = NonlinearReceiver1(constellation=self.constellation,\n                                                encoder=self._encoder,\n                                                llr_max=llr_max,\n                                                dtype=dtype)\n        elif receiver==\"nonlinear2\":\n            self._receiver = NonlinearReceiver2(constellation=self.constellation,\n                                                encoder=self._encoder,\n                                                llr_max=llr_max,\n                                                dtype=dtype)\n\n        self._channel = channel\n        self._return_nli = return_nli\n        self._bipolar_input = bipolar_input\n\n    #########################################\n    # Public methods and properties\n    #########################################\n\n    def call(self, inputs):\n        if self._bipolar_input:\n            y, no = inputs\n        else:\n            y, n, i = inputs\n\n            # Change dtype if necessary\n            y = tf.cast(y, self._rdtype)\n            n = tf.cast(n, self._rdtype)\n            i = tf.cast(i, self._rdtype)\n            no = n + i\n\n        # Reshape y for demapping\n        old_shape = tf.shape(y)\n        new_shape = tf.concat([old_shape[:-1], [-1]], axis=0)\n        y = tf.reshape(y, new_shape)\n\n        # Demap\n        if self._bipolar_input:\n            z = self._receiver(y, no)\n        else:\n            z = self._receiver(y, no, i)\n\n        # If an encoder has been set, use it to map bits to constellation points\n        if self._encoder is not None:\n            # The encoder is also a layer... seems like I need to implement\n            # a proper encoder/decoder to fit the current infrastructure\n\n            # I'll just make a fake encoder for illustration\n\n            # If encoder has been set, directly return BCE\n            x = self._encoder(z)\n            bce = -1 # Just a placeholder, fake encoder currently\n\n        x = sionna.mapping.Demapper(\n                \"app\",\n                self._constellation_transmitter,\n                \"app\",\n                return_bits=True,\n                with_prior=False,\n                dtype=tf.complex64).call(y, z) # call for mutual baviour\n\n        # Reshape LLRs to original shape\n        new_shape = tf.concat([old_shape[:-1], [x.shape[-1]]], axis=0)\n        llr = tf.reshape(x, new_shape)\n\n        # Return also the symbol logits if requested\n        if self._return_nli:\n            n = tf.reshape(no, new_shape)\n            if not self._bipolar_input:\n                i = tf.reshape(i, new_shape)\n            nli = self._get_symbol_logits(z, n, i)\n            return llr, nli\n        else:\n            return llr\n\n    @property\n    def encoder(self):\n        \"\"\"The encoder used for (de)modulation.\n\n        Can be `None`.\n        \"\"\"\n        if self._encoder is None:\n            return None\n        else:\n            return self._encoder\n\n    #########################\n    # Utility methods\n    #########################\n\n    def _get_symbol_logits(self, x, no, i): # pylint: disable=unused-argument\n        # Note: For an AWGN channel, the noise variance is constant and the\n        #       symbol logits equal the LLRs. This is not the case for\n        #       more general channel models. The following method computes\n        #       the symbol logits from the LLRs.\n\n        # The channel is first brought to unit variance\n        # For the AWGN channel this is not necessary but also works.\n        tf.debugging.assert_equal(tf.reduce_mean(no), 1.0)\n        x = x / tf.sqrt(no)\n\n        # Compute symbol logits\n        points = self._constellation_receiver.points #[M]\n        x = tf.reshape(x, [-1, 1])\n        points = tf.reshape(points, [1, -1])\n        d = sionna.metrics.pam_distances(x, points) #[n, M]\n        symbol_logits = sionna.metrics.pam_symbol_logits(x, points, d)\n\n        # Reshape symbol_logits to [...,num_points]\n        old_shape = tf.shape(symbol_logits)\n        new_shape = tf.concat(\n            [old_shape[:-2], points.shape], axis=0)\n        symbol_logits = tf.reshape(symbol_logits, new_shape)\n\n        return symbol_logits\n```  \n  "
"The `ResourceGrid` in the Sionna Python package is used to define a channel as well as to structure the mapping of data streams and precoded data streams to a set of OFDM numerology and resource elements. In 5G NR, the purpose of the `ResourceGrid` is to enable the simulation of a massive number of basestions and UEs, each of which might be active in different slots and subcarriers.\n\nThe `ResourceGrid` is utilized in the `ofdm_modulator` and `ofdm_demodulator` methods of the Sionna Python package to configure the resource grid for the OFDM modulation used in 4G and 5G standards. The grid is configured with the number of OFDM symbols, FFT size, subcarrier spacing, and other parameters that define the resource grid.\n\nThe `ResourceGrid` has key importance in the simulation of wireless communication systems as it directly determines the structure and mapping of data and control channels. By modifying the parameters of the `ResourceGrid`, you can simulate different channel structures and coding schemes to evaluate the performance of the system under different conditions. This is crucial in the design and optimization of communication systems.\n\nIt's important to note that the proper usage of `ResourceGrid` requires understanding of the underlying OFDM modulation and the 4G/5G communication standards. The actual usage of `ResourceGrid` would involve methods like `define_resource_grid()`, `show()`, and accessing the grid properties like `cyclic_prefix` and `num_tx` to simulate data and control channel mappings in an OFDM-based system."
"To generate a batch of frequency responses, you can follow these steps:\n\n1. First, you need to create an OFDM channel. In the given context, an `OFDMChannel` is created using the `GenerateOFDMChannel` function from the `sion.channel` module. This channel model will generate OFDM channel responses.\n\n```python\nchannel = sn.channel.OTABaseline(channel_model = sn.channel_model.CDMA2000(time_step = 1e-6,\n                                                                               subcarrier_spacing = 1.8e3),\n                                 channel_model = \"uplink\")\n```\n\n2. Next, you need to create an instance of the `OFDMChannel` layer. This instance will use the channel model to generate channel responses using the default settings, which includes the use of a `GenerateOFDMChannel` instance.\n\n```python\ncir_to_ofdm_channel = sn.channel.CIRToOFDMChannel(channel_model = \"td\")\n```\n\n3. Now, you can generate the channel responses. The channel responses are generated for the channel impulse response (a,tau) and the channel delay spread (std_dev). The shape of `a` is (batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps), where `num_paths` is the number of paths for the CDMA channel and `num_time_steps` is the number of time steps you want to simulate. The `tau` parameter represents the path delays in seconds.\n\n```python\na = np.ones([batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps])\ntau = np.random.uniform(0.1, 20, [batch_size, num_rx, num_tx, num_paths])\nh = cir_to_ofdm_channel(a, tau, std_dev)\n```\n\nIn this code, `h` will contain the channel responses. The shape of `h` will be (batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols, fft_size)."
"To configure transmitters and receivers for computing propagation paths in Sionna RT, follow the steps outlined below:\n\n1. Set up Scene: Load the scene in which you want to compute the propagation paths. You can use the `load_scene` function from the `sionna.rt` module to do this.\n\n```python\nscene = load_scene(sionna.rt.scene.munich)\n```\n\n2. Configure Antenna Array: Define a PlanarArray to be used by all transmitters and receivers. You can set the number of rows and columns, spacing, pattern, and polarization.\n\n```python\nscene.tx_array = PlanarArray(num_rows=1,\n                             num_cols=1,\n                             vertical_spacing=0.5,\n                             horizontal_spacing=0.5,\n                             pattern=\"tr38901\",\n                             polarization=\"V\")\n```\n\n3. Create and Add Transmitters: Define a Transmitter instance with a unique name, position, and orientation. The `look_at` function can be used to make the transmitter point towards a specific receiver. \n\n```python\ntx = Transmitter(name=\"tx\",\n                 position=[8.5,21,27])\nscene.add(tx)\ntx.look_at([45,90,1.5])\n```\n\n4. Create and Add Receivers: Define a Receiver instance with a unique name, position, and orientation.\n\n```python\nrx = Receiver(name=\"rx\",\n              position=[45,90,1.5],\n              orientation=[0,0,0])\nscene.add(rx)\n```\n\n5. Set Carrier Frequency: Set the carrier frequency of the scene to the desired frequency (in Hz).\n\n```python\nscene.frequency = 2.14e9\n```\n\n6. Compute Paths: Compute the paths between all transmitters and receivers. You can use the `compute_paths` function from the `sionna.rt` module to do this.\n\n```python\npaths = scene.compute_paths()\n```\n\nPlease note that the above code snippets are based on the given context and may need to be adjusted based on your specific use case or scene."
"This is the class definition of the deprecated `SymbolDemapperWithPrior`:\n\n```python\nclass SymbolDemapperWithPrior(SymbolDemapper):\n    # ...\n```\n\nThe `SymbolDemapperWithPrior` class is a part of the `sionna.mapping` module. It has been deprecated in favour of `SymbolDemapper`, which consolidates functionality from both classes. The primary function of `SymbolDemapperWithPrior` is to compute log-likelihood ratios (LLRs) or hard-decisions on bits for a tensor of received symbols. This computation assumes that prior knowledge about the bits is available.\n\nThe class takes the following parameters:\n\n- `constellation_type`: Specifies the type of constellation used, if it's a custom constellation, it allows the user to specify a `Constellation` instance.\n- `num_bits_per_symbol`: The number of bits per constellation symbol, for example, 4 for QAM16. It cannot be specified if `constellation` is provided.\n- `constellation`: An instance of the `Constellation` class, or None. In the latter case, `constellation_type` and `num_bits_per_symbol` must be provided.\n- `hard_out`: A boolean flag that indicates if the layer should output hard-decided bit values instead of soft values. Defaults to False.\n- `dtype`: Specifies the datatype of the input and output; either tf.complex64 or tf.complex128. Defaults to tf.complex64.\n\nIt's important to note that this class has been deprecated, and its usage is discouraged in the provided context. The recommended approach is to use `SymbolDemapper` instead, which encompasses the functionality of both `SymbolDemapper` and `SymbolDemapperWithPrior`. This reduces complexity and aligns with best practices."
"To enable diffuse reflections and observe their impact on the coverage map, you need to make the following adjustments to your scene and then recompute the coverage map:\n\n1. Set the `enable_diffraction` and `enable_diffuse_reflection` attributes of your scene to `True`. These properties can be found in the `Scene` class of the Sionna API, and they control whether diffuse reflections and diffractions are enabled in the scene. \n\n```python\nscene.enable_diffraction = True\nscene.enable_diffuse_reflection = True\n```\n\n2. Recompute the coverage map for your scene. You can use the `coverage_map` method of your `Scene` object to compute the coverage map. You can specify the `cm_cell_size` (the resolution of the coverage map) and `num_samples` (the number of rays to simulate) when calling this method.\n\n```python\ncm = scene.coverage_map(cm_cell_size=[1,1], num_samples=20e6, max_depth=5)\n```\n\n3. Visualize the coverage map. You can use the `render` method of your `Scene` object to render the coverage map. You can specify the `cm_vmin` and `cm_vmax` (the minimum and maximum values of the coverage map) and the `coverage_map` when calling this method.\n\n```python\nscene.render(my_cam, coverage_map=cm, cm_vmin=-200, cm_vmax=-90)\n```\n\nEnabling diffuse reflections allows the coverage map to include not only line-of-sight and specular reflections, but also diffuse reflections. Diffuse reflections occur when a wavefront hits a surface and is scattered in many different directions. These reflections can create \"hotspots\" in the coverage map where the signal strength is significantly higher than the surrounding areas. \n\nBy recomputing the coverage map with diffuse reflections enabled, you should see an increase in the amount of coverage, as the diffuse reflections can reach areas that are not in the line of sight of the transmitter. This can give you a more comprehensive view of the wireless coverage provided by the transmitter."
"To compute the Mean Squared Error (MSE) between the transmitted and recovered symbols, you can follow these steps:\n\n1. First, you need to have the transmitted symbols `x` and the recovered symbols `x_hat`. The transmitted symbols are a part of the context, in the section titled \"Mapping the data into constellation symbols\", these symbols are mapped to the QAM constellation. The recovered symbols `x_hat` are generated using the matched filtering and downsampling process, as described in the context.\n\n2. Next, compute the difference between the transmitted and recovered symbols. This can be done by subtracting the transmitted symbols from the recovered symbols. In Python, if you have two complex vectors `x` and `y`, you can compute the difference vector `z` as `z = x - y`.\n\n3. After computing the difference vector, you can then square each element of this vector to get the squared error for each symbol. In Python, if you have a complex vector `z`, you can compute the squared error vector `e` as `e = np.square(np.abs(x - x_hat))`.\n\n4. Finally, you can compute the MSE by taking the mean of all the elements in the squared error vector. In Python, if you have a complex vector `e`, you can compute the MSE using `np.mean(e)`.\n\nPutting it all together, the Python code to compute the MSE between the transmitted and recovered symbols would look like this:\n\n```python\nimport numpy as np\n\n# Compute the difference vector\nz = x - x_hat\n\n# Compute the squared error vector\ne = np.square(np.abs(z))\n\n# Compute the MSE\nMSE = np.mean(e)\n```\n\nPlease make sure that the symbols `x` and `x_hat` are defined and computed as described in the context. The code provided in the context for generating the QAM symbols and recovering the transmitted symbols should be used as is, without any modifications."
"The 'create_or_check_constellation' method from the 'Constellation' class in the 'sionna.mapping' module is used to ensure that a valid constellation object is returned, either by creating a new one or by checking the consistency and reusing an existing one. Constellation here refers to the pattern of bits that is used to represent each symbol in a digital modulation scheme.\n\nTo achieve this, the method takes three parameters: 'num_bits_per_symbol', 'constellation', and 'dtype'. \n- 'num_bits_per_symbol' is the number of bits per constellation symbol. For example, it would be 4 for QAM16. This parameter is only required if the constellation type is 'qam' or 'pam'. \n- 'constellation' is an instance of the Constellation class. If this parameter is provided, the 'num_bits_per_symbol' parameter is not required.\n- 'dtype' refers to the data type of the constellation. The constellation can be in either dtype=tf.complex64 or tf.complex128. The default value is tf.complex64.\n\nIf 'constellation' is not provided, the method ensures that the constellation type ('constellation_type') and the number of bits per symbol are consistent by creating a new constellation object with the correct settings. This is important because the modulation schemes require precise settings for proper operation. If an invalid or inconsistent constellation object is provided, it could lead to errors in the modulation process. Therefore, this method plays a crucial role in ensuring that the system behaves correctly and efficiently. \n\nIf 'constellation' is provided, the method checks for consistency (i.e., checks if the 'constellation' is of the type specified by 'constellation_type' and has the correct number of bits per symbol). If the check is successful, the provided constellation object is returned. If the check fails, an AssertionError is raised, indicating that the constellation is not consistent with the requirements. This is important for maintaining the integrity of the modulation scheme and ensuring that the symbol mapping and demapping processes work as intended."
"The `scat_keep_prob` argument in the ray tracing process plays a critical role in controlling the number of scattered paths that are considered during the ray tracing process. This argument is used when computing scattered paths, as demonstrated in the context with the Sionna package for wireless simulation using ray tracing.\n\nThe `scat_keep_prob` argument is a floating-point number, typically ranging between 0 and 1. It represents the probability that a scattered path is retained. In other words, the argument is used to drop a certain fraction of scattered paths to manage the total number of paths considered in the simulation. A higher value for `scat_keep_prob` means that more paths will be retained, while a lower value means that a lesser fraction of paths will be considered.\n\nThe primary motivation for the `scat_keep_prob` argument is to deal with the potentially large number of scattered rays in the ray tracing process, which can result in a large workload and memory usage. By adjusting the probability in the `scat_keep_prob` argument, the simulation can be tuned to balance between accuracy and efficiency. Specifically, by reducing the value of `scat_keep_prob`, the number of scattered paths is decreased, which can lead to a more efficient simulation, with minimal impact on the quality of the results.\n\nIn the context of the Sionna package for wireless simulation, the `scat_keep_prob` argument is used in the `compute_paths` function, which is part of the ray tracing process. The argument is set to a specific value, and the total number of paths is reported after the ray tracing process is completed. This allows a user to control the number of scattered paths and tailor the simulation to their specific needs."
"Here is the detailed information of sionna.rt.r_hat: [sionna.rt.r_hat(a, b, dtype=tf.complex64)](https://nvlabs.github.io/sionna/_modules/sionna/rt/utils.html#r_hat)\n\nComputes the unit normal vector $\\mathbf{\\hat{r}}$ in spherical coordinates corresponding to directions $\\mathbf{a}$ and $\\mathbf{b}$, and the angles $\\phi$ and $\\theta$ defined as in [(3)](https://nvlabs.github.io/sionna/em_primer.html#equation-spherical) .\n\n**Input**\n\n- `a` ([...,3], tf.float): First 3D vector.\n- `b` ([...,3], tf.float): Second 3D vector.\n- `dtype` (tf.complex64 or tf.complex128): Data type used for all computations. Defaults to tf.complex64.\n\n**Output**\n\n- `[...,3]`, tf.float: Vector $\\mathbf{\\hat{r}}$.\n- `[...,], tf.float: Azimuth angle $\\phi$, [0, 2$\\pi$].\n- `[...,], tf.float: Zenith angle $\\theta$, [0,$\\pi$].\n\nsource code:\n```python\ndef r_hat(a, b, dtype=tf.complex64):\n    r\"\"\"\n    Computes the unit normal vector :math:`\\mathbf{\\hat{r}}` in spherical\n    coordinates corresponding to directions :math:`\\mathbf{a}` and\n    :math:`\\mathbf{b}`, and the angles :math:`\\phi` and :math:`\\theta`\n    defined as in :eq:`spherical` .\n\n    Input\n    ------\n    a : [...,3], tf.float\n        First 3D vector\n\n    b : [...,3], tf.float\n        Second 3D vector\n\n    dtype : tf.complex64 or tf.complex128\n        Datatype used for all computations.\n        Defaults to `tf.complex64`.\n\n    Output\n    -------\n    : [...,3], tf.float\n        Vector :math:`\\mathbf{\\hat{r}}`\n\n    : [...], tf.float\n        Azimuth angle :math:`\\phi`, [0, 2\\pi].\n\n    : [...], tf.float\n        Zenith angle :math:`\\theta`, [0,pi].\n    \"\"\"\n    a = rotate_towards_z(a, dtype=dtype)\n    theta = tf.acos(a[...,2])\n    r_hat_theta = tf.stack([tf.sin(theta), tf.zeros_like(theta),\n                            tf.cos(theta)], -1)\n    b = rotate_towards_z(b, dtype=dtype)\n    c = cos_angle(a, b)\n    phi = tf.acos(c)\n\n    # Determine the sign of the phi\n    sgn = tf.sign(a[..., 1]*b[..., 0] - a[..., 0]*b[..., 1])\n    phi = phi*sgn + (1-sgn*sgn)/2*tf.constant(2*math.pi, phi.dtype)\n\n    # Compute r_hat\n    r_hat = tf.cos(theta)*r_hat_theta + tf.sin(theta)*tf.\n    where(phi[...,None], r_hat_theta)[0]\n    return r_hat, phi, theta\n```"
"Here is the detailed information:   \n  \n[sionna.ofdm.OFDMDetectorWithPrior(detector, output, resource_grid, stream_management, constellation_type=None, num_bits_per_symbol=None, constellation=None, dtype=tf.complex64, **kwargs)](https://nvlabs.github.io/sionna/_modules/sionna/ofdm/detection.html#OFDMDetectorWithPrior)  \n\nWraps a MIMO detector that expects prior information for use with the OFDM waveform.\n\nThe parameter detector is a callable (e.g., a function) that implements a MIMO detection algorithm with prior for arbitrary batch dimensions. It either the existing detector MaximumLikelihoodDetectorWithPrior or a custom detector that has the same input/output specification. For the latter, the parameters constellation_type, num_bits_per_symbol, constellation must be provided.\n\nThis class pre-processes the received resource grid y, channel estimate h_hat, and the prior information prior, and computes for each receiver the interference-plus-noise covariance matrix according to the OFDM and stream configuration provided by the resource_grid and stream_management. These quantities serve as input to the detection algorithm that is implemented by the callable detector.\n\nBoth detection of symbols or bits with either soft- or hard-decisions are supported.\n\n**Note**\nThe callable detector must take as input a tuple $(\\mathbf{y}, \\mathbf{h}, \\mathbf{prior}, \\mathbf{s})$ such that:\n\n- y ([,num_rx], or [,num_rx, num_streams] tf.complex) \u2013 1D or 2D tensor containing the received signals\n\n- h ([,num_rx, num_streams_per_rx], or [,num_rx, num_streams_per_rx_per_rx] tf.complex) \u2013 2D or 3D tensor containing the channel matrices\n\n- prior ([,num_streams x n], or [,num_streams, num_points] tf.float) \u2013 Prior for the transmitted signals. If output equals \"bit\", then LLRs for the transmitted bits are expected. If output equals \"symbol\", then logits for the constellation points are expected.\n\n- s ([,num_rx, num_rx], tf.complex) \u2013 2D tensor containing the noise-plus-interference covariance matrix\n\nIt must generate one of the following outputs depending on the value of output:\n\n- b_hat ([, num_streams, n*num_bits_per_symbol], tf.float) \u2013 LLRs or hard-decisions for every bit of every stream, if output equals \"bit\".\n\n- s_hat ([, num_streams, n, num_points], tf.float) or ([, num_streams, n], tf.int) \u2013 Logits or hard-decisions for constellation symbols for every stream. Hard-decisions correspond to the symbol indices. This output is only required if output is \"symbol\".\n\nParameters\n- detector (Callable):  Callable object (e.g., a function) that implements a MIMO detection algorithm with prior for arbitrary batch dimensions. Existing detectors such as MaximumLikelihoodDetectorWithPrior can be used, or a custom detector callable provided that has the same input/output specification.\n- output (One of [\"bit\", \"symbol\"], str): The type of output, either bit or symbol.\n- resource_grid (ResourceGrid): Instance of ResourceGrid.\n- stream_management (StreamManagement): Instance of StreamManagement.\n- constellation_type (One of [\"qam\", \"pam\", \"custom\"], str): For custom constellations, an instance of Constellation must be provided.\n- num_bits_per_symbol (int): The number of bits per constellation symbol, required for constellation_type in [\"qam\", \"pam\"].\n- constellation (Constellation): An instance of Constellation, or None. In the latter case, both constellation_type and num_bits_per_symbol must be provided.\n- dtype (One of [tf.complex64, tf.complex128] tf.DType): The dtype of h_hat. Must match the precision of the channel estimates. Defaults to tf.complex64."
"  \n7  \n0  \n  \n[Here is the detailed source code of sionna.channel.GaussianPriorSource](https://nvlabs.github.io/sionna/_modules/sionna/channel/utils.html#GaussianPriorSource).  \n  \n```python\nclass GaussianPriorSource:\n    r\"\"\"Class for generating samples from a multi-dimensional Gaussian prior.\n    For a given batch size, it returns a matrix of shape\n    ``[batch size, num_tx, num_ant, num_samples]``, filled with\n    samples from a multi-dimensional Gaussian distribution.\n\n    The ``[batch size, num_tx, num_ant, num_samples]`` shape is often\n    inconvenient when the samples are used to filter a channel\n    impulse response of shape ``[batch size, num_tx, num_ant, num_rx, num_paths, num_samples]``.\n    In this case, one typically wants to reshape the tensor to\n    ``[batch size, num_tx, num_ant, num_samples]``. This can be done using\n    the ``tf.transpose`` function.\n\n    Parameters\n    ----------\n    num_tx : int\n        Number of transmitters (equal to number of columns in the W\n        covariance matrix).\n\n    num_ant : int\n        Number of antenna per transmitter (equal to number of rows in the\n        W covariance matrix).\n\n    mean : [..., num_tx, num_ant]\n        Mean of the Gaussian prior for the transmitted signals.\n        The ``[..., num_tx, num_ant]`` part can be broadcastable.\n\n    cov : [..., num_tx, num_ant, num_tx, num_ant]\n        Covariance matrix of the Gaussian prior for the transmitted signals.\n        The ``[..., num_tx, num_ant, num_tx, num_ant]`` part\n        should be broadcastable. If ``cov`` is rank-one, i.e.,\n        ``[..., num_tx, num_ant, num_tx, num_ant, 1]``, then ``cov`` is interpreted\n        as a diagonal matrix.\n\n    batch_shape : list, int\n        Batch dimensions. If `None`, no extra batch dimensions are used.\n\n    dtype : tf.DType\n        Datatype for internal calculations and the output dtype.\n        Defaults to `tf.complex64`.\n\n    Output\n    ------\n    : [..., batch size, num_tx, num_ant, num_samples]\n        Tensor filled with samples from the multi-dimensional Gaussian\n        distribution.\n\n    Note\n    ----\n    One can create multiple instances of this class, which will be\n    automatically be added to the graph as non-blocking operations.  \n\n    Sample Use:\n    The following code snippet shows how to use this class as a\n    multi-dimensional Gaussian prior for generating channel\n    impulse responses.\n\n    ```python\n    # h = [batch size, num_tx, num_ant, num_rx, num_paths, num_samples]\n    num_tx = 8\n    num_ant = 4\n    num_paths = 20\n    num_s = 10\n    dtype = tf.complex128\n\n    # define the mean and covariance\n    mean = tf.zeros([num_tx, num_ant], dtype=dtype)\n    cov   = tf.constant(0.5+0.5j, shape=[num_tx, num_ant,num_tx, num_ant], dtype=dtype)\n\n    # 1: Create GaussianPriorSource\n    gps = GaussianPriorSource(num_tx, num_ant, mean, cov, dtype=dtype)\n\n    # 2: Generate Data\n    batch_size = 1024\n    s = tf.stack([gps( batch_size) for _ in range(num_s)], axis=4)\n    tf.shape(s)\n    # tf.TensorShape([batch_size, num_tx, num_ant, 20, num_s])\n\n    ```\n\n    \"\"\"\n\n    def __init__(self,\n                 num_tx,\n                 num_ant,\n                 mean,\n                 cov,\n                 batch_shape=None,\n                 dtype=tf.complex64):\n        super().__init__()\n\n        if batch_shape is None:\n            batch_shape = []\n        self._num_tx = num_tx\n        self._num_ant = num_ant\n        self._batch_shape = batch_shape\n        self._dtype = dtype\n\n        # We need these to be able to sample even if the user only wants to\n        # sample one value\n        mean = tf.expand_dims(mean, -1)\n        if cov.shape.ndims == 3:\n            cov = tf.expand_dims(tf.expand_dims(cov, -1), -1)\n        elif cov.shape.ndims == 2:\n            cov = tf.linalg.diag(tf.expand_dims(cov, -1), -1)\n        self._dist = tfp.distributions.MultivariateNormalFullCovariance(\n                                                        mean=mean, cov=cov)\n\n    def __call__(self, batch_size):\n        # sample will also add new dims if the user wants to sample only one\n        # value\n        samples = self._dist.sample([batch_size])\n        if self._batch_shape == []:\n            return tf.squeeze(samples, axis=-1)\n        else:\n            return tf.squeeze(samples, axis=-1).numpy()\n```  \n  "
"Here is the detailed information:   \n  \n[sionna.mapping.QAM2PAM(num_bits_per_symbol)](https://nvlabs.github.io/sionna/_modules/sionna/mapping.html#QAM2PAM)  \nTransforms QAM symbol indices to PAM symbol indices.\n\nFor indices in a QAM constellation, computes the corresponding indices for the two PAM constellations corresponding the real and imaginary components of the QAM constellation.\n\n**Parameters**\n\n- `num_bits_per_symbol` (int): The number of bits per QAM constellation symbol, e.g., 4 for QAM16.\n\n**Input**\n\n- `ind_qam` (Tensor, tf.int): Indices in the QAM constellation\n\n**Output**\n\n- `ind_pam1` (Tensor, tf.int): Indices for the first component of the corresponding PAM modulation\n- `ind_pam2` (Tensor, tf.int): Indices for the first component of the corresponding PAM modulation\n\n4G/5G Example\nind_qam = [1,5,7,11] for 16QAM (num_bits_per_symbol = 4)\n\nind_pam1 = [0,1,1,2]\nind_pam2 = [0,2,2,3]\n\n**Notes**\nThe indices are computed following the equation ind_pam = 2*ind_qam + ind_pam_offset . This method is as it, as the computation above does not hold in general.\n\nsource code:  \n```python\nclass QAM2PAM:\n    r\"\"\"Transforms QAM symbol indices to PAM symbol indices.\n\n    For indices in a QAM constellation, computes the corresponding\n    indices for the two PAM constellations corresponding the real and\n    imaginary components of the QAM constellation.\n\n    Parameters\n    ----------\n    num_bits_per_symbol : int\n        The number of bits per QAM constellation symbol, e.g., 4 for QAM16.\n\n    Input\n    ------\n    ind_qam : Tensor, tf.int\n        Indices in the QAM constellation\n\n    Output\n    -------\n    ind_pam1 : Tensor, tf.int\n        Indices for the first component of the corresponding PAM modulation\n\n    ind_pam2 : Tensor, tf.int\n        Indices for the first component of the corresponding PAM modulation\n\n    ind_max : int, [0, 1]\n        The maximum index of PAM constellation symbols. Two PAM\n        constellations share the same indices. Defaults to None\n        (i.e., assumes `num_bits_per_symbol` PAM symbols.)\n\n    Notes\n    -----\n    The indices are computed following the equation\n\n    :math:`ind\\_pam = 2*ind\\_qam + ind\\_pam\\_offset`.\n\n    This method is as it, as the computation above does not hold in general.\n    \"\"\"\n    def __init__(self,\n                 num_bits_per_symbol):\n        super().__init__()\n        assert num_bits_per_symbol in QAM2_PAM_OFFSETS, \\\n            \"Unsupported number of bits per symbol.\"\n        self._num_bits_per_symbol = num_bits_per_symbol\n\n    @property\n    def num_pam_symbols(self):\n        \"\"\"int: Number of (shared) symbols of the PAM constellations\"\"\"\n        return 2**self._num_bits_per_symbol\n\n    @property\n    def ind_pam_offset(self):\n        \"\"\"2-tuple of int: Offset for the indices in the PAM constellation\"\"\"\n        return QAM2_PAM_OFFSETS[self._num_bits_per_symbol]\n\n    def __call__(self, ind_qam):\n\n        ind_pam = self.num_pam_symbols*tf.cast(ind_qam, tf.int32)\n        ind_pam1 = ind_pam//2 + self.ind_pam_offset[0]\n        ind_pam2 = ind_pam//2 + self.ind_pam_offset[1]\n\n        return ind_pam1, ind_pam2\n```\n  "
"To load the required Sionna components, you will need to import the necessary modules. Below is a Python code snippet demonstrating this:\n\n```python\n# Load the signal module from Sionna\nfrom sionna.signal import QAM, PAM, QAMSource\n\n# Example to use Sionna to generate Square, 5-level PAM symbols \nnum_bits_per_symbol = 2\nPAM1 = PAM(num_bits_per_symbol) # Modulation scheme\nsource1 = PAMSource(num_bits_per_symbol) # Symbol source\nsource1([batch_size, num_symbols_per_burst]) # Generate random symbols\n```\n\nIn the code snippet, `QAM`, `PAM`, and `QAMSource` are components of the `sionna.signal` module that are being imported. This is done using Python's `import` statement. The symbols are modulation schemes, and `SymbolSource` is used to generate random symbols. These components are generally required when working with digital signal processing where modulation and demodulation of signals is needed. \n\nPlease note that the actual components you need will depend on the specific requirements of your work. The Sionna framework offers a wide range of components for simulating the PHY-layer link-level performance for many diverse 5G-compliant components. This includes various signal processing, channel models, and modulation techniques, among others."
"The provided code shows how to import the necessary modules for Sionna's API and set up the GPU configuration. Here's a breakdown of the steps:\n\n1. First, the code attempts to import the Sionna module. If it's not already installed, the code will install it using pip.\n\n```python\ntry:\n    import sionna\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna\n```\n\n2. The code then proceeds to import the required Python modules such as os, tensorflow, and matplotlib. It also tries to set the TensorFlow logging level to 'ERROR' to avoid warnings.\n\n3. The code then sets the GPU number to be used and the environment variables `CUDA_VISIBLE_DEVICES` and `TF_CPP_MIN_LOG_LEVEL`. The `CUDA_VISIBLE_DEVICES` environment variable is used to specify the GPU number when the system has multiple GPUs. Setting `gpu_num` to 0 uses the CPU. The `TF_CPP_MIN_LOG_LEVEL` environment variable is used to control TensorFlow logging.\n\n4. The code then imports the necessary Sionna components. These include various modules from Sionna's MIMO, OFDM, channel, LDPC encoding, LDPC decoding, mapping, demapping, binary source, ebnodb2no, sim_ber, and compute_ber. The specific modules imported are based on the requirements of the simulation being performed.\n\nPlease note that the code assumes that the necessary drivers for TensorFlow are already installed on the system and the GPUs are available. If the GPUs are not available, the code will run on the CPU."
"A 'CoverageMap' in the context of ray tracing with Sionna refers to a class that stores, for every propagation model and every receiver, the field strength on a grid within a specified region of the scene. It is essentially a detailed map of the signal strength in a given area, showing how that signal propagates from the transmitters or emitters present in the scene.\n\nThe 'CoverageMap' class can be used in combination with other classes such as 'Scene', 'Transmitter', and 'Receiver' to carry out tasks like computing propagation paths, generating channel impulse responses, and visualizing the wireless propagation environment.\n\nHere's an example of how to generate a coverage map in the context of Sionna API:\n\n```python\nscene = load_scene(...)  # Load a pre-made scene\nscene.coverage_map(     # Generate a coverage map\n    \"my_coverage_map\",  # Name of the coverage map\n    \"my_propagation_model\",  # Propagation model to use\n    1e6,              # Region is [-0.5*10, : ,0.5*10] in X and Y, and from 0 to 20 in Z\n    0.5,              # Grid spacing\n    [-7, 7],           # Transmitter grid is [-7,7]\n    0.5,              # Transmiter receiver spacing\n    True)             # LOS is True\n```\n\nThis code will generate a coverage map named \"my_coverage_map\" using the \"my_propagation_model\" propagation model, with an area of coverage of 1 square kilometer, a grid spacing of 0.5, transmitters located on a a 14x14 grid with a spacing of 0.5, and a line-of-sight (LOS) condition set to True.\n\nOnce computed, the coverage map can be visualized and analyzed to gain insight about the radio wave propagation in the scene. For instance, one can query the coverage map to obtain the field strength at a specific location, or one could use the coverage map to compute and visualize the diffraction paths between a transmitter and a receiver.\n\nNote: The code snippet provided is for demonstration purposes only. The actual usage may vary based on the specifics of the scene and the propagation model. Always refer to the official Sionna documentation and perform necessary validations when using the API."
"Input Parameters of TurboDecoder:\n\nThe TurboDecoder() takes log-likelihood ratio values (LLRs) as input. The decoder is based on the BCJR algorithm and takes two different types of inputs based on the `hard_out` parameter:\n\n1. For hard decision decoding:  \n   The input LLRs values must be signs of the log-likelihood ratios, denoted as $\\pm1$ , for each bit or ternary symbol of the codeword.  The input must be a tf.float32 tensor of arbitrary shape, but the last dimension must be compatible with the 'turbo code context', i.e., must have a size which is a multiple of the number of output symbols (or bits) per codeword.\n\n2. For soft decision decoding:  \n   Soft decision decoding is performed by providing log-likelihood ratios (LLRs) for each bit or ternary symbol. The input must be a tf.float32 tensor of arbitrary shape, but the last dimension must be compatible with the 'turbo code context', i.e., must have a size which is a multiple of the number of output symbols (or bits) per codeword.\n\nOutput Formats:\n\nThe TurboDecoder() output is controlled by the `output` parameter:\n\n1. For hard decision decoding:  \n   The decoder outputs a hard decision vector, represented as a `tf.int32` tensor, the last dimension being output for each input codeword.\n\n2. For soft decision decoding:  \n   The decoder, if `output` equals `\"app\"`, outputs the APP decoded vector as a `tf.float32` tensor, the last dimension being output for each input codeword.\n\nThe output tensors can be in any shape, but the dimensions within the output tensor must correspond to the code-specific parameters, such as the code rate and constraint length. The specific output format is determined by the `output` parameter."
"Input\n\n- Zenith angles wrapped in a tensor\n- Azimuth angles wrapped in a tensor\n\nOutput\n\n- Tuple of zenith pattern and azimuth pattern\n  - Both have the same shape and type, indicating the corresponding pattern\n\n[source code](https://nvlabs.github.io/sionna/_modules/sionna/rt/antenna.html#iso_pattern):\nsource code:\n```python\ndef iso_pattern(zenith, azimuth):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    Antenna pattern from :eq:7  for an ideal isotropic antenna (no directive gain)\n\n    The isotropic antenna is not implemented in this function but\n    in the :class:`~sionna.rt.Antenna` class. This function is provided\n    as an example of how an antenna pattern can be implemented and\n    used with the classes defined in this module.\n\n    This function returns the the following tuple:\n\n    1. Zenith pattern\n    2. Azimuth pattern\n\n    Input\n    -----\n     zenith: Wrapped zenith angles [rad]\n        Tensor of any shape.\n     azimuth: Wrapped azimuth angles [rad]\n        Tensor of the same shape as ``zenith``.\n\n    Output\n    ------\n     : Wrapped zenith pattern of the same shape as ``zenith``.\n     : Wrapped azimuth pattern of the same shape as ``zenith``.\n\n    ..  figure:: ../figures/iso_pattern.pdf\n        :align: center\n    \"\"\"\n    return (tf.ones_like(zenith),\n            tf.ones_like(azimuth))\n```"
"Firstly, you have to initiate a model. Here, I'll an OFDM Waveform estimator as an example:\n\n```python\nclass OFDMChannelEstimator(Algorithm): \n# This estimator relies on the LS channel estimation and the interpolation\n# is done according to the nearest base station assumption    \n\n    def __init__(self, **kwargs):\n        # Algorithm is a callable object in this case, but we still define\n        # a class to keep the same framework.\n        super().__init__(**kwargs)\n        self.ls_estimator = LeastSquaresFrequencyChannelEstimator()\n        self.ofdm_demodulator = OFDMDemodulator()\n        self.lmse_interpolator = LMSEInterpolator()\n        self.remove_pilot_subcarriers = RemovePilotSubcarriers()\n\n    def init_time_channel(self, input_ofdm_samples, input_pilot_pattern):\n        # Reshape the pilot pattern to [num_tx, num_bs, num_ofdm_symbols, fft_size//2]\n        input_pilot_pattern = tf.reshape(input_pilot_pattern,\n                                         [input_pilot_pattern.shape[0],\n                                          input_pilot_pattern.shape[1],\n                                          -1, self.fft_size//2])\n\n        # Compute LS channel estimates for all resource elements\n        channel_freq = self.ls_estimator(input_ofdm_samples)\n\n        # Interpolate the channel in the time domain\n        channel_time = self.lmse_interpolator(input_pilot_pattern, channel_freq)\n\n        # Remove pilot subcarriers. Reshape the channel to [num_tx, num_bs, num_ofdm_symbols, fft_size]\n        # The channel has the shape [num_tx, num_bs, num_ofdm_symbols, fft_size//2, num_streams_per_tx]\n        channel_time = self.remove_pilot_subcarriers(input_ofdm_samples, input_pilot_pattern, channel_time)\n\n        # Compute the frequency covariance\n        freq_cov = tf.reduce_sum(tf.reduce_sum(tf.abs(channel_freq)**2, axis=1, keepdims=True), axis=-1, keepdims=True)\n\n        # Compute time covariance\n        time_cov = tf.matmul(channel_time, tf.transpose(channel_time, [0,1,3,2,4]))\n\n        return channel_time, time_cov + tf.complex(0.0, 0.0)\n\n    @tf.function\n    def call(self, inputs):\n        input_ofdm_samples, input_pilot_pattern, no = inputs\n        #input_pilot_pattern has the shape of [num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols, num_effective_subcarriers]\n        # The number of effective subcarriers can be different for the Rxs and Txs due to the guard carriers.\n\n        # Transpose input_ofdm_samples from [num_tx, num_tx_ant, num_ofdm_symbols, fft_size] to\n        # [num_rx, num_rx_ant, num_ofdm_symbols, fft_size] \n        input_ofdm_samples = tf.transpose(input_ofdm_samples, [1,2,0,3])\n\n        # Channel estimates for all transmitters  from all the received signals\n        # The channel estimate from transmitter tx for receiver rx has the\n        # shape [num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols, fft_size//2]\n        channel_time, cov_time = self.init_time_channel(input_ofdm_samples, input_pilot_pattern)\n\n        # Compute LS estimates of the channel frequency response\n        # from all transmitters for the ant to ant channel rx_ant of the rx\n        # The channel estimate has the shape [num_tx, num_tx_ant, num_ofdm_symbols, fft_size//2]\n        channel_freq = self.ls_estimator(input_ofdm_samples)\n        # The channel frequency covariance from all transmitters for the rx_ant\n        # has the shape [num_tx, num_tx_ant, num_ofdm_symbols, fft_size//2, num_streams_per_tx]\n        # We sum over num_streams_per_tx to get the frequency covariance\n        freq_cov = tf.reduce_sum(tf.abs(channel_freq)**2, axis=-1, keepdims=True)\n\n        # Compute time covariance\n        # The time covariance from all transmitters for the rx_ant\n        # has the shape [num_tx, num_tx_ant, num_ofdm_symbols, fft_size, fft_size]\n        time_cov = tf.reduce_sum(tf.abs(channel_time)**2, axis=-1)\n        time_cov = insert_dims(time_cov, 2, 1, axis=-1)\n\n        # Add the two covariances\n        # Both covariances have the shape [num_rx, num_rx_ant, num_tx, num_tx_ant, num_ofdm_symbols, fft_size, fft_size]\n        # We need to sum over  all transmitters tx and their respective antenna\n        # for the wanted and interference transmitters.\n        cov_sum = freq_cov + time_cov\n        # We also need to add the thermal noise variance\n        no = tf.cast(no, dtype=channel_freq.dtype) # no has now the broadcastable shape of a channel symbol\n        no = insert_dims(no, 2, 1, axis=-1) # insert a dimension after n (number of PRS symbols) for the simulation of n channel symbols\n        cov_n = tf.linalg.diag(no) # Thermal noise is modelled as white and diagonal\n\n        # Apply the channel to the vector of all 1s\n        x = tf.ones([input_ofdm_samples.shape[0], 1, 1, 1, 1, self.num_effective_subcarriers], dtype=channel_freq.dtype)\n        # The channel output for all transmitters has the shape\n        # [num_rx, num_rx_ant, num_ofdm_symbols, fft_size]\n        y = channel_output = compute_ofdm_channel(input_ofdm_samples,  x, channel_time, cov_sum, cov_n, no=self._no, dtype=self.dtype)\n\n        # Transpose y from [num_rx, num_rx_ant, num_ofdm_symbols, fft_size] to\n        # [num_tx, num_tx_ant, num_ofdm_symbols,fft_size].\n        # This is required for the creation of the precoding matrices.\n        y = tf.transpose(y, [2, 0, 1, 3])\n\n        # Transpose input_ofdm_samples from [num_tx, num_tx_ant, num_ofdm_symbols, fft_size] to\n        # [num_rx, num_rx_ant, num_ofdm_symbols, fft_size].\n        input_ofdm_samples = tf.transpose(input_ofdm_samples, [1,2,0,3])\n\n        # Compute the effective channel by scattering\n        h, y = scatter_to_rx_tx_antenna(  channel_output=y,\n                                            transmitter_output=input_ofdm_samples,\n                                            num_rx_ant=self.num_rx_ant,\n                                            num_tx_ant=self.num_tx_ant)\n\n        # Compute the precoding matrices\n        g = compute_precoding_matrix(  h_freq=h,\n                                        resource_grid=input_pilot_pattern,\n                                        zf_precoding=False,\n                                        stream_management=input_pilot_pattern.stream_management)\n\n        # Compute the precoded vector\n        x_precoded = tf.squeeze(tf.matmul(g, x), axis=-1) # g is broadcastable, so we can multiply with x.\n\n        # Remove the pilot subcarriers from the precoded vector\n        output = RemoveNones()(x_precoded) # Equivalent to x_precoded.flatten() at the pilot positions\n\n        return output\n```\n\nThen, you can use the layers \"ApplyTimeChannel\" and \"ApplyOFDMChannel\" as follows:\n\n```python\n# Parameters \nnum_ofdm_symbols = 14 # 14 OFDM symbols in a slot\nfft_size = 2048     # FFT size\nsubcarrier_spacing = 30e3 # 30 kHz\nnum_tx = 1\nnum_tx_ant = 8\nnum_rx = 3\nnum_rx_ant = 2\n\n# Resource grid and pilot pattern\npilot_pattern = PilotPattern(num_tx, num_tx_ant, num_ofdm_symbols//2, fft_size, num_ofdm_symbols, \"kronecker\", \"no_doubles\", True)\nofdm_resource_grid = ResourceGrid(fft_size, num_ofdm_symbols, pilot_pattern, \"ones\")\nrg = RemovePilotSubcarriers(remove_pilot_pattern=pilot_pattern)\nresource_grid = rg(ofdm_resource_grid)\n\n# Channel Model\nchannel_model = UMi(   \"LOS\",      # Scenario\n            3,          # Carrier frequency [GHz]\n            30,         # System bandwidth [MHz]\n            \"D\",        # Direction\n            0,          # Number of non double sied subcarriers\n            0.15        # Active user ratio\n        )\n\n# Generate channel impulse response\ncir = channel_model( num_rx_per_rx!= ) # cir has the shape of [num_rx, num_rx_per_rx, num_tx, num_tx_per_tx, num_paths, num_time_steps].\n\n# Generate OFDM Channel from CIR\nchannel = OFDMChannel( channel_response = cir,\n            resource_grid = resource_grid,\n            normalize_channel = False,\n            return_channel = True\n        )\n\n# Generate the channel output \n# x is the vector of transmitted symbols, here random LLRs for the codeword\nx = tf.random.normal(shape=[batch_size, 1, 1, 1, 1, num_tx_ant, num_ofdm_symbols,fft_size], dtype=tf.complex64)\ny = channel(x)\n```\n\nFinally, you have to precodes the vector of transmitted symbols using the precoding matrices. This is performed by the following code.\n\n```python\nc = OFDMChannelDecoder() \nx_precoded, no = remove_necessary_prefixes([x, no]) \noutput = c([x_precoded, h_hat, no]) \n```\n\nThis code uses the OFDMChannelDecoder() layer to decode the received symbols. The inputs, in this case the precoded vector of transmitted symbols `x_precoded`, the channel frequency responses `h_hat`, and the noise variance `no`, are fed to the decoder to produce the output."
"To compute the log-likelihood ratios with the `Demapper` class in the 'sionna.mapping' module, you will need to follow these steps:\n\n1. **Import the Demapper class**: You can do this using the following line of code:\n   ```python\n   from sionna.mapping import Demapper\n   ```\n\n2. **Set up the parameters**:\n   You will need to specify the demapping method (e.g., \"app\" or \"maxlog\"), the constellation type (\"constellation_type\"), the number of bits per symbol (\"num_bits_per_symbol\"), the constellation object (\"constellation\"), whether the demapper should output hard decisions or soft values (\"hard_out\"), and whether prior knowledge is available (\"with_prior\"). \n\n3. **Create an instance of the Demapper class**: You can create an instance of the `Demapper` class using the following line of code:\n   ```python\n   demapper = Demapper(demapping_method, constellation_type, num_bits_per_symbol, constellation, hard_out, with_prior)\n   ```\n   Replace \"demapping_method\", \"constellation_type\", \"num_bits_per_symbol\", \"constellation\", \"hard_out\", and \"with_prior\" with the actual values based on your requirements.\n\n4. **Call the Demapper instance**: The `Demapper` instance you've created is a callable object, so you can call it directly with your received symbols, prior knowledge (if it's needed), and noise variance. If you've set \"hard_out\" to True, the demapper will return hard decisions (i.e., binary values for each bit). Otherwise, it will return soft values, which are the log-likelihood ratios (LLRs) for each bit.\n\nHere's a hypothetical example that demonstrates these steps. Please note that it's essential to define the parameters `demapping_method`, `constellation_type`, `num_bits_per_symbol`, `constellation`, `hard_out`, and `with_prior` before creating the `Demapper` instance.\n\n```python\n# Define the Demapper parameters\ndemapping_method = \"app\"        # The demapping method (\"app\" or \"maxlog\")\nconstellation_type = \"qam\"      # The constellation type (\"qam\", \"pam\", or \"custom\")\nnum_bits_per_symbol = 4         # The number of bits per constellation symbol (e.g., 4 for QAM16)\n\n# Create the Demapper instance\ndemapper = Demapper(demapping_method, constellation_type, num_bits_per_symbol, constellation, hard_out, with_prior)\n\n# Call the Demapper instance with your symbols, prior knowledge (if needed), and noise variance\n# The symbols `pnt_symbols` are a tensor of shape [batch_size, n, num_tx], the prior \n# knowledge `pnt_prior` is either `None` or a tensor of shape \n# [batch_size, n, num_tx, num_bits_per_symbol]`, and `no` is the noise variance \n# which is a tensor of shape `[batch_size, n, num_rx, num_rx_ant]`.\n#\n# The Demapper will return LLRs, which are the log-likelihood ratios for each bit.\nllr = demapper([pnt_symbols, pnt_prior, no])\n```\n\nIn this example, replace \"[pnt_symbols, pnt_prior, no]\" with your actual data, and use the `call` method as described, making sure you have defined the parameters `demapping_method`, `constellation_type`, `num_bits_per_symbol`, `constellation`, `hard_out`, and `with_prior` before using the code."
"To obtain the Channel Impulse Responses (CIRs) in the model, you can follow these steps:\n\n1. Start by initializing a model of continuous-time channel impulse responses. In the given context, the CDL model is used for this purpose. The CDL model can be initialized using the `cdl` function from the `sionna.channel` module. This function generates a model of a clustered delay line wireless channel with a Rician, Tuman or Non-line-of-Sight (NLoS)  multipath components.\n\n    ```python\n    cdl_model = cdl(mindr=cdl_mindr, maxdr=cdl_maxdr, \n                    mindelay_spread=cdl_mindelay_spread, \n                    maxdelay_spread=cdl_maxdelay_spread, \n                    los_angles=los_angles, \n                    num_clusters=num_cdl_clusters, \n                    rician_factor=cdl_rician_factor, \n                    with_nlos=with_cdl_nlos, \n                    carrier_frequency=cdl_carrier_frequency, \n                    direction=cdl_direction)\n    ```\n\n    The `cdl` function parameters are set using the specified values for the scenario being modeled.\n\n2. Initialize a Gaussian noise vector that represents the noise power for each of the two dimensions (I/Q) of the considered resource grid elements. This can be done using the `tf.complex` function with `tf.zeros` for the `real` and `imag` parameters, and `tf.random.normal` for the `imag` parameter. The `tf.complex` function creates complex numbers with the real and imaginary parts provided as separate arguments, and `tf.random.normal` generates the Gaussian distributed noise.\n\n    ```python\n    noise = tf.complex(tf.zeros(rg.num_resource_grid_elements),\n                        tf.random.normal(rg.num_resource_grid_elements, dtype=cfg.dtype))\n    ```\n\n3. Obtain the CIRs by simulating the channel. Use the `cdl_model` initialized in step 1 and the `batch_size`, `num_time_steps`, `sampling_frequency`, and `num_samples` values set according to the requirements of the scenario being modeled.\n\n    ```python\n    a, tau = cdl_model(batch_size, num_time_steps, sampling_frequency)\n    ```\n    In the above line of code, `batch_size` is the number of example sequences that are processed in parallel, `num_time_steps` is the number of time steps for the channel impulse response simulation, and `sampling_frequency` is the frequency in Hertz at which the channel impulse response is sampled. These values should be set according to the specific needs of your simulation.\n\n    The function `cdl_model` returns the channel coefficient `a` and the channel delay `tau` for each of the `batch_size` example sequences. The shapes of `a` and `tau` will depend on `batch_size` and `num_time_steps`. \n\n    4. (Optional) Transform the CIRs to the frequency domain or time-domain by using the `cir_to_ofdm_channel` or `cir_to_time_channel` function. These functions require the sampling frequency and the subcarrier spacing as additional parameters.\n\n    ```python\n    # For frequency domain representation\n    h_freq = cir_to_ofdm_channel(tau, a, rg, sampling_frequency, c_frequency=cfg.subcarrier_spacing)\n    # For time domain representation\n    h_time = cir_to_time_channel(tau, a, oversampling_factor, num_ofdm_symbols, l_tot)\n    ```\n\n    In the `cir_to_ofdm_channel` function, `tau` and `a` are the channel delays and channel coefficients obtained from the continuous-time model, and `sampling_frequency` is the sampling frequency (in Hertz) for the discrete-time representation. `rg` is an instance of the `ResourceGrid` class, and `c_frequency` is the carrier frequency (in Hz). The `subcarrier_spacing` parameter is the subcarrier spacing in the same unit as `c_frequency`. The `cir_to_time_channel` function requires `tau`, `a`, `oversampling_factor`, `num_ofdm_symbols`, and `l_tot` correspond to the channel delay, channel coefficients, the over-sampling factor, the number of OFDM symbols, and the total length of the discrete-time channel impulse response, respectively.\n\n    Note: The specific functions and steps used in this explanation are based on the Sionna package and are contingent on the requirements of the scenario being modeled. Please adjust them as needed for your particular use case. It may also be helpful to refer to the Sionna documentation for additional details on the functions and classes used."
"Here is the detailed information of List2LLRSimple:   \n  \n[sionna.mimo.List2LLRSimple(num_bits_per_symbol, llr_clip_val=20.0, **kwargs)](https://nvlabs.github.io/sionna/_modules/sionna/mimo/utils.html#List2LLRSimple)  \n\nComputes LLRs from a list of candidate vectors (or paths) provided by a MIMO detector.\n\nThe following channel model is assumed:\n\n$\\bar{\\mathbf{y}} = \\mathbf{R}\\bar{\\mathbf{x}} + \\bar{\\mathbf{n}}$\n\nwhere $\\bar{\\mathbf{y}}\\in\\mathbb{C}^S$ are the channel outputs, $\\mathbf{R}\\in\\mathbb{C}^{S\\times S}$ is an upper-triangular matrix, $\\bar{\\mathbf{x}}\\in\\math{C}^S$ is the transmitted vector whose entries are uniformly and independently drawn from the constellation $\\mathcal{C}$, and $\\bar{\\mathbf{n}}\\in\\mathbb{C}^S$ is white noise with $\\mathbb{E}\\left[\\bar{\\mathbf{n}}\\right]=\\mathbf{0}$ and $\\mathbb{E}\\left[\\bar{\\mathbf{n}}\\bar{\\mathbf{n}}^{\\mathsf{H}}\\right]=\\mathbf{I}$.\n\nIt is assumed that a MIMO detector such as KBestDetector produces $K$ candidate solutions $\\bar{\\mathbf{x}}_k\\in\\mathcal{C}^S and their associated distance metrics $d_k$ for $k=1,\\dots,K$. This layer can also be used with the real-valued representation of the channel.\n\nThe LLR for the $i\\text{th}$ bit of the $k\\text{th}$ stream is computed as\n\n$\\begin{split}LLR(k,i) &= \\log\\left(\\frac{\\Pr(b_{k,i}=1|\\bar{\\mathbf{y}},\\mathbf{R})}{\\Pr(b_{k,i}=0|\\bar{\\mathbf{y}},\\mathbf{R})}\\right)\\\\\n    &\\approx \\min_{j \\in  \\mathcal{C}_{k,i,0}}d_j - \\min_{j \\in  \\mathcal{C}_{k,i,1}}d_j\\end{split}$\n\nwhere $\\mathcal{C}_{k,i,1}$ and $\\mathcal{C}_{k,i,0}$ are the set of indices in the list of candidates for which the $i\\text{th}$ bit of the $k\\text{th}$ stream is equal to 1 and 0, respectively. The LLRs are clipped to $\\pm LLR_\\text{clip}$ which can be configured through the parameter llr_clip_val.\n\nIf $\\mathcal{C}_{k,i,0}$ is empty, $LLR(k,i)=LLR_\\text{clip}$; if $\\mathcal{C}_{k,i,1}$ is empty, $LLR(k,i)=-LLR_\\text{clip}$.\n\nThis class is used as a base layer for the different MIMO detectors. It is purposely feed independent of the underlying detector algorithm to experiment with new detector/algorithm and analyze their performance in an (asymptotically) identical setup.\n\n**Parameters**\n\n- `num_bits_per_symbol` (int): Number of bits per constellation symbol, e.g., 4 for QAM16.\n- `llr_clip_val` (float): The absolute value of the LLRs are clipped to this value. Defaults to 20.0. Cannot be a TensorFlow variable.\n\n**Input**\n\n- `(y, r, dists, path_inds, path_syms)` \u2013 Tuple:\n  - `y` ([..., M], tf.complex or tf.float): Channel outputs of the whitened channel. For the real-valued representation, it is assumed to be tf.complex with imaginary parts being zero.\n  - `r` ([num_streams, num_streams], same dtype as y): Upper triangular channel matrix of the whitened channel.\n  - `dists` ([..., num_paths], tf.float): Distance metric for each path (or candidate).\n  - `path_inds` ([..., num_paths, num_streams], tf.int32): Symbol indices for every stream of every path (or candidate).\n  - `path_syms` ([..., num_path, num_streams], same dtype as y): Constellation symbol for every stream of every path (or candidate).\n\n**Output**\n\n- `llr` ([..., num_streams, num_bits_per_symbol], tf.float): LLRs for all bits of every stream. The shape is [..., num_streams*num_bits_per_symbol]."
"Here is the definition of MaximumLikelihoodDetector: sionna.ofdm.MaximumLikelihoodDetector(output, demapping_method, resource_grid, stream_management, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, dtype=tf.complex64, **kwargs)  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/ofdm/detection.html#MaximumLikelihoodDetector)   \nsource code:\n```python\nclass MaximumLikelihoodDetector(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"MaximumLikelihoodDetector(output, demapping_method, resource_grid, stream_management, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, dtype=tf.complex64, **kwargs)\n\n    Maximum-likelihood (ML) detection for OFDM MIMO transmissions.\n\n    This layer implements ML detection for OFDM MIMO transmissions. Both ML\n    detection of symbols or bits with either\n    soft- or hard-decisions are supported. The OFDM and stream configuration are provided\n    by a :class:`~sionna.ofdm.ResourceGrid` and\n    :class:`~sionna.mimo.StreamManagement` instance, respectively. The\n    actual detector is an instance of\n    :class:`~sionna.mimo.MaximumLikelihoodDetector`.\n\n    Parameters\n    ----------\n    output : One of [\"bit\", \"symbol\"], str\n        Type of output, either bits or symbols. Whether soft- or\n        hard-decisions are returned can be configured with the\n        ``hard_out`` flag.\n\n    demapping_method : One of [\"app\", \"maxlog\"], str\n        Demapping method used\n\n    resource_grid : ResourceGrid\n        Instance of :class:`~sionna.ofdm.ResourceGrid`\n\n    stream_management : StreamManagement\n        Instance of :class:`~sionna.mimo.StreamManagement`\n\n    constellation_type : One of [\"qam\", \"pam\", \"custom\"], str\n        For \"custom\", an instance of :class:`~sionna.mapping.Constellation`\n        must be provided.\n\n    num_bits_per_symbol : int\n        Number of bits per constellation symbol, e.g., 4 for QAM16.\n        Only required for ``constellation_type`` in [\"qam\", \"pam\"].\n        The parameter is ignored for ``constellation_type=\"custom\"``.\n\n    constellation : Constellation\n        Instance of :class:`~sionna.mapping.Constellation` or `None`.\n        In the latter case, ``constellation_type``\n        and ``num_bits_per_symbol`` must be provided.\n\n    hard_out : bool\n        If `True`, the detector computes hard-decided bit values or\n        constellation point indices instead of soft-values.\n        Defaults to `False`.\n\n    dtype : One of [tf.complex64, tf.complex128] tf.DType (dtype)\n        The dtype of ``y``. Defaults to tf.complex64.\n        The output dtype is the corresponding real dtype (tf.float32 or tf.float64).\n\n    Input\n    ------\n    (y, h_hat, err_var, no) :\n        Tuple:\n\n    y : [...,num_rx_ant,fft_size], tf.complex\n        Received OFDM resource grid after cyclic prefix removal and FFT\n\n    h_hat : [...,num_rx_ant,num_streams_per_tx,fft_size//2,:], tf.complex\n        Channel estimates for all streams from all transmitters\n\n    err_var : [...,num_rx_ant,num_streams_per_tx,fft_size//2,:], tf.float\n        Variance of the channel estimation error\n\n    no : [...,num_rx_ant,..fft_size], tf.float\n        Variance of the AWGN\n\n    Output\n    ------\n    One of:\n\n    : [...,num_streams_per_tx,num_data_symbols*num_bits_per_symbol], tf.float\n        LLRs or hard-decisions for every bit of every stream, if ``output`` equals `\"bit\"`.\n\n    : [...,num_streams_per_tx,num_data_symbols, num_points], tf.float or [...,num_streams_per_tx,num_data_symbols], tf.int\n        Logits or hard-decisions for constellation symbols for every stream, if ``output`` equals `\"symbol\"`.\n        Hard-decisions correspond to the symbol indices.\n    \"\"\"\n    def __init__(self,\n                 output,\n                 demapping_method,\n                 resource_grid,\n                 stream_management,\n                 constellation_type=None,\n                 num_bits_per_symbol=None,\n                 constellation=None,\n                 hard_out=False,\n                 dtype=tf.complex64,\n                 **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        assert dtype in [tf.complex64, tf.complex128],\\\n            \"dtype must be tf.complex64 or tf.complex128.\"\n\n        # Instantiate the maximum-likelihood detector\n        Detector = MaximumLikelihoodDetector_\n        self._detector = Detector(output=output,\n                                  num_bits_per_symbol=num_bits_per_symbol,\n                                  constellation=constellation,\n                                  demapping_method=demapping_method,\n                                  num_streams = stream_management.num_streams_per_rx,\n                                  dtype=dtype.real_dtype)\n\n        # Preprocessing steps\n        resource_grid = resource_grid\n        stream_management = stream_management\n        cyclic_prefix_remover = ofdm.CyclicPrefixRemoval(\n                                                    resource_grid.cyclic_prefix_length)\n        fft = ofdm.FastFourierTransform()\n        frequency_selectivity = ofdm.FrequencySelectiveChannel(\n                                                resource_grid, stream_management)\n        # We have to process the received signal per OFDM symbol\n        process_ofdm_symbol = lambda y : (\n                                cyclic_prefix_remover(y)\n                                # Only part with non-negative lag will be used for detection\n                                # as done by the channel_equalizer\n                                # The -1 is used to revert the effect of the last CP sample\n                                # of the OFDM symbol which is also the last PN-symbol of the\n                                # PRNG; cp_length = fft_size//4\n                                [:-resource_grid.cyclic_prefix_length//4, -1]\n                                tf.reverse)\n        self._preprocess = tflib.utils.MapWithExtraArgs(process_ofdm_symbol)\n\n    def build(self, input_shape): #pylint: disable=unused-argument\n        # We do not need to build a model, so we do nothing.\n        # All the \"building\" is done when calling the maximum-likelihood detector.\n\n    def call(self, inputs):\n        y, h_hat, err_var, no = inputs\n\n        # Save the dtype of y_hat for the end\n        dtype = y.dtype\n\n        # (Inverse) Pre-whitening of the channel\n        # h whithese\n        h = whiten_channel(h_hat, err_var, no)\n        # whiten signal\n        y = whiten_signal(y, err_var, no, expand_to_rank_of(h))\n\n        # Prepare the channel input: only the first 0..K channel inputs\n        # will be selected by the channel_select function\n        y = tf.pad(y, pad_before = [[0, 0], [0, 0], [0, 0], [0, 1]], mode=\"constant\") # pylint: disable=line-too-long\n        y = channel_input_concat([y, tf.zeros(y.shape.as_list()[0:3]+[1], y.dtype)]) # pylint: disable=line-too-long\n        h = tf.expand_dims(h, axis=-2)\n        h = tf.repeat(h, y.shape[3], axis=0)\n        inputs = [y, h]\n\n        # Determine the number of preceeding noise samples for each channel input.\n        # Note that this also accounts for the channel inputs being interleaved\n        # by different stream management strategies.\n        n_o = tf.reduce_sum(no, axis=-1)\n        n_o = tf.expand_dims(n_o, axis=-1)\n        n_o = tf.repeat(n_o, y.shape[3], axis=0)\n        inputs.append(n_o)\n\n        # Process the received signal one symbol at a time\n        z = self._preprocess(y)\n        llr_max = None\n        for i in range(z.shape[0]):\n            z_ = z[i:i+1] # i+1 to have z_ as 3-dim tensor\n            llr = self._detector(inputs, z_)\n            if llr_max is None:\n                llr_max = tf.zeros_like(llr)\n            llr_max = tf.maximum(llr, llr_max)\n\n        llr_max = tf.squeeze(llr_max, axis=0) # remove the single symbol dimension\n        return tf.cast(llr_max, dtype)\n```"
"Here is the detailed information:  \n[sionna.fec.polar.utils.generate_5g_ranking(k, n, sort=True)](https://nvlabs.github.io/sionna/_modules/sionna/fec/polar/utils.html#generate_5g_ranking)\n\nReturns information and frozen bit positions of the 5G Polar code as defined in Tab. 5.3.1.2-1 in [3GPPTS38212] for given values of k and n.\n\n**Note: ** Tab. 5.3.1.2-1 in [3GPPTS38212] provides the indices for the *systematic* encoder. In the returned positions, the non-systematic part is accounted for, which is equivalent to adding n to all `info_pos`.\n\n**Input**\n\n- `k` (int) \u2013 The number of information bits per codeword.\n- `n` (int) \u2013 The desired codeword length. Must be a power of two.\n- `sort` (bool) \u2013 Defaults to True. Indicates if the returned indices are sorted.\n\n**Output**\n\n- `frozen_pos, info_pos` (int, int) \u2013 2-Tuple:\n    - `frozen_pos` (ndarray) \u2013 An array of ints of shape [n-k] containing the frozen position indices.\n    - `info_pos` (ndarray) \u2013 An array of ints of shape [k] containing the information position indices.\n\n\n**Raises**\n\n- `AssertionError` \u2013 If k or n are not positve ints.\n- `AssertionError` \u2013 If sort is not bool.\n- `AssertionError` \u2013 If k or n are larger than 1024.\n- `AssertionError` \u2013 If n is less than 32.\n- `AssertionError` \u2013 If the resulting coderate is invalid (>1.0).\n- `AssertionError` \u2013 If n is not a power of 2.\n\nsource code:  \n```python\ndef generate_5g_ranking(k, n, sort=True):\n    # pylint: disable=line-too-long\n    r\"\"\"Returns information and frozen bit positions of the 5G Polar code as\n    defined in Tab. 5.3.1.2-1 in [3GPPTS38212]_ for given values of ``k`` and ``n``.\n\n    Input\n    -----\n    k : int\n        The number of information bit per codeword.\n\n    n : int\n        The desired codeword length. Must be a power of two.\n\n    sort: bool\n        Defaults to True. Indicates if the returned indices are sorted.\n\n    Output\n    ------\n    frozen_pos, info_pos : int, int\n        2-Tuple\n\n    frozen_pos : ndarray of ints\n        An array of ints of shape `[n-k]` containing the frozen position indices.\n\n    info_pos : ndarray of ints\n        An array of ints of shape `[k]` containing the information position\n        indices.\n\n\n    Raises\n    ------\n    AssertionError\n        If ``k`` or ``n`` are not positve ints.\n\n    AssertionError\n        If ``sort`` is not bool.\n\n    AssertionError\n        If ``k`` or ``n`` are larger than 1024.\n\n    AssertionError\n        If ``n`` is less than 32.\n\n    AssertionError\n        If the resulting coderate is invalid (`>1.0`).\n\n    AssertionError\n        If ``n`` is not a power of 2.\n    \"\"\"\n\n    # tab 5.3.1.2-1 from 38.212 3GPP TS\n    if n== 8:\n        if k == 2:\n            pos_frozen = np.array([6, 7])\n            pos_info = np.array([0, 1])\n        elif k == 3:\n            pos_frozen = np.array([5, 6, 7])\n            pos_info = np.array([0, 1, 2])\n        elif k == 4:\n            pos_frozen = np.array([4, 5, 6, 7])\n            pos_info = np.array([0, 1, 2, 3])\n        else:\n            raise AssertionError(\"Invalid code parameters\")\n    elif n== 64:\n        if k == 7:\n            pos_frozen = np.array([56, 57, 58, 59, 60, 61, 62, 63])\n            pos_info = np.array([0, 1, 2, 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 22, 23,\n                                  24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n                                  38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n                                  52, 53, 54, 55])\n        elif k == 8:\n            pos_frozen = np.array([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59,\n                                   60, 61, 62, 63])\n            pos_info = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n                                  16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,\n                                  30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n                                  44, 45, 46, 47])\n        elif k == 9:\n            pos_frozen = np.array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n                                   52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n                                   64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75])\n            pos_info = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n                                  16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,\n                                  30, 31, 32, 33, 34, 35, 36, 37, 38, 39])\n        elif k == 10:\n            pos_frozen = np.array([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n                                   44, 45, 46, 47, 48, 49, 72, 73, 74, 75, 76, 77,\n                                   78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n                                   90, 91, 92, 93, 94, 95, 96, 97, 100, 101, 102, 103,\n                                   104, 105, 106, 107, 108, 109, 110, 111, 112, 113,\n                                   114, 115, 116, 117, 120, 121, 122, 123, 124, 125,\n                                   126, 127])\n            pos_info = np.array([0, 1, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n                                  21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 92, 93, 94,\n                                  95, 96, 97, 98, 99, 102, 103, 104, 105, 106, 107,\n                                  108, 109, 110, 111, 118, 119, 120, 121, 122, 123, 124,\n                                  125, 126, 127])\n        else:\n            raise AssertionError(\"Invalid code parameters\");\n    elif n== 128:\n        if k == 8:\n            pos_frozen = np.array([120, 121, 122, 123, 124, 125, 126, 127,\n                                   128, 129, 130, 131, 132, 133, 134, 135,\n                                   136, 137, 138, 139, 140, 141, 142, 143,\n                                   144, 145, 146, 147, 148, 149, 150, 151,\n                                   152, 153, 154, 155, 156, 157, 158, 159,\n                                   160, 161, 162, 163, 164, 165, 166, 167])\n            pos_info = np.array([0, 1, 2, 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 22, 23,\n                                  24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n                                  38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n                                  52, 53, 54, 55])\n        elif k == 16:\n            pos_frozen = np.array([96, 97, 98, 99, 100, 101, 102, 103, 104, 105,\n                                   106, 107, 108, 109, 110, 111, 112, 113, 114, 115,\n                                   116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n                                   126, 127, 130, 131, 132, 133, 134, 135, 136, 137,\n                                   138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n                                   148, 149, 150, 151, 152, 153, 154, 155])\n            pos_info = np.array([0, 1, 2, 3, 12, 13, 14, 15, 28, 29, 30, 31, 44, 45, 46,\n                                  47, 58, 59, 60, 61, 74, 75, 76, 77, 82, 83, 84, 85,\n                                  86, 87, 90, 91, 94, 95, 108, 109, 110, 111, 124,\n                                  125, 126, 127])\n        elif k == 20:\n            pos_frozen = np.array([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75,\n                                   76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87,\n                                   88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 100, 101,\n                                   102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n                                   112, 113, 114, 115, 118, 119, 120, 121, 122, 123,\n                                   126, 127])\n            pos_info = np.array([0, 1, 4, 5, 38, 39, 42, 43, 56, 57, 60, 61, 72, 73, 76,\n                                  77, 80, 81, 84, 85, 94, 95, 108, 109, 112, 113,\n                                  116, 117, 130, 131, 134, 135, 146, 147, 150, 151,\n                                  154, 155, 170, 171, 174, 175, 178, 179, 192, 193,\n                                  200, 201, 204, 205, 218, 219, 222, 223])\n        else:\n            raise AssertionError(\"Invalid code parameters\");\n    else:\n        raise AssertionError(\"n must be 8, 16, 32, 64, 128 (and k according k)\");\n\n\n    if sort:\n        pos_frozen = np.sort(pos_frozen)\n        pos_info = np.sort(pos_info)\n\n    return [pos_info, pos_frozen]\n```  \n  "
"To run the channel modeling in Sionna, follow these steps:\n\n1. First, configure the CDL model. This involves setting parameters such as the delay spread, direction (whether it's an uplink or downlink transmission), carrier frequency, UT array, BS array, minimum speed, etc. \n\n```python\nDELAY_SPREAD = 100e-9\nDIRECTION = \"uplink\"\nCDL_MODEL = \"C\"\nSPEED = 10.0\n\nCDL = sn.channel.tr38901.CDL(CDL_MODEL, DELAY_SPREAD, CARRIER_FREQUENCY, UT_ARRAY, BS_ARRAY, DIRECTION, min_speed=SPEED)\n```\n\n2. Next, setup the resource grid for the OFDM system. This can be done using the `ResourceGrid` class.\n\n3. Then, create an instance of the `OFDMChannel` class, which will use the CDL model to generate channel frequency responses.\n\n```python\nCHANNEL = sn.channel.tr38901.OTWChannel(CDL, RESOURCE_GRID, interpolator)\n```\n\n4. To get the channel frequency responses, compute the channel at the frequency's null index, i.e., `n`, by calling `CHANNEL(frequencies, n)` where `frequencies` is a list of the non-null subcarrier frequencies.\n\n5. The channel frequency response can be used to generate channel impulse responses (CIRs) in the time domain. This can be done using the `h_freq_to_time` function.\n\n6. After obtaining the channel impulse responses, they can be applied to the transmitted signal to simulate the signal received at the receiver. The `ApplyCIR` layer can be used for this in a Keras model.\n\nPlease note that the actual code and parameter values may vary based on the specific requirements of your setup. The above steps provide a general guideline for how to use Sionna for channel modeling."
"Here is the detailed information of the function: \n\n[sionna.fec.utils.load_alist(path)](https://nvlabs.github.io/sionna/_modules/sionna/fec/utils.html#load_alist)\n\nRead alist-file [[MacKay]](https://nvlabs.github.io/sionna/api/fec.utils.html#mackay) and return nested list describing the parity-check matrix of a code.\n\nMany code examples can be found in [[UniKL]](https://nvlabs.github.io/sionna/api/fec.utils.html#unikl).\n\nInput\npath (str) \u2013 Path to file to be loaded.\n\nsource code:\n```python\ndef load_alist(path):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    Read alist-file [MacKay2007]_ and return nested list describing\n    the parity-check matrix of a code.\n\n    Many code examples can be found in [UniKL]_.\n\n    Input\n    -----\n    path: str\n        Path to file to be loaded.\n\n    Output\n    ------\n    alist: list\n        A nested list containing the imported alist data.\n    \"\"\"\n    with open(path, 'r') as f:\n        content = f.readlines()\n\n    # remove comments\n    content = [l for l in content if not l.startswith(\"%\")]\n\n    # split lines\n    content = [...]\n```"
"Here are useful links of it:\n\n[Run in Google Colab](https://colab.research.google.com/github/nvlabs/sionna/blob/main/examples/5G_NR_PUSCH_Tutorial.ipynb)\n\n[View on GitHub](https://github.com/nvlabs/sionna/blob/main/examples/5G_NR_PUSCH_Tutorial.ipynb)\n\n[Download notebook](https://nvlabs.github.io/sionna/examples/5G_NR_PUSCH_Tutorial.ipynb)"
"The `cameras` property of the `Scene` in Sionna's Ray Tracing module is a property that lists all the cameras that are currently available for rendering within the specific scene. These can be cameras that are either defined as part of the scene file or instantiated using the API. \n\nFor anyone new to Sionna Ray Tracing, a `Scene` can be considered as the environment or setting for which ray tracing or other related operations are to be performed. The `Scene` can include multiple components like transmitters, receivers, cameras, etc.\n\nThe `cameras` property is not just a list of strings or names of the cameras, but rather a list of camera objects (`Camera` class instances) that have been either created or added to the scene. These cameras serve as viewpoints for the rendering of the scene. \n\nThe `cameras` property is used in the code snippet provided in the context to print out the list of all available cameras within the scene:\n```python\nprint(scene.cameras)\n```\nThis command would print the details of all the cameras. \n\nThe `cameras` property can be manipulated when working with the scene. For example, a new camera can be created and added to the scene using the `add` method:\n```python\nscene.add(cam)\n```\nIn this line of code, `cam` is a `Camera` object that has been created with a particular name and configuration, and `add` is a method of the `Scene` class used to add the camera object to the scene.\n\nThe `cameras` property can also be used to render the scene from the perspective of a specific camera. If no camera is provided as an argument to the `render` method, the default camera of the scene is used. However, you could also do something like this:\n```python\nscene.render(cam)\n```\nWhere `cam` is a previously defined `Camera` object. This creates a render of the scene from the viewpoint of that specific camera.\n\nTo summarize, the `cameras` property in the `Scene` class is a crucial component when working with scenes in Sionna Ray Tracing, as it allows you to list, manipulate, and use predefined or custom cameras within your scenes. Cameras are used to provide viewpoints for rendering or other types of visual or data analysis within the scene."
"Here is the detailed information of RaisedCosineFilter:   \n  \n[sionna.signal.RaisedCosineFilter(span_in_symbols, samples_per_symbol, beta, window=None, normalize=True, trainable=False, dtype=tf.float32, **kwargs)](https://nvlabs.github.io/sionna/_modules/sionna/signal/filter.html#RaisedCosineFilter)  \n\nLayer for applying a raised-cosine filter of length K to an input x of length N.\n\nThe raised cosine filter is defined by \n$h_{\\text{RC}}(t) = \\begin{cases}\n        \\frac{1}{2T} \\left(1 + \\cos\\left(\\frac{\\pi t}{T}\\right) \\right), & \\text{if }t \\in [-T, T)\\\\\n        0, & \\text{otherwise}\n    \\end{cases}$\nwhere $T$ is the roll-off factor and the filter length K = N (span_in_symbols) + 1  is an odd number. The filter operates on the last dimension of the input x. If normalize is True, the filter is normalized to ensure unit average power per coefficient.\n\nThe filter is applied through discrete convolution.\n\n### Parameters\n\n- `span_in_symbols` (int): Filter span as measured by the number of symbols.\n- `samples_per_symbol` (int): Number of samples per symbol.\n- `beta` (float): Roll-off factor. Must be in the range $[0,1]$.\n- `window` (string or Window instance): Windowing function applied to the filter coefficients. This can be a string indicating one of Hann, Hamming, Blackman, or BlackmanHarris, or an instance of Window. Custom window instances are applied using the string method. Windowing is applied after normalization and training, if applicable. If window is set to None, then no window is applied. Custom windows must have a dtype specified, and are applied after the dtype has been verified.\n- `normalize` (bool): If True, the filter is normalized to ensure unit average power per coefficient. Defaults to True.\n- `trainable` (bool): If True, the filter coefficients are trainable. Defaults to False.\n- `dtype` (tf.DType): The dtype of the filter coefficients. Defaults to tf.float32.tf.float32.\n    - `kwargs` \u2013 Additional keyword arguments. Note that the function is designed as a Keras layer, and the arguments are put in a dict for easy handling.\n\n### Input\n\n- `x` ([..., N], tf.complex): The input to which the filter is applied. Assumes an odd filter length K, such that (K-1)/2 samples are applied on the left and right side of the current sample.\n\n### Output\n\n- `y` ([..., N + K - 1], tf.complex): Filtered output. The output is tf.complex64 only if both the dtype of the filter and the dtype of the input are tf.complex64. Otherwise, it is tf.complex128. The length of the output is N + K - 1. N, and K are the last dimensions of the input and filter length, respectively."
"Here is the detailed information of sionna.signal.Upsampling:   \n\nsionna.signal.Upsampling (up, dtype=tf.complex64)\n  \n### Upsampling\n\nUpsample an input by inserting zeros between elements.\n\nThis operation inserts zeros at regular intervals in the data sequence and increases the data rate. The upsampling factor must be specified as an integer and conform to the \"sample-domain\", i.e., only integer values are allowed. The element index of the zero-valued elements is called the \"upsampling shift\" or \"delay\".\n\n### Parameters\n\n- `up` (int): Upsampling factor. This is the number of zero-valued samples to insert between the non-zero samples.\n- `dtype` (tf.DType): The dtype of the filter coefficients. Defaults to tf.complex64. The same dtype will be used for the output.\n\n### Input\n\n- `x` ([,n], tf.complex): Input sequence to upsample.\n- \n### Output\n\n- `y` ([,n*up], tf.complex): Uprsampled sequence."
"The 'polarization_model_1' and 'polarization_model_2' functions from the sionna.rt.antenna module are part of a set of utility functions used for antenna pattern calculations in radio propagation modeling. Both functions implement a first polarization model from 3GPP specification 38.901 (\"TR38.901\"). These can be used when simulating properties of antennas in a network, such as gain or radiation patterns, to help determine the characteristics of the electromagnetic fields they produce.\n\nThe 'polarization_model_1' function takes the zenith and azimuth patterns of a vertically polarized antenna as inputs and computes corresponding zenith and azimuth patterns after polarization following the Model-1 method specified by 3GPP TR 38.901. The function returns the zenith pattern and the azimuth pattern.\n\nInput parameters for 'polarization_model_1' function:\n1. c_tilde_theta: Zenith pattern for all positions wrapped within [0,pi] [rad], as a complex array.\n2. theta: Zenith angles for wrapped positions [0,pi] [rad], as a float array.\n3. phi: Azimuth angles for wrapped positions [-pi, pi) [rad], as a float array.\n4. slant_angle: Slant angle of the linear polarization [rad]. A slant angle of zero implies vertical polarization.\n\nThe function doesn't accept keyword arguments and requires all input parameters to be provided positionally.\n\nThe 'polarization_model_2' function works similarly but implements the Model-2 method. This involves taking the zenith, azimuth, and plus/minus angle as inputs and computing corresponding zenith and azimuth patterns after polarization following the Model-2 method specified by 3GPP TR 38.901. The function returns the zenith pattern and the azimuth pattern.\n\nInput parameters for 'polarization_model_2' function:\n1. c_tilde_theta: Zenith pattern for all positions wrapped within [0,pi] [rad], as a complex array.\n2. theta: Zenith angles for wrapped positions [0,pi] [rad], as a float array.\n3. phi: Azimuth angles for wrapped positions [-pi, pi) [rad], as a float array.\n4. theta_pm: Plus/minus zenith angles [rad], for which the zenith pattern will be computed. This can either be a single value or an array.\n5. slant_angle: Slant angle of the linear polarization [rad]. A slant angle of zero means vertical polarization.\n\nThe function doesn't accept keyword arguments and requires all input parameters to be provided positionally.\n\nFor both of these functions, the computations are based on equations defined in the 3GPP specifications. This includes formulae for transforming a basic antenna pattern into a fully polarized pattern, taking into account different angles and polarization methods. These might be useful in simulating signal propagation in a variety of scenarios, especially related to wireless or mobile networking."
"Here is the detailed information of Mapper:   \n  \nsionna.mapping.Mapper(constellation_type=None, num_bits_per_symbol=None, constellation=None, return_indices=False, dtype=tf.complex64, **kwargs)  \n \nLayer that maps binary tensors to points of a constellation.\n\n### Parameters\n\n- **constellation_type** (str): One of [\"qam\", \"pam\", \"custom\"]. For \"custom\", an instance of `Constellation` must be provided.\n- **num_bits_per_symbol** (int): The number of bits per constellation symbol, e.g., 4 for QAM16. Only required for `constellation_type` in [qam, pam].\n- **constellation** (Constellation): An instance of `Constellation` or None. In the latter case, `constellation_type` and `num_bits_per_symbol` must be provided.\n- **return_indices** (bool): If enabled, symbol indices are additionally returned. Defaults to False.\n- **dtype** (One of [tf.complex64, tf.complex128], tf.DType): The output dtype. Defaults to tf.complex64.\n\n### Input\n\n- **x** [,n], tf.float or tf.int: 2D or higher-dimension tensor with with binary entries.\n\n### Output\n\n- **[,n/Constellation.num_bits_per_symbol], tf.complex**: The mapped constellation symbols.\n- **[,n/Constellation.num_bits_per_symbol], tf.int32**: The symbol indices corresponding to the constellation symbols. Only returned if `return_indices` is True.\n\n### Raises\n\n- AssertionError: If constellation is not compatible with constellation_type and num_bits_per_symbol.  \n  \n**Note: **   \nThe last output, symbol_indices, is only returned if return_indices is True.  \nThe last dimension of x must be an integer multiple of the number of bits per constellation symbol.  \n  \n  \n  "
"The `LDPC5GEncoder` class is a part of the `sionna.fec.ldpc.encoding` module, used to encode data for transmission using 5G NR compatible LDPC codes. The parameters and their types for the `LDPC5GEncoder` constructor are:\n\n1. `k` (int): Defining the number of information bits per codeword. This is the input size of the 5G NR LDPC encoder.\n\n2. `n` (int): Defining the desired codeword length. This is the output size of the the 5G NR LDPC encoder.\n\n3. `num_bits_per_symbol` (int, None): Defining the number of bits per QAM symbol.\n\n4. `dtype` (tf.DType): Defaults to `tf.float32`. Defines the output datatype of the encoder, internal precision remains `tf.uint8`.\n\nThe context provides a code example on how the encoder is instantiated and used, showing that the constructor could be called with the above parameters. Example usage demonstrated in the context is:\n\n```python\nencoder = LDPC5GEncoder(k = 100, n = 200) # for any k, n values as required\n```\n\nIt is essential to ensure that appropriate integer values are provided for `k` and `n` to reflect the desired use of the LDPC encoder, and the `num_bits_per_symbol` parameter should be set to None unless a specific modulation scheme should be used (for example, QAM), which would then define the modulation order."
"Answer:\n\n1. **Complex2real_vector**: This function processes a vector of complex values into its real-valued equivalent. It essentially separates the complex vector into its real and imaginary components, effectively doubling the dimensionality of the input vector. The real-valued equivalent of the initial complex vector is then computed, following the same process as is done for real-valued vectors. This function is used in situations where operations on real-valued vectors are easier to perform than on complex-valued vectors, which might be useful for certain transformations or computations.\n\n2. **Real2complex_vector**: This function operates in the reverse manner of the `Complex2real_vector` function. It takes a real-valued vector and converts it back into a complex-valued vector by interpreting the first half as the real part and the second half as the imaginary part. This is particularly useful in scenarios where computations have been done on real-valued data which need to be transformed back into the corresponding complex representation.\n\n3. **Complex2real_matrix**: This function deals with matrices in a similar way as the `Complex2real_vector`, by doubling the dimensions to translate complex matrices into their real-valued equivalents. It is particularly applicable in situations where complex matrices need to be treated as real-valued for computations or transformations.\n\n4. **Real2complex_matrix**: This function complements the `Complex2real_matrix` by converting a real-valued matrix back into its complex-valued equivalent. It interprets the first half of the input matrix as the real part and the second half as the imaginary part, and combines this information to produce the complex-valued matrix.\n\n5. **Complex2real_covariance**: This function transforms a complex-valued covariance matrix into its real-valued equivalent. It applies the same principle of doubling dimensions as seen in the previous functions, and then computes the real-valued equivalent of the covariance matrix based on the real and imaginary parts of the original complex matrix.\n\n6. **Real2complex_covariance**: This is the counterpart of the `Complex2real_covariance` function, handling the conversion of a real-valued covariance matrix back into its complex-valued form. It interprets the first half of the given real-valued covariance matrix as the real part and the second half as the imaginary part, and combines this information to reconstruct the complex-valued covariance matrix.\n\n7. **Complex2real_channel**: This function is used for transforming a complex-valued channel into its real-valued equivalent. It creates separate real-valued channels for the real and imaginary parts of the original complex channel and combines them following a 2MIMO to MIMO mapping, effectively doubling the dimensionality of the original complex channel to form the real-valued equivalent.\n\n8. **Real2complex_channel**: This function reverses the process of `Complex2real_channel`, converting a real-valued channel back into its complex-valued form. It assumes that the input real-valued channel corresponds to the real-valued equivalent of a complex-valued channel, and combines the real and imaginary parts following a MIMO to 2MIMO mapping to recover the original complex-valued channel.\n\nUnderstanding these functions mainly revolves around comprehending the mapping and demapping steps, as well as knowing when each function should be used. The context in which these functions are applied will typically dictate their usage."
"Here is the detailed information of FlatFadingChannel:   \n  \n[sionna.channel.FlatFadingChannel](https://nvlabs.github.io/sionna/_modules/sionna/channel/flat_fading_channel.html#FlatFadingChannel)  \n  \nGenerates channel impulse responses corresponding to a flat fading channel model.\n\nOnly the channel impulse response is generated by this module. The channel add awgn module can be used to add white noise.\n\nThe channel model is based on the following paper:  \nA. C. So and J. M. Guo, \u201cEfficient simulation for the EMI-induced outage due to\nCBF in OFDM-based HSD systems,\u201d IEEE Int. Symp. Model. and Charact. of\n        Electronic and Mach. (CHEME), 2011.  \n\nThe channel impulse response $\\{a_m(t), \\tau_m\\}$, $0 \\leq m \\leq M-1$, for a given channel\nbandwidth $W$, maximum delay spread $D$ and carrier frequency $f_c$, are generated as follows:  \n$\\tau_m = \\frac{M-1}{W}\\left(0.5 - U(0, 1)\\right)$  \n$a_m(t) = \\frac{1}{W}\\sqrt{\\frac{2}{\\pi}}\\sum_{k=0}^{K-1} \\hat{a}_k\n        \\exp\\left(j2\\pi W \\tau_m (t - k) + \\phi_k\\right)$, $t \\in \\left[0, \\lceil\\frac{M}{W} \\rceil\\right]$\nwhere $\\lceil . \\rceil$ is the ceil function, $U(0, 1)$ is the uniform distribution, $W$ the channel bandwidth, $M$ the maximum delay spread, $K$ the number of paths, $\\hat{a}_k$ and $\\phi_k$ the fading amplitude and phase for the $k^{th}$ path and $\\tau_m$ the $m^{th}$ delay\ns as [sionna.channel.FlatFadingChannel](https://nvlabs.github.io/sionna/_modules/sionna/channel/flat_fading_channel.html#FlatFadingChannel)  \n\n**Example**\nThe following code snippet shows how to generate channel impulse responses between an arbitrary pair of transmitters and receivers using the FlatFadingChannel module.\n\n```python\n# Number of TX and RX antennas\nnum_tx_ant = 4\nnum_rx_ant = 8\n# Instantiate a FlatFadingChannel object\nff = FlatFadingChannel(num_tx_ant, num_rx_ant)\n# Generate 5 channel impulse responses with 10ns delay spread\nbatch_size = 5\ndelay_spread = 10e-9\na, tau = ff(batch_size, delay_spread)\nprint(a.shape)  # [batch_size, num_rx_ant, num_tx_ant, num_paths]\nprint(tau.shape)  # [batch_size, num_rx_ant, num_tx_ant, num_paths]\n```\n\n**Result**\n\u200b[batch_size, num_rx_ant, num_tx_ant, num_paths], tf.float32 \u2013 Path coefficients\n\n\u200b[batch_size, num_rx_ant, num_tx_ant, num_paths], tf.float32 \u2013 Path delays (in seconds)\n\n### Properties\n\n- **`num_rx_ant`**\n  Number of receive antennas.\n- **`num_tx_ant`**\n  Number of transmit antennas.\n\n### Methods\n\n- **`__call__(batch_size, delay_spread)`**\n  - **Input**\n    - `batch_size` (int): The batch size.\n    - `delay_spread` (float): The maximum delay spread in seconds.\n  - - **Output**\n    - `a` ([batch_size, num_rx_ant, num_tx_ant, num_paths], tf.float) \u2013 Path coefficients.\n    - `tau` ([batch_size, num_rx_ant, num_tx_ant, num_paths], tf.float) \u2013 Path delays (in second)."
"Here is the definition of PilotPattern: \n\nsionna.ofdm.PilotPattern(mask, pilots, trainable=False, normalize=False, dtype=tf.complex64)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/ofdm/pilot_pattern.html#PilotPattern) \n\n```python\n#\n# SPDX-FileCopyrightText: Copyright (c) 2021-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n\"Class defining a pilot pattern for an OFDM ResourceGrid\"\n\nimport tensorflow as tf\nimport numpy as np\nfrom collections.abc import Sequence\nfrom tensorflow.keras.layers import Layer\nfrom sionna.utils import flatten_last_dims\n\n\nclass PilotPattern(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    Class defining a pilot pattern\n\n    This class defines a pilot pattern object that is used to configure an\n    :class:`~sionna.ofdm.ResourceGrid`.\n\n    Parameters\n    ----------\n    mask : [num_tx, num_streams_per_tx, num_ofdm_symbols, num_effective_subcarriers], bool\n        Tensor indicating resource elements that are reserved for pilot\n        transmissions. This mask is fully configurable and indicates which\n        resource elements are to be reserved for pilot transmissions.\n        Note that the provided ``mask`` must be at least element-wisely non-larger than the\n        automatically generated mask to allow the configuration of fully trainable\n        pilot sequences. If ``normalize`` is `True`, the pilots are normalized to have unit\n        average energy across the last dimension. This normalization is\n        automatically applied after new values for pilots have been set.\n\n    pilots : [num_tx, num_streams_per_tx, num_pilots], tf.complex\n        The pilot symbols to be mapped onto the ``mask``.\n        The pilots are mapped onto the resource grid using the\n        :class:`~sionna.ofdm.ResourceGrid` class.\n\n    trainable : bool\n        If `True`, the :class:`~sionna.ofdm.ResourceGrid` will configure\n        internally `pilots` to be trainable. Defaults to `False`.\n\n    normalize : bool\n        If `True`, the pilots are normalized to have unit average energy\n        across the last dimension. This normalization is not applied\n        internally but can be used to control of the pilots are normalized\n        or not. Defaults to `False`.\n\n    dtype : tf.Dtype\n        Defines the datatype for internal calculations and the output\n        dtype. Defaults to `tf.complex64`.\n    \"\"\"\n\n    def __init__(self,\n                 mask,\n                 pilots,\n                 trainable=False,\n                 normalize=False,\n                 dtype=tf.complex64\n                ):\n        super().__init__()\n        self._dtype = dtype\n        self.mask = mask\n        self.pilots = pilots\n        self.trainable = trainable\n        self.normalize = normalize\n\n    def generate(self, resource_grid):\n        \"\"\"Generate a resource grid with pilots.\n\n        This method generates for every transmitter and stream a\n        full resource grid with pilot symbols.\n\n        Input\n        ------\n        resource_grid :  ResourceGrid\n            An instance of :class:`~sionna.ofdm.ResourceGrid` that\n            defines the sparsity pattern of the pilot ``mask``.\n        \"\"\"\n\n        # Define a zero-like tensor with the shape of the full resource grid\n        # but with an additional dimension for the number of pilots.\n        shape = resource_grid.shape.as_list() + [self.num_pilots]\n        x = tf.zeros(shape, self._dtype)\n\n        # Next, we gather the trainable pilots.\n        # There are gathered on-the-fly to avoid that pilots are\n        # training on unused positions.\n        pilots = self._pilots\n        pilots = tf.gather(pilots, self._gather_indices, axis=-1)\n\n        # Define the dense representation of the mask to index into the\n        # zero-initialized tensor to insert the pilot symbols.\n        dense_mask = np.zeros([num_tx, num_streams_per_tx] \\\n                              + resource_grid.sparsity_pattern \\\n                              + [self._num_pilots])\n        for tx_ind in range(num_tx):\n            for stream_ind in range(num_streams_per_tx)\n                dense_mask[tx_ind, stream_ind, : , :] =  \\\n                    tf.squeeze(self.mask[tx_ind, stream_ind, :, :], axis=0)\n\n        insert_ind = tf.cast(tf.where(dense_mask), tf.int32)\n        insert_ind = tf.concat(insert_ind, axis=-1)\n\n        # Gather for every tx and stream the right insert indices\n        tx_ind = resource_grid.tx_ind\n        stream_ind = resource_grid.stream_ind\n        c = tf.gather(insert_ind, [tx_ind, stream_ind],\n                       batch_dims=1, axis=0)\n\n        # Insert the pilot symbols into the gather indices\n        tf.tensor_scatter_nd_update(x, c, pilots)\n\n        # Pilots are normalized after insertion to ensure that also the\n        # last position get updated.\n        if self.normalize:\n            x = tf.transpose(x, perm=[2, 3, 0, 1])\n            x = tf.cast(x, tf.complex128)  # promote to tf.complex128 for large signals\n            x = tf.signal.normalize(x, axis=0)\n            x = tf.cast(x, self._dtype)\n            x = tf.transpose(x, perm=[2, 3, 0, 1])\n\n        resource_grid._pilot_pattern = x # pylint: disable=protected-access\n\n    @property\n    def mask(self):\n        r\"\"\"Mask of the pilot pattern\n\n        The mask is equal to :func:`~sionna.ofdm.ResourceGrid.pilot_mask`\n        and can be used to ensure that the pilots are assigned to a\n        :class:`~sionna.ofdm.ResourceGrid` according to a\n        sparsity pattern.\n        \"\"\"\n        return self._mask\n\n    @mask.setter\n    def mask(self, mask):\n        if mask is None:\n            self._mask = None\n            self._num_pilots = 0\n            return\n\n        # Reshape the mask to the correct shape\n        mask = tf.cast(mask, tf.bool)\n        shape = mask.shape\n        new_shape = [shape[-3], shape[-2], shape[-1]]\n        mask = tf.reshape(mask, new_shape)\n\n        assert mask.shape[-3]<mask.shape[-1], \\\n                \"All resource elements cannot be pilots.\"\n\n        # Update the internal parameters\n        self._mask = mask\n        self._check_pilots()\n        self._generate_index_list()\n\n    @property\n    def pilots(self):\n        r\"\"\"The possibly normalized tensor of pilot symbols.\n\n        Note that if the :meth:`~.pilots` property is assigned to, the\n        pilots will be normalized to have unit average energy across the\n        last dimension. This normalization is applied after new values for\n        the pilots have been set. If this is not the desired behavior,\n        turn normalization off.\n        \"\"\"\n        return self._pilots\n\n    @pilots.setter\n    def pilots(self, pilots):\n        self._pilots = pilots\n        self._check_pilots() # check new pilots for consistency\n        self._generate_index_list()\n\n    @property\n    def pilot_ind(self):\n        \"\"\"Returns the indices of the pilot symbols.\"\"\"\n        return self._pilot_ind\n\n    @property\n    def pilot_lengths(self):\n        \"\"\"Returns the number of pilot symbols per transmit stream.\"\"\"\n        return self._pilot_lengths\n\n    @property\n    def pilot_ind(self):\n        \"\"\"Returns the gather indices for the full sequence of pilots.\n\n        The indices can be used to gather a sequence of flattened\n        resource grids  with:\n        ``rg = tf.gather(., pilot_ind, batch_dims=2)``\n\n        Outputs\n        -------\n        : [num_tx, num_streams_per_tx, num_pilots]\n            The gather indices.\n        \"\"\"\n        return self._pilot_ind\n\n    @property\n    def trainable(self):\n        \"\"\"Indicates if the pilots are trainable or not.\"\"\"\n        return self._trainable\n\n    @trainable.setter\n    def trainable(self, value):\n        if not isinstance(value, bool):\n            raise TypeError(\"trainable should be bool\")\n        self._trainable = value\n        if hasattr(self, \"_pilots\") and self._pilots is not None:\n            self._check_pilots() # pilots might need to be re-scaled\n\n    @property\n    def normalize(self):\n        \"\"\"Indicates if the pilots are normalized or not.\"\"\"\n        return self._normalize\n\n    @normalize.setter\n    def normalize(self, value):\n        if not isinstance(value, bool):\n            raise TypeError(\"normalize should be bool\")\n        self._normalize = value\n        if self._normalize and hasattr(self, \"_pilots\") and \\\n                                                    self._pilots is not None:\n            self._check_pilots() # pilots might need to be re-scaled\n\n    @property\n    def num_pilot_symbols(self):\n        \"\"\"The total number of pilot symbols.\"\"\"\n        return self.pilot_ind.shape[-1]\n\n    @property\n    def gather_indices(self):\n        r\"\"\"Gather indices to sample `num_pilot_symbols` from a flattened\n\n        resource grid.\n\n        The indices can be used to sample  for  `num_pilot_symbols`  from\n        a flattened resource grid:\n        \u02d9rg = tf.gather(., gather_indices)``\n\n        where ``rg`` is the flattened resource grid.\n\n        Output\n        ------\n        : [num_tx, num_streams_per_tx, num_pilots]\n            The gather indices.\n        \"\"\"\n        return self._gather_indices\n\n    @property\n    def num_trainable_pilot_symbols(self):\n        \"\"\"The number of  trainable pilot symbols.\"\"\"\n        return self._num_trainable_pilot_symbols\n\n    @property\n    def num_data_symbols(self):\n        r\"\"\"Number of  data symbols per transmit stream.\n\n        This is a derived property and is mainly provided\n        to enable compatibility with the :class:`~sionna.ofdm.ResourceGrid`.\n        \"\"\"\n        return self._num_data_symbols\n\n    @property\n    def num_effective_subcarriers(self):\n        r\"\"\"Number of effective subcarriers.\n\n        This is a derived property and is mainly provided\n        to enable compatibility with the :class:`~sionna.ofdm.ResourceGrid`.\n        \"\"\"\n        return self._num_effective_subcarriers\n\n    @property\n    def num_ofdm_symbols(self):\n        r\"\"\"Number of  OFDM symbols.\n\n        This is a derived property and is mainly provided\n        to enable compatibility with the :class:`~sionna.ofdm.ResourceGrid`.\n        \"\"\"\n        return self._num_ofdm_symbols\n\n    def _check_pilots(self):\n        \"\"\"Check that mask and pilots have the same shape and pilots are\n        trainable.\"\"\"\n        if self._pilots is not None:\n            self._pilots = tf.cast(self._pilots, self._dtype)\n            is_trainable = isinstance(self._pilots, tf.Variable) \\\n                                                    and self.trainable\n            if self._pilots.shape[0:2] != self._mask.shape[0:2] :\n                msg = \"pilots must have shape [num_tx, num_streams_per_tx, n], \" \\\n                f\"got {_pilots.shape}\"\n                raise ValueError(msg)\n            elif not is_trainable and \\\n                self._pilots.shape[2]<self._mask.shape[2]:\n                msg = \"pilots last dimension must be >=num_pilot_symbols\"\n                raise ValueError(msg)\n            elif is_trainable and \\\n                self._pilots.shape[2]< self._mask.shape[2] and \\\n                not self._trainable:\n                msg = \"pilots last dimension must be >=num_pilot_symbols\"\n                raise ValueError(msg)\n\n    def _generate_index_list(self):\n        \"\"\"Generate for each tx and stream the indices of the pilot symbols.\"\"\"\n\n        self._pilot_ind = tf.squeeze(tf.where(flatten_last_dims(self.mask)),\n                                      axis=-1)\n\n        # We store also the lengths of the indices of the individual streams\n        # This is mainly done to sample from different streams in case\n        # a stream is inactive.\n        # The property pilot_ind should be used to get the indices.\n        z = tf.zeros(1, dtype=tf.int32)\n        pilot_lengths = tf.math.add(z,  tf.reduce_sum(self.mask, axis=-1))\n        self._pilot_lengths = tf.squeeze(tf.tile(pilot_lengths, [1, 1]),\n                                          axis=-1)\n        pilot_ind =  tf.squeeze(tf.zeros_like(self.mask, dtype=tf.int32) +  z)\n        flag = False\n        for i in range(self._mask.shape[2]):\n            m = self._mask[:,:,i]\n            p = tf.where(m, pilot_ind, - m.shape[1] - 1)\n            p = tf.cast(tf.math.maximum(p,0), tf.int32)  # set background to max\n            if flag:\n                pilot_ind = tf.concat([pilot_ind, p], axis=-1)\n            else:\n                pilot_ind = p\n            flag = True\n        self._pilot_ind = tf.squeeze(pilot_ind) # remove empty dims\n\n        # 0 or -1 indicate that a stream is inactive. We remove these here.\n        active_ind = tf.where(self._pilot_lengths>0, self._pilot_ind, 0)\n        self._pilot_ind = active_ind\n\n        # gather indices to sample  `num_pilot_symbols` from a flattened\n        # rg.\n        gather_ind = tf.squeeze(tf.where(flatten_last_dims(self._pilot_ind))),\n        self._num_trainable_pilot_symbols = tf.shape(gather_ind)[0]  # number of indices to gather\n        self._gather_indices = gather_ind   # store indices for gather\n\n        # Number of pilot symbols for the pilot.\n        self._num_pilots =  self._pilots.shape[2]\n\n        # Number of  data symbols.\n        self._num_data_symbols =  self.mask.shape[2] - self._num_pilots\n\n        # Number of effective subcarriers.\n        self._num_effective_subcarriers = self._pilots.shape[3]\n\n        # Number of OFDM symbols.\n        self._num_ofdm_symbols = self._pilots.shape[2]\n\ndef show(tx_ind=None, stream_ind=None, show_pilot_ind=False):\n    \"\"\"show(tx_ind=None, stream_ind=None, show_pilot_ind=False)\n\n    Visualizes the pilot patterns for some transmitters and streams.\n\n    Input\n    ------\n    tx_ind : list, int\n        Indicates the indices of transmitters to be included. Defaults to\n        None (i.e., all transmitters included).\n\n    stream_ind : list, int\n        Indicates the indices of streams to be included. Defaults to None\n        (i.e., all streams included).\n\n    show_pilot_ind : bool\n        Indicates if the indices of the pilot symbols should be shown.\n        Defaults to False.\n    \"\"\"\n    if tx_ind is not None:\n        if not isinstance(tx_ind, int):\n            raise TypeError(\"tx_ind must be a list of ints.\")\n        if tx_ind<0 or tx_ind>=self.num_tx:\n            raise ValueError(\"Invalid tx_ind.\")\n    else:\n        tx_ind = list(range(self._mask.shape[0])) # All transmitters\n    if stream_ind is not None:\n        if not isinstance(stream_ind, int):\n            raise TypeError(\"stream_ind must be a list of ints.\")\n        if stream_ind<0 or stream_ind>=self.num_streams_per_tx:\n            raise ValueError(\"Invalid stream_ind.\")\n    else:\n        stream_ind = list(range(self._mask.shape[1])) # All streams\n    c = tf.gather(self.pilot_ind, [tx_ind, stream_ind], batch_dims=1, axis=0)\n    fig, ax = plt.subplots(figsize=(16, 4))\n    ax.set_prop_cycle(plt.rcParams['axes.prop_cycle'])\n    tx_ind = 0\n    for ind in c:\n        p = self._pilots.numpy()[ind]\n        ax.scatter(range(p.shape[0]), np.real(p), label=f\"Tx {tx_ind}, Stream {ind}\")\n        tx_ind += 1\n    plt.xlabel(\"Pilot index\")\n    plt.ylabel(\"Pilot value\")\n    plt.grid()\n    plt.legend()\n\n    if show_pilot_ind:\n        fig, ax = plt.subplots(figsize=(16, 4))\n        plt.plot(np.real(c.flatten()))\n        ax.plot(np.imag(c.flatten()))\n        plt.xlabel(\"Pilot index\")\n        plt.ylabel(\"Pilot value\")\n        plt.grid()\n\n\n```"
"To import the necessary packages for using Sionna, including TensorFlow and NumPy, follow these steps:\n\n1. Firstly, ensure that you have the necessary packages installed in your Python environment. For Sionna, you can refer to their [installation guide](https://nvlabs.github.io/sionna/installation.html).\n\n2. Use `try` and `except` blocks to check if the packages are already installed. If they are not, initiate the installation process using pip. Here is how you can do it for Sionna, TensorFlow, and NumPy:\n\n```python\ntry:\n    import sionna as sn\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna as sn\n\ntry:\n    import tensorflow as tf\nexcept ImportError as e:\n    # Install TensorFlow if package is not already installed\n    import os\n    os.system(\"pip install tensorflow\")\n    import tensorflow as tf\n\ntry:\n    import numpy as np\nexcept ImportError as e:\n    # Install Numpy if package is not already installed\n    import os\n    os.system(\"pip install numpy\")\n    import numpy as np\n```\n\n3. After the packages are installed, the `import` command is used to load them into the Python script for use. For Sionna, since it was loaded into a module named `sn`, you would use `sn` for referencing its components. For TensorFlow and NumPy, you can directly import them using `import tensorflow as tf` and `import numpy as np` respectively.\n\n4. Ensure that the packages are installed in the correct Python environment. If you are using a Jupyter notebook and the packages are installed in a different environment, you may need to specify the environment in the `sys.path` to locate the packages."
"Params: \n\n- `pilot_pattern`: An instance of `PilotPattern`.\n- `time_avg` (bool): If enabled, measurements will be averaged across OFDM symbols (i.e., time). This is useful for channels that do not vary substantially over the duration of an OFDM frame. Defaults to `False`.\n\nInput: \n\n- `h_hat`: Channel estimates for the pilot-carrying resource elements.\n- `err_var`: Channel estimation error variances for the pilot-carrying resource elements.\n\nOutput: \n\n- `h_hat`: Channel estimates across the entire resource grid for all transmitters and streams.\n- `err_var`: Channel estimation error variances across the entire resource grid for all transmitters and streams. \n\nRaises:\n\n- `AssertionError`: If the `pilot_pattern` is not an instance of `PilotPattern`.\n- `ValueError`: If the number of pilot symbols is not the same for all transmitters."
"\n\nHere is the detailed information of SymbolLogits2Moments:   \n  \nComputes the mean and variance of a constellation from logits (unnormalized log-probabilities) on the constellation points.\n\nMore precisely, given a constellation $\\mathcal{C} = \\left[ c_0,\\dots,c_{N-1} \\right]$ of size $N$, this layer computes the mean and variance according to\n\n$$\n\\begin{align}\n    \\mu &= \\sum_{n = 0}^{N-1} c_n \\Pr \\left(c_n \\lvert \\mathbf{\\ell} \\right)\\\\\n    \\nu &= \\sum_{n = 0}^{N-1} \\left( c_n - \\mu \\right)^2 \\Pr \\left(c_n \\lvert \\mathbf{\\ell} \\right)\n\\end{align}\n$$\n\nwhere $\\mathbf{\\ell} = \\left[ \\ell_0, \\dots, \\ell_{N-1} \\right]$ are the logits, and\n\n$$\n\\Pr \\left(c_n \\lvert \\mathbf{\\ell} \\right) = \\frac{\\exp \\left( \\ell_n \\right)}{\\sum_{i=0}^{N-1} \\exp \\left( \\ell_i \\right) }.\n$$\n\n**Note:**\nIt is assumed that the logits are for a (trainable) variable or tensor. Hence, the\ndefined operation is differentiable. This is typically used within a\ncomputation graph with its gradient used for training.\n\n### Parameters\n\n- `constellation_type` (One of \"qam\", \"pam\", \"custom\", str) - For custom, an instance of `Constellation` must be provided.\n  - `num_bits_per_symbol` (int) - The number of bits per constellation symbol, e.g., 4 for QAM16. Only required for `constellation_type` in [qam, pam].\n  - `constellation` (Constellation or None) - An instance of `Constellation` or None. In the latter case, `constellation_type` and `num_bits_per_symbol` must be provided.\n  - `dtype` (One of [tf.float32, tf.float64], tf.DType) - The dtype for the input and output. Defaults to tf.float32.\n\n### Input\n\n- `logits` ([..., n, num_points], tf.float) - Logits on constellation points.\n\n### Output\n\n- `mean` ([..., n], tf.float) - Mean of the constellation.\n- `var` ([..., n], tf.float) - Variance of the constellation.\n\n**Property**\n\n- `samples_per_symbol` (int) - The number of samples per symbol, i.e, the oversampling factor.\n\n**Method: `samples(logits, seed=None)`**\n\nDraw samples from the constellation.\n  \nInput\n    \u2022 `logits` ([..., n, num_points], tf.float) - Logits on constellation points.\n    \u2022 `seed` (int or None) - The seed for the random generator. None leads to random seeds.\n\nOutput\n    \u2022 ([..., n, num_samples], tf.float) - A tensor of (potentially) random symbol samples.\n   \nsource code: [SymbolLogits2Moments](https://nvlabs.github.io/sionna/_modules/sionna/mapping.html#SymbolLogits2Moments)"
"This function is used in the context of wireless propagation modelling to compute the frequency response of the channel at given frequencies, based on a given Channel Impulse Response (CIR). \n\nAn CIR is a response, either in time or frequency domain, that characterizes the response of a communication channel to an input signal. This response includes the effects of propagation delay, multipath reflection, and scattering.\n\nThe CIR is represented with amplitude coefficients (a) and time delay coefficients (tau) for each path and for each link of the system.\n\nThe function computes the frequency response using the given complex-valued path coefficients (a), real-valued path delay (tau), and for the specified `frequencies`. The `normalize` parameter, if set to `True`, causes the channel to be normalized over the resource grid to ensure a unit average energy per resource element.\n\nThe function returns the channel frequency responses, i.e., for each pair of transmitter and receiver, for each antenna of the receiver, and for each stream between transmitter and receiver, the channel frequency responses over the specified frequencies and for each time step. These channel frequency responses are represented as complex numbers, and they characterize the wireless channel as seen by the Transmitters and Receivers at the considered time step."
"The given class `EPDetector` is a part of the 'sionna.mimo' module and is used in MIMO Expectation Propagation (EP) detection. This method is utilized to compute Log-Likelihood Ratios (LLRs) or hard-decisions for bits or symbols in a MIMO system. \n\nParameters of the Class:\nThe `EPDetector` class is instantiated with these set of parameters:\n- **output**: (String) Describes the output type of the detector. It can either be LLRs for bits or hard-decisions for bits or symbols. This is controlled via the configuration of the detector.\n- **num_bits_per_symbol**: (Integer) The number of bits per constellation symbol, e.g., 4 for QAM16. \n- **hard_out**: (Boolean) If true, the detector computes hard-decided bit values or constellation point indices instead of soft-values. \n- **l**: (Integer) Number of iterations during the computation. \n- **beta**: (Float) The update smoothing parameter. \n- **dtype**: (tf.DType) The datatype used for internal calculations and the output. \n\nSignificance in Simulations:\nIn terms of its significance in simulations, this particular class and method are foremost used for MIMO detections in simulations. Sionna offers the EP-based detector as a layer for Keras models, hence it's convenient to use within the model. It supports both differentiabledetection to compute LLRs, and classical, non-differentiabledetection for hard-decisions. \n\nIt's important to note that this method implements the MIMO Expectation Propagation (EPEP) detector. The EPEP works iteratively by 1) fitting a multi-dimensional EP distribution to the received signals, and 2) applying a channel reliability masking, which then converts the estimated distribution into an equivalent Gaussian one. This parameter also works on the principle of updating smoothing, where it applies a coefficient to all new values before they are used. This can be used to control the impact of the new value on the existing values. \n\nThis method is computationally intensive and generally used for uplink scenarios with a large number of users and antennas. It offers a novel solution for the exact LLRs in MIMO detection, standing apart from many traditional detectors."
"To set up and run an instance of the `EDFA` class in the Sionna package, you would typically follow these steps:\n\n1. First, you need to import the appropriate class. In this case, you would use the following line of Python code:\n\n    ```python\n    from sionna.channel.optical.EDFA import EDFA\n    ```\n\n2. You would then create an instance of the `EDFA` class, specifying the required parameters. The EDFA class parameters are set according to the Amplifier Parameters in the Optical Amplification Module of the Sionna package documentation. These parameters typically include values for the gain `G`, the noise figure `F`, the carrier frequency `f_c`, the bit rate `B`, and others as needed. You can either set these to a specific value or to `None` to use the default. \n\n    Here is an example of setting up the `EDFA` instance with the default values:\n\n    ```python\n    e = EDFA(\n        g=5.0,\n        f=4.0,\n        f_c=193.55e12,\n        dt=1.0e-12,\n        with_dual_polarization=False,\n        dtype=tf.complex64\n    )\n    ```\n\n3. Once your `EDFA` instance is set up, you can then use it to amplify an input signal. The input to the `EDFA` instance would typically be a tensor (e.g., `x`), and the output would be the amplified signal tensor `y`, along with a scalar `G_act` representing the actual gain used, which is generally required for evaluating the amplifiers performance. \n\n    An example of running the `EDFA` instance with an input signal `x` would be:\n\n    ```python\n    # Assume that `x` is a tensor containing the input signal to be amplified and `G` is the gain.\n    y, g_act = e(x, g)\n    ```\n\nPlease note that, depending on your specific use case, you might need to adjust the parameters or use the `EDFA` class in a different way. The above example is a general case, and you should always refer to the specific module's documentation for any particular considerations."
"For utilizing the `render_to_file()` function within the `Scene` class, the following method should be applied to an instantiated scene object. This will subsequently output a rendering of the scene to a file.\n\nInstance Method Call:\n```python\nscene = Scene()\nscene.render_to_file(filename = \"my_file_path.png\")\n```\n\nThe returned output could be a path or a definition of a `RenderedView`, representing the rendered view of the scene.\n\nThe parameters for `render_to_file()` are described as follows:\n\n- `filename` (str): The name of the output file, for example, \"my_file_path.png\". It can include a relative or absolute path.\n- `cam` ([`Camera`], optional): The camera object to use for rendering. If not provided, the active camera within the scene is used.\n- `coverage_map` ([`CoverageMap`], optional): An instance of `CoverageMap` to overlay on the rendering, or `None` (default) if no coverage map is to be used.\n- `cm_tx` (int|str): The index or name of the transmitter for which to display the coverage information. This parameter is only used if `coverage_map` is not `None`.\n- `cm_db_scale` (bool): If set to `True`, the color scale for the coverage map is logarithmic, i.e., in dB. Defaults to `True`.\n- `cm_vmin`, `cm_vmax` (float): The minimum and maximum values in the colormap for path gains. These values are in dB if `cm_db_scale` is `True`, or in linear scale if it is `False`. If not specified, they default to the minimum and maximum path gain value in the coverage map.\n- `dtype` (tf.DType): The datatype to use for the rendering. Defaults to `tf.float32`.\n- `show_paths` (bool): If set to `True`, paths should be shown. Defaults to `True`.\n- `show_devices` (bool): If set to `True`, radio devices are shown. Defaults to `True`.\n- `show_orientations` (bool): If set to `True`, the orientations of objects are shown. Defaults to `False`.\n- `show_desired_ind` (int): The index of the desired visualization in multi-dictionary renderings. Defaults to `-1` (i.e., not to be shown).\n- `num_samples` (int): The number of rays thrown per pixel. Defaults to 512.\n- `resolution` ([,2], int): The size of the rendered figure. Defaults to [655 (width), 500 (height)].\n- `fov` (float): The field of view in degrees. Defaults to 45.0\u00b0.\n\nOutput: \n- `str`: The absolute path of the file to which the view has been rendered.\n- `RenderedView`: An object that represents the rendered view data. This is returned if no filename is provided."
"Answer:\n  \nThe function `wrap_angle_0_360` is a member of the 'sionna.channel.utils' submodule and it is meant to serve as a utility function for wrapping an angle to be within the range (0, 360). This means that if the angle is initially less than 0, the function will add 360 to it, repeatedly, until the result falls within the range of 0 to 360. Similarly, if the angle is initially within the range, the function will just return the angle as it is. The input 'angle' is expected to be a tensor, and the output will also be a tensor of the same shape as the input. \n\nHere is how you might use it in a programming scenario:\n\n```python\nfrom sionna.channel.utils import wrap_angle_0_360\nimport tensorflow as tf\n\n# Create a tensor of angles\nangles = tf.constant([-90, 90, 450])\n\n# Use the function to wrap the angles to (0, 360)\nwrapped_angles = wrap_angle_0_360(angles)\n\nprint(wrapped_angles)\n```\nThis will print the angles wrapped to be within the range of (0, 360).\n \nIn case, the input angle is in negative and positive exceeding range (i.e < -360 , > 360) , the function will throw an error as the valid input is specified only for the range (0, 360).\n\nPlease make sure to pass a proper tensor of angles as input to the function, ensuring it is within the defined range (0, 360) to get the output. The function will raise an error if the input is not within the specified range or not a proper tensor."
"Selected source code link:  [sionna.channel.ChannelModel](https://nvlabs.github.io/sionna/_modules/sionna/channel/channel_model.html#ChannelModel)  \n```python\nclass ChannelModel(ABC):\n    r\"\"\"Abstract class that defines an interface for channel models.\n\n    Any channel model which generates channel impulse responses\n    must implement this interface.\n    All the channel models available in Sionna, such as\n    :class:`~sionna.channel.tr38901.UMi`, implement this interface.\n\n    Input\n    -----\n    batch_size : int\n        Batch size\n\n    Output\n    ------\n    a : [batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps], tf.complex\n        Path coefficients\n\n    tau : [batch size, num_rx, num_tx, num_paths], tf.float\n        Path delays [s]\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def delay_spread(self):\n        r\"\"\"Delay spread [s]\"\"\"\n        pass\n\n    @delay_spread.deleter\n    def delay_spread(self):\n        del self._delay_spread\n\n    @property\n    @abstractmethod\n    def mean_a(self):\n        \"\"\"Mean power per 'a' coefficient [linear]\"\"\"\n        pass\n\n    @mean_a.deleter\n    def mean_a(self):\n        del self._mean_a\n```"
"From the given context, the `render_to_file` function belongs to the `sionna.rt.Scene` class which is used for radio propagation simulation. \n\nHere is the detailed information of the function:\n\n-  **Method: 'render_to_file'  - Renders the scene from the viewpoint of a camera or the interactive viewer and saves the resulting image**\n   `camera` (str | Camera) -  The name or instance of a `Camera`. If an interactive viewer was opened with `preview()`, set to 'preview' to use its viewpoint.\n   `filename` (str) -  Filename for saving the rendered image. e.g. 'my_scene.png'\n   `paths` (Paths | None) -  Simulated paths generated by `compute_paths()` or None . If None, only the scene is rendered. Defaults to None.\n   `show_paths` (bool) - If `paths` is not None, shows the paths. Defaults to True.\n   `show_devices` (bool) - If `paths` is not None, shows the radio devices. Defaults to True.\n   `coverage_map` (CoverageMap | None) - An optional `CoverageMap` to overlay in the scene for visualization. Defaults to None.\n   `cm_tx` (int | str) - When `coverage_map` is specified, controls which of the transmitters to display the coverage map for. Either the transmitter's name or index can be given. Defaults to 0.\n   `cm_db_scale` (bool) -  Use logarithmic scale for coverage map visualization. Defaults to True.\n   `cm_vmin, cm_vmax` (float | None) - For coverage map visualization, defines the range of path gains that the colormap covers. These parameters should be provided in dB if `cm_db_scale` is set to True, or in linear scale otherwise. If set to None, then covers the complete range. Defaults to None.\n   `num_samples` (int) - Number of rays thrown per pixel. Defaults to 512.\n   `resolution` ([2], int) - Size of the rendered figure. Defaults to [655, 500].\n   `fov` (float) - Field of view in degrees. Defaults to 45.\n\nPlease use the detailed information to get the method's functionality right in the implementation. "
"To configure DeepMIMO for use with Sionna, you need to set up the DeepMIMO parameters as per your requirements. Here are the parameters that need to be configured in the provided context:\n\n1. `'dataset_folder'`: This parameter should be set to the path where the DeepMIMO scenario files are stored.\n\n2. `'scenario'`: This parameter should be set to the specific DeepMIMO scenario you wish to use. In the given context, it is set to 'O1_60'.\n\n3. `'num_paths'`: This parameter should be set to the maximum number of paths a channel impulse response should consist of. \n\n4. `'active_BS'`: This parameter should be set to an array that contains the indices of the base stations that are to be included in the dataset.\n\n5. `'user_row_first'` and `'user_row_last'`: These parameters dictate the first and last row of user locations for which the channels are to be generated.\n\n6. Antenna parameters such as `'bs_antenna'` and `'ue_antenna'` should be set as per your requirements. For instance, you might want to set the shape of the antenna arrays in the x, y, and z dimensions.\n\n7. The `'OFDM_channels'` parameter should be set according to the type of channels you want to generate. In this case, it is set to 0, which means the channels will be generated in the time domain.\n\nAfter configuring these parameters, a DeepMIMO dataset is generated using these settings. The provided context shows code to generate a DeepMIMO dataset using the set parameters.\n\nPlease note that the configuration might vary depending on your specific use case and the Sionna API requirements."
"Answer:\nTo use the Interleaver module in Sionna package, follow the steps below:\n\n1. Set up the system and import the necessary modules using the following code:\n\n```python\n    import os\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Use CPU\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n    import sionna\n    from sionna.fec.interleaving import  RandomInterleaver\n    from sionna.fec.interleaving import Deinterleaver\n    import tensorflow as tf\n```\n\n2. Initialize the layers for the system\n\n```python\n    interleave = RandomInterleaver(seed=1234,  # an explicit seed can be provided\n                               keep_batch_constant=False,  # if True, all samples in the batch are permuted with the same pattern\n                               axis=-1)  # axis which shall be permuted\n    deinterleaver = Deinterleaver(interleaver=interleaver)  # connect interleaver and deinterleaver\n```\n\n3. If 'c' is your tensor, permute the tensor using the following code:\n\n```python\n    c_int = interleave(c)\n```\n   This will return the permuted tensor 'c_int'.\n\n4. To reconstruct the original order, use the deinterleaver with the permuted tensor. If 'c_deint' is the tensor, obtain the deinterleaved tensor using the following code:\n\n```python\n    c_deint = deinterleaver(c_int)\n```\n   This will return the original tensor 'c'.\n\n5. You can also provide an explicit seed if a new random seed should be used for each call. In this case, the system will use a random seed defined by the explicit seed for the permutation.\n\n\nRemember to replace 'c' with your tensor while calling the function.\n\nNote: The code snippets provided in the context are based on the assumption that the tensors are already created. Please make sure to create the tensors before using these code snippets."
"The `MMSEPICDetector` function is important as it implements an Expectation Maximization (EM) algorithm for MIMO (Multiple-Input, Multiple-Output) detection. This algorithm works with prior knowledge of the transmitted bits or constellation symbols to compute minimum mean-square error (MMSE) symbol estimates or bit log-likelihood ratios (LLRs). The algorithm's effectiveness comes from its simulation accuracy in both high and low signal-to-noise ratios (SNRs).\n\nNote that the function is based on an existing algorithm, so it's crucial to check if this one meets your exact needs, or if adjustments can be made.\n\nUsage of this function pertains to fields that would apply an EM algorithm for MIMO detection. This might be used in the more advanced settings for wireless communication or signal processing.\n\nThe output will depend on the arguments, but generally, the function will return a batch of LLRs or hard-decisions for every bit of every stream if `output` equals `\"bit\"`. If `output` equals `\"symbol\"`, it will return either batch of hard-decisions or logits for constellation symbols for every stream. Hard-decisions imply the chosen symbol. This function supports both soft- and hard-decisions. The symbol logits are essentially the log-likelihoods on the constellation symbol, and can be transformed into LLRs (for the bits) with the `LogitsToLLR` function in the case of soft-decision.\n\nKeep in mind that parameters such as `output`, `hard_out`, `constellation`, etc., must be set as per the specifications of your system or data. If no specific values are provided, the function's defaults will be used. This function also uses TensorFlow, so if it's to be used in Graph mode with XLA, the `xla_compat` parameter must be set to true."
"   \n  \nHere is the definition of RaisedCosineFilter: sionna.signal.RaisedCosineFilter(beta, span_in_symbols, samples_per_symbol, window=None, normalize=True, trainable=False, dtype=tf.float32, **kwargs)  \n    \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/signal/filter.html#RaisedCosineFilter)  \n\n```python\nclass RaisedCosineFilter(Filter):\n    # pylint: disable=line-too-long\n    r\"\"\"RaisedCosineFilter(beta, span_in_symbols, samples_per_symbol, window=None, normalize=True, trainable=False, dtype=tf.float32, **kwargs)\n\n    Layer for applying a raised-cosine filter of ``length`` K\n    to an input ``x`` of length N.\n\n    The raised-cosine filter is defined by\n\n    .. math::\n        h_{\\text{RC}}(t; \\beta, T) = \\begin{cases}\n                        \\frac{1}{T} \\left(1 + \\beta\\left( \\frac{4}{\\pi}-1\\right) \\right), & \\text{if }t = 0\\\\\n                        \\frac{\\beta}{T} \\left( \\frac{1+\\frac{2}{\\pi}\\sin\\left(\\frac{\\pi t}{\\beta T}\\right) }{1-\\frac{4}{\\pi}\\sin\\left(\\frac{\\pi t}{\\beta T}\\right) \\right) \\right), & \\text{if }t = \\pm T/4\\\\\n                        0, & \\text{otherwise}\n                      \\end{cases}\n\n    where :math:`\\beta` is the roll-off factor and :math:`T` the symbol duration.\n\n    The filter length K is equal to the filter span in symbols (``span_in_symbols``)\n    multiplied by the oversampling factor (``samples_per_symbol``).\n    If this product is even, a 1 will be added.\n\n    The filter is applied through discrete convolution.\n\n    An optional windowing function ``window`` can be applied to the filter.\n\n    The `dtype` of the output is `tf.float` if both ``x`` and the filter coefficients are `tf.float`.\n    Otherwise, it is `tf.complex`.\n\n    Three padding modes are available for applying the filter:\n\n    *   \"full\" (default): Returns the convolution at each point of overlap between ``x`` and the filter.\n        The length of the output is N + K - 1. Zero-padding of the input ``x`` is performed to\n        compute the convolution at the borders.\n    *   \"same\": Returns an output of the same length as the input ``x``. The convolution is computed such\n        that the coefficients of the input ``x`` are centered on the coefficient of the filter with index\n        (K-1)/2. Zero-padding of the input signal is performed to compute the convolution at the borders.\n    *   \"valid\": Returns the convolution only at points where ``x`` and the filter completely overlap.\n        The length of the output is N - K + 1.\n\n    Parameters\n    ----------\n    beta : float\n        Roll-off factor in the range :math:`[0,1]`.\n\n    span_in_symbols : Int\n        Filter span as measured by the number of symbols.\n\n    samples_per_symbol : Int\n        Number of samples per symbol, i.e., the oversampling factor.\n\n    window: Window or string ([\"hann\", \"hamming\", \"blackman\"])\n        Instance of :class:`~sionna.signal.Window` that is applied to the filter coefficients.\n        Alternatively, a string indicating the window name can be provided. In this case,\n        the chosen window will be instantiated with the default parameters. Custom windows\n        must be provided as instance.\n        The window is applied after filter length normalization.\n\n    normalize : bool\n        If `True`, the filter is normalized to have unit power.\n        Defaults to `True`.\n\n    trainable : bool\n        If `True`, the filter coefficients are trainable variables.\n        Defaults to `False`.\n\n    dtype : tf.DType\n        The `dtype` of the filter coefficients.\n        Defaults to `tf.float32`.\n\n    Input\n    -----\n    x : [..., N], tf.complex or tf.float\n        The input to which the filter is applied.\n        The filter is applied along the last dimension.\n\n    padding : string ([\"full\", \"valid\", \"same\"])\n        Padding mode for convolving ``x`` and the filter.\n        Must be one of \"full\", \"valid\", or \"same\". Case insensitive.\n        Defaults to \"full\".\n\n    conjugate : bool\n        If `True`, the complex conjugate of the filter is applied.\n        Defaults to `False`.\n\n    Output\n    ------\n    y : [...,M], tf.complex or tf.float\n        Filtered input.\n        It is `tf.float` only if both ``x`` and the filter are `tf.float`.\n        It is `tf.complex` otherwise.\n        The length M depends on the ``padding``.\n    \"\"\"\n    def __init__(self, beta, span_in_symbols, samples_per_symbol, window=None, normalize=True, trainable=False, dtype=tf.float32, **kwargs):\n        super().__init__(span_in_symbols, samples_per_symbol, window, normalize, trainable, dtype, **kwargs)\n        self._beta = beta\n        # Force the coefficient to be trainable\n        super().set_trainable(True)\n\n    @property\n    def beta(self):\n        \"\"\"Roll-off factor\"\"\"\n        return self._beta\n\n    @property\n    def name(self):\n        return \"raised_cosine_\" + super().name\n\n    @property\n    def _coefficients_source(self):\n        h_rc = self._raised_cosine(self.sampling_times,\n                                self.beta)\n        h_rc = tf.constant(h_rc, self.dtype)\n        return h_rc\n```   \n  "
"The PAM2QAM class in the sionna.mapping module is a utility class that helps with the transformation of PAM (Pulse Amplitude Modulation) symbol indices or logits to QAM (Quadrature Amplitude Modulation) symbol indices or logits. In modulation, a constellation diagram is used to visualize the signal points in signal space where each point represents a state or symbol for the modulation scheme.\n\nFor two PAM constellation symbol indices or logits, corresponding to the real and imaginary components of a QAM constellation, this class's function computes the QAM symbol index or logits. This transformation allows the reuse of PAM modulation components for QAM modulation.\n\nThe class takes the number of bits per QAM symbol as one of its initialization parameters. The transformation from PAM to QAM involves some bit shaping processes which are required for QAM modulation. \n\nAs a result, the PAM2QAM class contributes to the efficient and effective handling of modulation tasks by providing a clean and reusable way to manage the transformation between PAM and QAM constellations. This might be particularly useful in scenarios which require dynamically changing modulation schemes or involve the nesting of PAM and QAM constellations."
"To configure `StreamManagement` for an uplink scenario using Python code, you need to define the following variables:\n\n- `num_tx` (Number of transmitters): Set this equal to the number of User Equipments (UEs) or mobile devices. Each UE or mobile device is assumed to have a single antenna unless otherwise specified.\n- `num_rx` (Number of receivers): This depends on whether you are simulating an uplink or downlink scenario. If you are simulating an uplink, you could set `num_rx = 1` for a single base station. If you are simulating a downlink, you might have multiple receivers, equal to the number of base stations.\n- `num_streams_per_tx` (Number of streams per transmitter): This can be any non-negative number, representing the amount of data that each transmitter can send simultaneously.\n- `rx_tx_association` (Receiver-transmitter association): This should be a binary 2D array of shape (`num_rx`, `num_tx`), where `rx_tx_association[i,j] = 1` means that Receiver `i` is associated with Transmitter `j`. In this setup, you could associate each receiver with a single transmitter.\n\nHere's how you could define these variables in Python:\n\n```python\nnum_tx = 4  # Number of transmitters (UEs)\nnum_rx = 1  # Number of receivers (BS)\nnum_streams_per_tx = 1  # Number of streams per transmitter\n# Receiver-transmitter association\n# Each receiver is associated with a single transmitter\nrx_tx_association = np.zeros([num_rx, num_tx], dtype=int)\nrx_tx_association[0, 0] = 1\n```\n\nThis configuration represents a 4x1 uplink transmission with a single UE or mobile device sending one stream to a single base station. The `rx_tx_association` array indicates that the first receiver is associated with the first transmitter. If you want to simulate multiple UEs or mobile devices, or adjust the number of streams per transmitter, you can simply change the value of `num_tx` or `num_streams_per_tx`."
"In the context of higher order modulation, constellations refer to the set of possible symbols that are used to represent bits in the process of data transmission. For example, in a 16-QAM (Quadrature Amplitude Modulation) scheme, the constellation would be the set of 16 possible combinations of bit values, where each possible combination is represented by a point in a complex plane.\n\nBit-channels, on the other hand, are the channels through which individual bits are transmitted. In the process of higher order modulation, each bit is mapped to a specific symbol in the constellation, and each of these symbols is transmitted over a separate bit-channel. For example, in a 16-QAM scheme, each bit is mapped to one of the 16 possible symbols in the constellation, and each of these symbols is transmitted over a separate bit-channel.\n\nIn the given Python context, the creation of a Model implementing the System with the ED and PDF shows the implementation of a communication system over an Additive White Gaussian Noise (AWGN) channel, using higher-order modulation (e.g., 16-QAM). The model is instantiated with the number of bits per symbol and the constellation type (qam or pam). The constellation itself is generated using the sn.constellation function. After the transmission of the symbols over the channel, the symbols are demapped to bits using the sn.Demapper layer. \n\nThe concept of bit-channels is not explicitly mentioned in this context, but it is implied in the process of demapping symbols to bits. In the demapping process, the system needs to determine which bit (or bits) is associated with each symbol. This is done using the 'decision metric', which is a measure of the reliability of the received symbol with respect to each possible bit-channels. The demapper uses these decision metrics to generate Log-Likelihood Ratios (LLRs) for each bit, which are then input to the LDPC decoder for the error correction process."
"The function in Sionna's mapping module is used to generate a PAM (Pulse Amplitude Modulation) constellation. After the PAM modulation has been completed, the signal will no longer be binary; it will contain multiple bits. This constellation is a real-valued vector where each element is a constellation point of an M-ary PAM constellation. The number of bits per symbol that the constellation carries is specified by the `num_bits_per_symbol` parameter, and this must be a positive integer. \n\nFollowing the generation of the PAM constellation, the binary nature of the bit stream is slightly departed from, and it's noted that this configuration can allow for a more efficient modulation, especially when considering higher data rates. \n\nThe PAM constellation is not necessarily normalized to have unit power; this is controlled via the `normalize` parameter which defaults to `True`. When `normalize` is set to `True`, the function will normalize the PAM constellation to have unit power. If set to `False`, no normalization will be applied. \n\nThe function ultimately outputs an array representing the PAM constellation, with the number of elements equal to 2 raised to the power of `num_bits_per_symbol`. The output type of this array is specified as `np.float32`, which denotes a NumPy float32 type. \n\nTo clarify, an M-ary PAM constellation is a constellations (in this case, a PAM constellation) that are used in modulation schemes. These are the set of allowed states of a digital symbol in the modulation scheme. In a PAM scheme, pulses are used to modify the amplitude of the signal. The constellation is the pattern of bit symbols which define points in the complex plane. \n\nPlease note that this explanation is based on the given context and specific details of the `PAM` or its binary representation are subject to change depending on the actual implementation."
"To use the \"List2LLR\" class in the Sionna package, the following assumptions must be fulfilled:\n\n1. The class \"List2LLR\" is a callable object used to perform list to Log-Likelihood Ratio (LLR) conversion on a MIMO channel output based on a list of candidate codewords and the channel symbol probabilities. \n\n2. It requires that the `list_size` and `algorithm` be provided as parameters. The `list_size` parameter is an integer that defines the size of the list, that is, the number of candidate codewords. The `algorithm` parameter, which is a string, references the algorithm used for the conversion. It can be either \"zf\" or \"mmse\", relating to the existing precoding vectors.\n\nImplementations should instantiate the class as follows, assuming one has the Sionna package installed and the appropriate components are imported:\n\n```python\nfrom sionna.mimo import List2LLR\n\nllr_converter = List2LLR(list_size=4, algorithm=\"mmse\")\n```\n\n3. The precoding matrices and the channel symbol probabilities are the input to the callable object. The channel symbol probabilities are a complex-valued tensor of shape [batch_size, num_tx, num_streams_per_tx, num_symbols]. The conversion algorithm and the list of candidate solutions determine the output type, which can be one of the following: \n - An LLR for every bit of every stream if `output` equals \"bit\", with shape [batch_size, num_tx, num_streams_per_tx, num_bits_per_symbol].\n - An LLR for the constellation symbol for every stream if `output` equals \"symbol\", with shape [batch_size, num_tx, num_streams_per_tx, num_symbols, num_bits_per_symbol].\n\n4. For the MMSE (Minimum Mean Squared Error) algorithm, assuming perfect CSI (Channel State Information), one needs to provide the function with the precoding matrices. In case of the ZF (Zero Forcing) algorithm, the conversion is based on the number of non-zero elements in the precoding matrices.\n\nPlease note that the above points are based on the provided information and context and should be adjusted according to the actual implementation and usage."
"The `MMSEPICDetector` is used for detecting symbols in an OFDM MIMO system. It wraps the functionality of the `compute_e_mse()` and `kappa_mse()` functions, and implements the MMSE-PIC (Minimum Mean Squared Error - Parallel Interference Cancellation) detection algorithm. The OFDM symbol and stream indices are provided by a `resource_grid` and `stream_management` instance, respectively.\n\nThe conditions under which the `MMSEPICDetector` is used are:\n\n1. The system is required to detect symbols in an OFDM MIMO system.\n\n2. The `detector` parameter in the `resource_grid` is set to 'mmse_pic'.\n\n3. The `stream_management` parameter is set to 'mmse_pic'.\n\n4. The `output` parameter in the `resource_grid` is set to 'bit' for computing Log Likelihood Ratios (LLRs) or to 'symbol' for hard-decided symbols.\n\nThe class takes the following parameters:\n\n- `output`: Type of output, either 'bit' for LLRs or 'symbol' for hard symbols.\n- `resource_grid`: Instance of `ResourceGrid`.\n- `stream_management`: Instance of `StreamManagement`.\n- `constellation_type`: Type of constellation, like 'qam', 'pam', or 'custom'. For custom constellations, an instance of `Constellation` must be provided.\n- `num_bits_per_symbol`: Number of bits per constellation symbol, e.g., 4 for QAM16. Required only for constellation types 'qam' and 'pam'.\n- `constellation`: Instance of `Constellation` or None. If None, `constellation_type` and `num_bits_per_symbol` must be provided.\n- `hard_out`: If true, the detector computes hard-decided bit values or constellation point indices instead of soft values. Defaults to False.\n- `dtype`: Precision used for computations, either tf.complex64 or tf.complex128. Defaults to tf.complex64.\n\nThe class can be used in Graph mode with XLA, i.e., within a function that is decorated with `@tf.function(jit_compile=True)`. The use of XLA is controlled by the `sionna.Config.xla_compat` attribute, as described in the documentation for `xla_compat`. The use of XLA must be set to true for the function using the `MMSEPICDetector`.\n\nThe use of the `MMSEPICDetector` is illustrated in the given context, where the class is instantiated as part of a resource grid to set the detector used for stream 2 to 'mmse_pic'. The `detector` parameter of the `resource_grid` is used to specify the detector."
"`QAM2PAM` is a class from the `sionna.mapping` module that transforms Quadrature Amplitude Modulation (QAM) symbol indices to PAM (Pulse Amplitude Modulation) symbol indices. For a given QAM constellation point, this function computes the indices for the two PAM constellation. The indices are for the first coordinate of the PAM modulation.\n\nThe transformation is based on the splitting of the bit sequence of the PAM modulation that is generated by the QAM modulation. As mentioned in the context, the number of bits per QAM constellation symbol (e.g., 4 for QAM16) is an input parameter provided during the instantiation of the `QAM2PAM` class.\n\nThe modulation process is not directly handled in the provided context. It's typically handled in a preceding part of the system that deals with digital communication standards. The necessary formulas and functions for the modulation process would be provided from such a precedent part, but those details are absent from this specific context. The transformation would generate indices for the two PAM constellations that are used to modulate the real and imaginary parts of the QAM modulation.\n\nPlease note that this explanation focuses on the context provided and doesn't take into account the actual implementation details of the `QAM2PAM` class, which would require access to the source code or the API documentation. The actual implementation may involve operations on tensors, as is common when dealing with transformations on digital signals, but this is not explicitly mentioned in the provided context."
"To use the Sionna's function `verify_gm_pcm` to verify that the generator matrix and parity-check matrix are orthogonal in GF(2), you need to provide the generator matrix `gm` and the parity-check matrix `pcm` as arguments. \n\nHere is how you can do it with a Python code snippet.\n\nFirst, let's assume you have already created or loaded the generator matrix `gm` and the parity-check matrix `pcm`. The details of creating or loading these matrices are not provided in the context. \n\n```python\nfrom sionna.fec.utils import verify_gm_pcm\n\n# Your code to create or load the generator matrix `gm` and `pcm`.\n# This code is just a placeholder and you need to replace it with\n# the actual code that creates or loads the matrices.\n\n# Then verify the matrices orthogonality using the function.\nresult = verify_gm_pcm(gm, pcm)\n\n# `result` now contains a boolean value that indicates whether the \n# generator matrix and parity-check matrix are orthogonal in GF(2).\n```\n\nMake sure that the `gm` and `pcm` matrices are binary matrices (consist of 0's and 1's) before passing them to the `verify_gm_pcm` function. The function will return `True` if the matrices are orthogonal (i.e., if the generator matrix is orthogonal to the parity-check matrix in GF(2)), otherwise it will return `False`.\n\nIf the matrices are orthogonal, it means that they are suitable for encoding linear block codes, which can guarantee that the code can be transposed from an encoder's perspective to a corresponding decoder's perspective, commonly known as the parity-check matrix representation. This is a fundamental property for error correction codes, which ensures efficient error correction and detection."
"from sionna.fec.linear import LinearEncoder"
"This class is designed as a layer to perform Orthogonal Frequency Division Multiplexing (OFDM) based MIMO detection, assuming prior knowledge of the transmitted data or constellation points is available.\n\nWorking with OFDM-based waveforms, the class preprocesses the provided OFDM channel resource grid and the prior information on the transmitted data to compute the input covariance matrices for the MIMO detector. The channel estimation error variance is also taken into account during this computation. Then, a MIMO detection algorithm that requires knowledge of these covariance matrices is applied to the received signals to get estimates of the transmitted data and the corresponding covariance matrices.\n\nThis method allows for advanced preprocessing of the received signals and better handling of the channel conditions, which can significantly impact the performance of the MIMO detection algorithm.\n\nIn the context of OFDM-based transmissions, the class preprocesses the received OFDM resource grid (after synchronization, timing, and frequency offset correction) and computes the channel frequency response for each subcarrier. This response, along with the prior information on the transmitted signals (or bits), is used to compute the channel covariance matrix, the Gram matrix. The class then calls the MIMO detection function using the computed Gram matrix and the received signals.\n\nThe use of this class requires a callable (e.g., a function) that implements a MIMO detection algorithm. This callable must take as input a tuple ($\\mathbf{y}, \\mathbf{h}, \\mathbf{prior}, \\mathbf{s}$) and generate either LLRs on the transmitted bits or logits on the constellation symbols.\n\nIn summary, the `OFDMDetectorWithPrior` class accomplishes the following:\n1. Transform the received OFDM resource grid and prior information on the transmitted signals into input covariance matrices for the MIMO detection algorithm.\n2. Take into account the channel estimation error variance during the above computation.\n3. Use a callable MIMO detection algorithm that requires knowledge of these covariance matrices to process the received signals and produce estimates of the transmitted data.\n\nNote: This explanation is based on the provided context and is general for a wide range of applications. The actual implementation and the details of how the class is used may vary depending on the specific requirements of your application."
"To use the \"bin2int_tf\" function to convert a binary tensor to an integer tensor, you would have to pass a binary tensor (a tensor filled with 0s and 1s) as an argument to the function. The binary representation in the tensor is considered from the most significant bit to the least significant bit.\n\nHere is the syntax to use the function:\n\n```python\nfrom sionna.fec.utils import bin2int_tf\nimport tensorflow as tf\n\n# arr is your binary tensor, for example arr = [0, 1, 1]\narr = tf.constant(arr, dtype=tf.int32)\noutput = bin2int_tf(arr)\n```\n\nThe `arr` variable contains your binary tensor, and the `bin2int_tf(arr)` is the function call that converts the binary tensor `arr` to an integer. The resulting integer tensor is stored in the `output` variable.\n\nFor example, a binary tensor like arr = [1, 0, 1, 1, 0] would be converted to an integer as per its binary representation (from most significant bit to least significant bit).\n\nPlease note that the binary tensor should consist of 0s and 1s and the tensor should be of int type. The function will output an integer tensor. \n\nEnsure that TensorFlow and the sionna library are installed in your Python environment before you run this function. Details of its installation can be found here: [https://www.tensorflow.org/install](https://www.tensorflow.org/install) and [https://nvlabs.github.io/sionna/installation.html](https://nvlabs.github.io/sionna/installation.html)"
"The given context provides the use of the `TB5GScrambler` in the context of the Scrambling for a Python package for wireless simulation. \n\nIn the 5G NR compliance mode, the `TB5GScrambler` is used to \"implements the 5GNR (a.k.a. 5G) compliant uplink scrambling as defined in\" the relevant standards. Here is the usage of the `TB5GScrambler` as per the context:\n\nFirstly, the relevant module needs to imported\n```python\nfrom sionna.fec.scrambling import TB5GScrambler\n```\nThen an instance of the `TB5GScrambler` is to be created. The `TB5GScrambler` requires a `n_rnti` which is \"identifies the terminal to the base station and has to be provided by the user.\" \n\nIn the given context, the instantiation of `TB5GScrambler` is shown: \n```python\ntb5g_scr = TB5GScrambler(n_rnti=1) # n_rnti is the RNTI identifier TB5GScrambler is compliant with 5G standards \n```\nAfter the instantiation, the `TB5GScrambler` can be used to scramble the bits as per the 5G standards. The relevant bit sequence `c` is not shown in the given context and would be a an input for the `TB5GScrambler`.\n\nAs the 5G NR standards complaint Scrambler requires a specific `n_rnti` parameter, the provided `TB5GScrambler`  is primarily for that use case and might not be adaptable to different scrambling parameters. \n\nThe use of the `TB5GScrambler` is shown in the given context in the following lines:\n```python\n# use the scrambler\nc = tf.range(14) # example c is a bit sequence \nc_scr = tb5g_scr(c)\n```\nIn this, a bit sequence `c` is provided as an input to the `TB5GScrambler` and the scrambled bit sequence `c_scr` is generated."
"Here is the detailed information:   \n  \n[sionna.fec.utils.int_mod_2](https://nvlabs.github.io/sionna/_modules/sionna/fec/utils.html#int_mod_2)  \n\nEfficient implementation of modulo 2 operation for integer inputs.\n\nThis function assumes integer inputs or implicitly casts to int.\n\nRemark: the function tf.math.mod(x, 2) is placed on the CPU and, thus, causes unnecessary memory copies.\n\n- **Input**:\n  - `x` ([,n], int) \u2014- Tensor of int64 containing only 0\u2019s and 1\u2019s.\n\n- **Output**:\n  - Returns a tensor with same shape and datatype as `x`.\n\nsource code:\n```python\ndef int_mod_2(x):\n    # pylint: disable=invalid-name\n    \"\"\"Efficient implementation of modulo 2 operation for integer inputs.\n\n    This function assumes integer inputs or implicitly casts to int.\n\n    Remark: the function `tf.math.mod(x, 2)` is placed on the CPU and, thus,\n    causes unnecessary memory copies.\n\n    Input\n    -----\n    x : [,n], int\n        Tensor of int64 containing only 0's and 1's.\n\n    Output\n    ------\n    : same shape and datatype as ``x``\n\n    \"\"\"\n    # pylint: enable=invalid-name\n    if not isinstance(x, tf.Tensor):\n        msg = \"int_mod_2() expected a tensor as 'x'. but got \"+str(type(x))\n        raise TypeError(msg)\n    if x.dtype in [tf.uint8, tf.uint16, tf.uint32, tf.uint64, tf.int8, tf.int16, tf.int32, tf.int64]:\n        shift = tf.math.mod(tf.reduce_sum([x,1], axis=0), 2)\n        y = tf.bitwise_and(x, shift)\n        return y\n    else:\n        # fall back is rather slow\n        return tf.math.mod(x, 2)\n```"
"Here is the detailed information of sionna.utils.SymbolSource:   \n\n[sionna.utils.SymbolSource(constellation=2, num_bits_per_symbol=1, seed=None, dtype=tf.complex64, add_awgn=True, return_indices=False, verbose=False, **kwargs)](https://nvlabs.github.io/sionna/_modules/sionna/utils/misc.html#SymbolSource)\n\nLayer to create random complex-valued vector of symbols with a specified constellation. Optionally, AWGN noise can be added.\n\n### Parameters\n\n- **constellation** (`str` or `Constellation`): The constellation object or string. In the latter case, it can be one of ['qam', 'pam', 'custom']. For 'custom', an instance of `Constellation` must be provided.\n\n- **num_bits_per_symbol** (`int`): The number of bits per constellation symbol, e.g., 4 for QAM16. Only required for constellation types 'qam' and 'pam'.\n\n- **seed** (`int` or `None`): The seed for the random generator. If set to `None`, a random integer is used.\n\n- **dtype** (`tf.complex64`, `tf.complex128`, `tf.DType`): The output dtype. Defaults to `tf.complex64`.\n\n- **add_awgn** (`bool`): If set to `True`, AWGN noise is added to the generated symbols. Defaults to `True`.\n\n- **return_indices** (`bool`): If set to `True`, the function also returns the symbol indices. Defaults to `False`.\n\n- **verbose** (`bool`): If set to `True`, the symbol indices and constellation points are printed. Defaults to `False`.\n\n### Input\n\n- **shape** (`1D tensor/array/list`, `tf.int`): The desired shape of the output vector.\n\n### Output\n\n- **symbols** (`tf.complex`): Random vector of constellation symbols of shape `shape`.\n\n- **symbol_ind** (, `tf.int32`): The symbol indices. Only returned if `return_indices` is set to `True`.\n\nsource code:\n```python\nclass SymbolSource(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"SymbolSource(constellation=2, num_bits_per_symbol=1, seed=None, dtype=tf.complex64, add_awgn=True, return_indices=False, verbose=False, **kwargs)\n\n    Layer to create random complex-valued vector of symbols with a\n    specified constellation. Optionally, AWGN noise can be added.\n\n    Parameters\n    ----------\n    constellation : ``str`` or :class:`~sionna.mapping.Constellation`\n        The constellation object or string. In the latter case, it can be\n        one of `['qam', 'pam', 'custom']`. For \"custom\", an instance of\n        :class:`~sionna.mapping.Constellation` must be provided.\n\n    num_bits_per_symbol : ``int``\n        The number of bits per constellation symbol, e.g., 4 for QAM16.\n        Only required for constellation types \"qam\" and \"pam\".\n\n    seed : ``int`` or ``None``\n        The seed for the random generator. If set to ``None``, a random\n        integer is used.\n\n    dtype : tf.complex64, tf.complex128, tf.DType\n        The output dtype. Defaults to tf.complex64.\n\n    add_awgn : ``bool``\n        If set to ``True``, AWGN noise is added to the generated symbols.\n        Defaults to ``True``.\n\n    return_indices : ``bool``\n        If set to ``True``, the function also returns the symbol indices.\n        Defaults to ``False``.\n\n    verbose : ``bool``\n        If set to ``True``, the symbol indices and constellation points\n        are printed. Defaults to ``False``.\n\n    Input\n    -----\n    shape : `1D tensor/array/list`, `tf.int`\n        The desired shape of the output vector.\n\n    Output\n    ------\n    symbols : [...,n], tf.complex\n        Random vector of constellation symbols of shape ``shape``.\n\n    symbol_ind : [...,n], tf.int32\n        The symbol indices. Only returned if ``return_indices``\n        is set to ``True``.\n    \"\"\"\n    def __init__(self,\n                 constellation=2,\n                 num_bits_per_symbol=1,\n                 seed=None,\n                 dtype=tf.complex64,\n                 add_awgn=True,\n                 return_indices=False,\n                 verbose=False,\n                 **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n\n        # Create constellation object\n        constellation = Constellation.create_or_check_constellation(\n                                                        constellation,\n                                                        num_bits_per_symbol,\n                                                        dtype=dtype)\n        self._constellation = constellation\n\n        # Set seed if provided or generate new random seed\n        self._seed = None\n        if seed is not None:\n            self._seed = [seed, tf.cast(seed, tf.int32)]\n        else:\n            self._seed = tf.random.uniform((2,),\n                                            minval=0,\n                                            maxval=999999999,\n                                            dtype=tf.int32)\n\n        self._add_awgn = add_awgn\n        self._return_indices = return_indices\n        self._verbose = verbose\n\n    def call(self, inputs):\n        shape = inputs\n        const = self._constellation\n        seed = self._seed\n        add_awgn = self._add_awgn\n        return_inidces = self._return_indices\n        verbose = self._verbose\n\n        # cast to requested dtpe (tf.complex64 or tf.complex128)\n        const = tf.cast(const, self.dtype)\n\n        # Create random symbol indices\n        symbol_ind = tf.random.uniform(shape=shape,\n                                        minval=0,\n                                        maxval=const.n_points,\n                                        dtype=tf.int32,\n                                        seed=seed[0])\n\n        # Gather the constellation points according to the symbol indices\n        symbols = tf.gather(const.points, symbol_ind, axis=0)\n\n        if verbose is True:\n            print(\"Symbol indices: \\n\", symbol_ind)\n            print(\"Constellation points: \\n\", const.points)\n\n        if add_awgn is True:\n            no = tf.cast(tf.pow(const.energ_per_bit * const.over_samp, 0.5),\n                         symbols.dtype)\n            w = tf.complex(\n                        tf.random.normal(shape, tf.zeros_shape, no.dtype),\n                        tf.random.normal(shape, tf.zeros_shape, no.dtype))\n            symbols = symbols + w\n\n        if return_inidces:\n            return symbols, symbol_ind\n        else:\n            return symbols\n```"
"Here is the detailed information of ChannelModel:   \n  \nLink of the source code:  https://nvlabs.github.io/sionna/_modules/sionna/channel/channel_model.html#ChannelModel    \n  \nSource code:\n```python\n#\n# SPDX-FileCopyrightText: Copyright (c) 2021-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n\"\"\"\nAbstract class that defines an interface for channel models\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nimport tensorflow as tf\n\nclass ChannelModel(ABC):\n    # pylint: disable=line-too-long\n    r\"\"\"Abstract class that defines an interface for channel models.\n\n    Any channel model which generates channel impulse responses\n    must implement this interface.\n    All the channel models available in Sionna, such as\n    :class:`~sionna.channel.tr38901.UMi`,\n    :class:`~sionna.channel.tr38901.UMa`,\n    :class:`~sionna.channel.tr38901.UMiOtoine`,\n    :class:`~sionna.channel.tr38901.HoV`, etc., implement this interface.\n\n    Input\n    -----\n    batch_size : int\n        Batch size\n\n    num_time_steps : int\n        Number of time steps\n\n    sampling_frequency : float\n        Sampling frequency [Hz]\n\n    Output\n    ------\n    a : [batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps], tf.complex\n        Path coefficients\n\n    tau : [batch size, num_rx, num_tx, num_paths], tf.float\n        Path delays [s]\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, batch_size, num_time_steps, sampling_frequency):\n        # pylint: disable=line-too-long\n        a, tau = self._generate_rips(batch_size, num_time_steps)\n        return a, tau\n\n    @abstractmethod\n    def _generate_rips(self, batch_size, num_time_steps):\n        # pylint: disable=line-too-long\n        \"\"\"Generates Rician or Rayleigh multipath components\n\n        Generate  line-of-site (LoS) path with average power `a` and delay\n        `tau` as well as `k` non LoS paths with powers `a` and delays `tau`.\n\n        The distribution can be Rician or Rayleigh.\n\n        Input\n        ------\n        batch_size : int\n            Batch size\n\n        num_time_steps : int\n            Number of time steps\n\n        Output\n        ------\n        a : [batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps], tf.complex\n            Path coefficients\n\n        tau : [batch size, num_rx, num_tx, num_paths], tf.float\n            Path delays [s]\n        \"\"\"\n        pass\n```    \n  "
"To build a custom `RadioMaterial` in the Sionna package and attach it to a `SceneObject`, you need to define a callback function that computes the material properties (`relative_permittivity` and `conductivity`) from the frequency. Then, you can create an instance of the material using this callback function. Here is the general process:\n\n1. Define a callback function to compute the material properties:\n\n```python\ndef my_material_callback(f_hz):\n   relative_permittivity = ... # Compute relative permittivity for the given frequency\n   conductivity = ... # Compute conductivity for the given frequency\n   return (relative_permittivity, conductivity)\n```\n\nReplace the `...` in the code with your actual implementation to compute the relative permittivity and conductivity based on the frequency `f_hz`.\n\n2. Create a `RadioMaterial` instance using the callback function:\n\n```python\ncustom_material = RadioMaterial(\"my_material\",\n                                frequency_update_callback=my_material_callback)\n```\n\nYou need to replace the `my_material_callback` with the actual name of your callback function. The `frequency_update_callback` parameter is set to the name of your callback function.\n\n3. Attach the `RadioMaterial` to a `SceneObject`:\n\n```python\nscene_object = SceneObject(\"my_object\")\nscene_object.radio_material = custom_material\n```\n\nYou need to replace \"my_object\" with the name of your `SceneObject`. The `radio_material` property of the `SceneObject` is then set to the `RadioMaterial` instance you created.\n\nIn this process, the callback function is used to compute the material properties dynamically based on the frequency. The callback function is used in a mechanistic way by the Sionna API and must therefore adhere to the defined signature. The material parameters can be assigned to TensorFlow variables or tensors, which allows the material to be trainable."
"To perform maximum a posteriori (MAP) decoding with a `BCJRDecoder` instance from the Sionna package, you need to follow these steps:\n\n1. **Create an Instance of the BCJRDecoder**: You need to create an instance of the `BCJRDecoder` by providing the appropriate arguments. If you are using an AWGN channel, you can create the instance as shown in the example below:\n\n    ```python\n    from sionna.fec.conv.decoding import BCJRDecoder\n    \n    # Create an instance of the BCJRDecoder\n    bcjr_decoder = BCJRDecoder(\n        trellis=trellis,       # An instance of the Trellis object\n        terminate=True,       # If the codeword is terminated (default is True)\n        hard_out=False)        # Indicate whether to return hard-decided codewords\n    ```\n\n    In this example, `trellis` should be an instance of the `Trellis` object that you've created during the encoding stage. The `terminate` parameter indicates whether the codeword was terminated (this should match the setting used during encoding), and the `hard_out` parameter indicates whether the decoder should return hard-decided codewords (i.e., binary values) or soft-decisions (i.e., log-likelihood ratios).\n\n2. **Provide the Input LLRs for Decoding**: The LLRs that you provide to the `BCJRDecoder` represent the received (noisy) channel LLRs. These LLRs can either be for the channel directly (referred to as `llr_ch`), or they can include the a priori information (referred to as `(llr_ch, llr_a)`). If you have calculated the a priori information, it should be provided along with the channel LLRs. The a priori information indicates the existing knowledge about the reliability of each bit before taking the channel measurements.\n\n    3. **Decode the Input LLRs Using the BCJRDecoder**: Once the `BCJRDecoder` instance is created and the LLRs are provided, you can use the instance to decode the input LLRs. The decoder will perform the MAP decoding process, which attempts to find the mostly likely sequence of transmitted codewords that would result in the observed LLRs.\n\n    4. **Receive the Output**: After the decoding process, you will receive the decoded output. The output can be in the form of binary hard decisions (i.e., codeword bits) or soft-decision log-likelihood ratios, depending on the configuration of the `BCJRDecoder` instance and the parameter `hard_out`.\n\n**Note About the `hard_out` Parameter**:**\n\nThe `hard_out` parameter is used to specify the type of output that the decoder provides:\n\n- If `hard_out=True`, the decoder will provide hard-decided bit values. This means that for each bit of the output codeword, you will receive a binary value (for example, `0` or `1`).\n\n- If `hard_out=False`, the decoder will provide soft-output values in the form of log-likelihood ratios (LLRs). The LLR for a bit is a real-valued quantity that provides information about the reliability of the received bit, which is a more advanced form of decoding output.\n\nThe type and shape of the output tensor from the `BCJRDecoder` will depend on the configuration of the `Trellis` object used during the encoding stage, as well as the parameters `hard_out` and `terminate` used during the decoder creation. However, in general, you would expect the decoded output to be a tensor of bit values (for hard-decoding) or LLR values (for soft-decoding), which matches the shape of the input LLRs."
"To make use of the function `ebnodb2no` from the `sionna.utils.misc` module, you will need to pass an Eb/No value in dB (ebno_db) as a float, the number of bits per symbol (num_bits_per_symbol) as an integer, the specified coderate (coderate) as a float, and optionally a resource grid (resource_grid) which is an instance of the `ResourceGrid` class in the `sionna.ofdm` module. \n\nThe function will return the noise power spectral density N0, which is a float. This N0 value is calculated using the input parameters: the energy per information bit (E_b), the number of bits per symbol, the coderate, and the additional overheads related to the OFDM transmissions if an resource grid instance is provided. \n\nAn example usage might be:\n\n```python\nimport sionna.utils.misc as misc\n\n# input parameters\nebno_db = 10. # in dB\nnum_bits_per_symbol = 2 # for QPSK\ncoderate = 0.75 # for a 0.75 rate code\n\n# calculate N0\nN0 = misc.ebnodb2no(ebno_db, num_bits_per_symbol, coderate)\n\nprint(N0)\n```"
"To evaluate the baseline and the two autoencoder-based communication systems, you can use the `sim_ber` function from the 'Sionna' package. This function computes the bit error rate (BER) and block error rate (BLER) for a Keras model. \n\nHere's how you can do it:\n\n1. First, you need to define the models for the baseline and the two autoencoder-based communication systems. These models should implement the encoder and decoder layers using the BinarySource, GaussianPrior, and LDPC5GDecoder layers as described in the provided context.\n\n2. Once the models are defined, you can use the `sim_ber` function to evaluate them. The `sim_ber` function requires the following parameters:\n   - **model**: The Keras model to evaluate.\n   - **ebno_db**: The range of SNR values (in dB) for evaluation.\n   - **batch_size**: The number of examples processed by Keras in parallel.\n\nThe `sim_ber` function returns the following metrics:\n   - **BER**: The bit error rate.\n   - **BLER**: The block error rate.\n\nFor the baseline and the two autoencoder-based communication systems, you can call the `sim_ber` function with the corresponding model and a range of SNR values to get the BER and BLER metrics.\n\nPlease note that the provided context shows how to define the models and train them using a custom training loop. However, it doesn't show how to evaluate the models. The actual evaluation is not shown in the context."
"To configure the notebook to use only a single GPU and allocate only as much memory as needed, you can use the TensorFlow `tf.config` module. Here are the steps:\n\n1. List the physical devices (GPUs in this case) available for TensorFlow to detect, using `tf.config.list_physical_devices('GPU')`. If you have multiple GPUs, you will see a list of all the available GPUs.\n\n2. Check how much memory is allocated on the GPU you want to use, by using `tf.config.experimental.get_memory_growth()`.\n\n3. To set the memory growth on the GPU, which allows TensorFlow to allocate only as much GPU memory as needed for the runtime allocations, you can use `tf.config.experimental.set_memory_growth()`. This needs to be done for the GPU object obtained from `tf.config.list_physical_devices()`.\n\n4. Finally, you can set the notebook to use only a specific GPU by setting the `os.environ`[\"CUDA_VISIBLE_DEVICES\"] variable to the index of the GPU you want to use. If you want to use the CPU, you can set it to an empty string (\"\").\n\nHere is the code snippet from the provided context that demonstrates these steps:\n\n```python\nimport tensorflow as tf\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\ntf.config.set_visible_devices(gpus[0], 'GPU')\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use \"\" to use the CPU\n```\n\nIn this code, `gpus[0]` is used to select the first GPU. If you want to use a specific GPU, you can change `gpus[0]` to the index of the GPU you want to use. If you want to use the CPU, you can set `gpus[0]` to an empty string."
"To use the \"j_fun\" function to calculate the J-function in the Sionna package, you need to follow these steps:\n\n1. Import the required function from the package:\n    ```python\n    from sionna.fec.utils import j_fun\n    ```\n\n2. Prepare your input data. The \"mu\" parameter in this function is a float or an ndarray of floats. The values for \"mu\" should typically be in the range of -30 to 30. \n\n3. Call the function with your input data. For example:\n    ```python\n    result = j_fun(mu)\n    ```\n\n    Replace `mu` with your actual value.\n\nThe function will return a float or an ndarray (depending on your input) which represents the calculated J-function value. Please ensure that your input values are within the defined range for optimal performance."
"Here is the definition of PUSCHTransmitter: sionna.nr.PUSCHTransmitter(pusch_configs, return_bits=True, output_domain='freq', dtype=tf.complex64, verbose=False, **kwargs)  \n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/nr/pusch_transmitter.html#PUSCHTransmitter)  \n  \nsource code:  \n```python\n#\n# SPDX-FileCopyrightText: Copyright (c) 2021-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n\n\"\"\"\nThis module defines transmitters for the NR PUSCH component.\n\"\"\"\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer\nimport numpy as np\nimport scipy\n\n# Not directly needed, but the imports are required for the explanations\nfrom pusch_config import PUSCHConfig\nfrom tb_encoder import TBEncoder\nfrom mapper import Mapper\nfrom dmrs import DMRS\nfrom dmrs_config import DMRSConfig\nfrom layer_mapper import LayerMapper\nfrom precoder import Precoder\nfrom nvp_recorder import NVPSubcarrierFilter\nfrom ofdm_modulator import OFDMModulator\nfrom resource_grid_mapper import ResourceGridMapper\nfrom pusch_antenna_array import AntennaArray\n\n\nclass PUSCHTransmitter(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"PUSCHTransmitter(pusch_configs, return_bits=True, output_domain=\"freq\", dtype=tf.complex64, verbose=False, \\**kwargs)\n\n    Transmitter for the NR PUSCH component. This component\n    is used to transmit 5G NR PUSCH slots. The exact\n    behavior is configured through a set of :class:`~sionna.nr.PUSCHConfig`\n    objects.\n\n    This class is actually a Keras layer, meaning that it can be used\n    as part of a Keras model.\n\n    ..  code-block:: Python\n\n        pusch_transmitter = PUSCHTransmitter(pusch_configs)\n        pusch_transmitter()\n\n    Parameters\n    ----------\n    pusch_configs : :class:`~sionna.nr.PUSCHConfig` or list of  :class:`~sionna.nr.PUSCHConfig`\n        Object or objects of the spectral configuration.\n        In case of multiple configurations, a list of configurations\n        must be provided.\n\n    return_bits : bool, optional\n        If set to `True`, the layer also returns the information bits that\n        were transmitted. Defaults to `True`.\n\n    output_domain : One of [\"freq\", \"time\"],  string\n        Domain for the output.\n        Defaults to \"freq\".\n\n    dtype : One of [tf.complex64, tf.complex128], tf.DType\n        Dtype of `x`.\n        Defaults to tf.complex64.\n\n    verbose : bool\n        Defaults to False.\n    \"\"\"\n    def __init__(self,\n                 pusch_configs,\n                 return_bits=True,\n                 output_domain=\"freq\",\n                 dtype=tf.complex64,\n                 verbose=False,\n                 **kwargs):\n\n        if output_domain not in (\"time\", \"freq\"):\n            raise ValueError(\"Invalid value for `output_domain`\")\n\n        if dtype not in (tf.complex64, tf.complex128):\n            raise ValueError(\"Unsupported dtype.\")\n\n        super().__init__(dtype=dtype, **kwargs)\n\n        if isinstance(pusch_configs, list):\n            self._num_tx = len(pusch_configs)\n            self._default_config = None\n        elif isinstance(pusch_configs, PUSCHConfig):\n            pusch_configs = [pusch_configs]\n            self._num_tx = 1\n            self._default_config = pusch_configs\n        else:\n            raise ValueError(\"Invalid pusch_configs.\")\n\n        # Instantiate TB encoder for each pusch config\n        self._tb_encoder = TBEncoder(pusch_configs,\n                                     return_bits=return_bits,\n                                     output_domain=output_domain,\n                                     dtype=dtype,\n                                     verbose=verbose)\n\n        # Combine all pusch_configs to a single dmrs config\n        dmrs_configs = []\n        for pusch_config in pusch_configs:\n            for l in pusch_config.l_deploy:\n                # Only look for corresponding dmrs config\n                if l not in pusch_config.dmrs_grid.l_v:\n                    continue\n                l_idx = pusch_config.dmrs_grid.l_v.index(l)\n                dmrs_configs.append(DMRSConfig(pusch_config.dmrs_grid.l_v[l_idx],\n                                               pusch_config.carrier_config))\n        self._dmrs_config = DMRSConfig.combine(*dmrs_configs)\n\n\n        # Create antenna array\n        self._antenna_array = AntennaArray(self._default_config[0].dmrs_grid.precoding_matrix,\n                                          self._default_config[0].dmrs_grid.num_cyclic_prefix,\n                                          dtype)\n\n        # Create layer mapper\n        self._layer_mapper =  LayerMapper(self._default_config[0].tb_size,\n                                            self._default_config[0].dmrs_grid.num_layers,\n                                            dtype)\n\n        #  Create  DMRS\n        self._dmrs = DMRS(self._dmrs_config,\n                        self._default_config[0].dmrs_grid,\n                        self._antenna_array,\n                        dtype)\n\n        # Create resource grid mapper\n        self._resource_grid_mapper = ResourceGridMapper(self._default_config[0].resource_grid,\n                                                        dtype=dtype)\n\n        # Create precdoder\n        self._precoder = Precoder(self._default_config[0].dmrs_grid,\n                                self._default_config[0].num_layers,\n                                \"nr\",\n                                dtype)\n\n        if output_domain == \"time\":\n            self._ofdm_modulator = OFDMModulator(\n                fft_size=self._default_config[0].resource_grid.fft_size,\n                l_min = self._default_config[0].dmrs_grid.l_min,\n                l_max = self._default_config[0].dmrs_grid.l_max,\n                num_time_samples = self._default_config[0].resource_grid.num_scs\n            )\n\n        # Flag indicating if the DMRS is supposed to be mapped on the data\n        # symbols. This is not the case if the input is a codeword.\n        self._map_dmrs_to_data = True\n\n    #########################################\n    # Public methods and properties\n    #########################################\n\n    @property\n    def pusch_configs(self):\n        \"\"\"Configurations for the PUSCH transmitter.\"\"\"\n        return self._pusch_configs\n\n    @property\n    def return_bits(self):\n        \"\"\"Indicates if bits are transmitted or not.\"\"\"\n        return self._tb_encoder.return_bits\n\n    def call(self, batch_size, snr_db=None, num_channel_symbols=None):\n        \"\"\"Creates a batch of PUSCH transmitters.\n\n        The information bits are generated by the TBEncoder. If\n        `return_bits` is `True`, the returned batch of transmitters and the\n        corresponding batch of information bits can be obtained by:\n\n        .. code-block:: Python\n\n            pusch_transmitter = PUSCHTransmitter(pusch_configs, return_bits=True)\n            x, b = pusch_transmitter( batch_size)\n\n        If `return_bits` is `False`, only the batch of transmit signals ``x``\n        is returned.\n\n        Inputs\n        ------\n        batch_size : int\n            Batch size\n\n        snr_db : Tensor, dtype=float\n            The SNR value in dB. Defaults to `None`.\n\n        num_channel_symbols : int\n            Number of data symbols per codeword. Defaults to `None`.\n\n        Output\n        -------\n        x : [batch size, num_tx, num_tx_ant, num_ofdm_symbols, fft_size], tf.complex\n            Batch of time-domain PUSCH signals\n\n        b : [batch size, num_tx, tb_size], tf.float\n            The transmitted information bits. Only returned if\n            `return_bits` is `True`.\n        \"\"\"\n        x, b =  self._tb_encoder(batch_size,\n                                    snr_db,\n                                    num_channel_symbols)\n        if self._tb_encoder.output_domain==\"freq\":\n            # If the output is in the frequency domain, we need to map the DMRS\n            # to the data symbols.\n            if self._map_dmrs_to_data:\n                x, dmrs_m = self._map_dmrs_to_data_(x)\n        elif self._tb_encoder.output_domain==\"time\":\n            if self._tb_encoder.return_bits:\n                # Also TBEncoder returns time-domain signal\n                x, b, dmrs_m = self._dmrs_to_time_caller( x, b)\n            else:\n                x, dmrs_m = self._dmrs_to_time_caller(x)\n\n        if self._tb_encoder.return_bits:\n            return x, b, dmrs_m\n        else:\n            return x\n\n    #########################\n    # Private methods\n    #########################\n\n    def _map_dmrs_to_data_(self, x):\n        # x is the input signal\n\n        # Prepare output array\n        x_eff = tf.zeros(x.shape, x.dtype)\n\n        # Indices for the first and last DMRS symbol\n        d_0, d_f =  self._dmrs_symbol_indices(self._default_config[0].num_cdm_groups_without_data)\n\n        # Map DMRS to data\n\n        # Empty symbols for cdm_groups_without_data\n        x_eff = tf.tensor_scatter_nd_add(x_eff,\n                                    self._dmrs.ind_dmrs[0:d_0].numpy(),\n                                    self._dmrs.dmrs[0:d_0].numpy())\n\n        x_eff = tf.tensor_scatter_nd_add(x_eff,\n                                    self._dmrs.ind_dmrs[d_f:].numpy(),\n                                    self._dmrs.dmrs[d_f:].numpy())\n\n        # Transpose input tensor to split physical resource grid and precoded\n        # data/cdm_groups\n        x_c,d = tf.transpose(x, [1, 3])\n\n        # Add precoding to tx signal\n        x_c = self._precoder(x_c, d)\n\n        # Empty symbols for  cdm_groups\n        x_eff = tf.tensor_scatter_nd_add(x_eff,\n                                    self._dmrs.ind_cdm_groups_without_data.numpy(),\n                                    x_c.numpy())\n\n        # Add all layers. The \"last\" parameter means that the output domain of\n        # the layer must be 'data'.\n        x_eff = self._layer_mapper(x_eff)\n\n        # Put the rest of the input signal on top of the layers.\n        x_eff = tf.tensor_scatter_nd_add(x_eff,\n                                    self._tb_encoder.ind_data.numpy(),\n                                    x_eff.numpy())\n\n        dmrs_m = np.ones(self._default_config[0].num_resource_blocks,\n                        np.bool)\n        for n in range(self._default_config[0].num_syms_per_slot):\n            if np.mod(n, 14) in [3, 4, 9, 10]:\n                dmrs_m[self._default_config[0].num_cdm_groups_without_data//2 + n] \\\n                    = False\n\n        # Invert dmrs mask such that the zeros correspond to no DMRS signals.\n        dmrs_m = np.invert(dmrs_m)\n        dmrs_m = tf.cast(dmrs_m, x.dtype)\n        dmrs_m = insert_zeroes_at_dmrspilot(self._default_config[0], dmrs_m)\n        dmrs_m = tf.cast(dmrs_m, tf.int8)\n\n        # Remove the \"ind\" dimensions by using squeeze. This is in particular\n        # required when the num of tx = 1.\n        dmrs_m = tf.squeeze(dmrs_m)\n\n        # Equal to x_eff, but in tf.int8. This is only needed when\n        # the ind_xs has been quantized.\n        x_eff = tf.squeeze(x_eff)\n        x_eff = tf.saturate(x_eff, tf.int8.min, tf.int8.max)\n        x_eff = tf.cast(x_eff, tf.int8) # use int to support gather\n        x_eff = tf.expand_dims(x_eff, -1) # insert new dim for output symbol\n        eff_shift = tf.expand_dims(0, axis=-1) # insert new dim for gather\n\n        # Output from gather. Gathering is used to map modulated symbols to the\n        #  correct positions in the (possibly) mapped resource grid.\n        x = tf.gather(x_eff, self._tb_encoder.ind_cw_channel, axis=0)\n        x = tf.roll(x, eff_shift, axis=-1)\n\n        #  Put the precoded data/cdm_groups on top of the gather output.\n        x = tf.tensor_scatter_nd_add(x, self._tb_encoder.ind_data.numpy(), x.numpy())\n\n        # Apply nvp-recording\n        nvp_rec_filter = NVPSubcarrierFilter(self._default_config[0].resource_grid,\n                                            num_ofdm_symbols = self._default_config[0].num_syms_per_slot)\n\n        # Compute output of nvp-recording\n        output_dict = nvp_rec_filter({\"x\":x, \"no\":0.0})\n        x = output_dict[\"x\"]\n\n        # Bring tx signal in the right shape\n        # Insert batch dim for consistency\n        x = tf.expand_dims(x, 1)\n\n        # Put first DMRS on top gather output.\n        x = tf.tensor_scatter_nd_add(x, self._dmrs.ind_dmrs[0].numpy(),\n                                    self._dmrs.dmrs[0].numpy())\n        # Put second DMRS on top of that.\n        x = tf.tensor_scatter_nd_add(x, self._dmrs.ind_dmrs[1].numpy(),\n                                    self._dmrs.dmrs[1].numpy())\n\n        return x, dmrs_m\n\n\n    def _dmrs_symbol_indices(self, num_empty_symbols):\n\n        num_empty_symbols = int(num_empty_symbols)\n\n        # Number of DMRS symbols.\n        # Assumed to be 2. This could be made as input to the function\n        num_dmrs_syms = 2\n\n        # Calculate indices of empty symbols\n        # First empty symbol is 0\n        ind = np.zeros([num_empty_symbols,], np.int32)\n        # Last empty symbol\n        ind[-1] = num_empty_symbols + num_dmrs_syms - 1\n\n        # Indices of DMRS symbols\n        ind_dmrs = np.empty([num_dmrs_syms,], np.int32)\n        ind_dmrs[0] =  ind[0] + 1 + int(0.5*num_empty_symbols)\n        a = 7 * np.floor( ind_dmrs[0] / 14.)\n        ind_dmrs[1] = ind[0] + 1 + int(0.5*num_empty_symbols) + int(a)\n\n        # Shift indices for empty symbols\n        ind_dmrs -= ind_dmrs[0]\n\n        # Shift indices for empty symbols\n        ind_dmrs += ind[-1] + 1\n\n        return ind_dmrs\n\n    def _dmrs_to_time_caller(self, x, b=None):\n        # x in the frequency domain\n        # b in for the transform in time. Is actually not needed if\n        # the dmrs is mapped to the input data.\n\n        batch_size = tf.shape(x)[0]\n\n        # Compute number of\n        num_cdm_groups = self._default_config[0].num_cdm_groups_without_data\n\n        # Split input tensor in blocks of size\n        # num_empty_symbols = int64(num_cdm_groups *\n        #                 dmrs_length // num_tx_ant)\n        z = tf.signal.fftshift(x, axes=-1)\n        z, _  = tf.linalg.dft([z, z]) # pylint: disable=unbalanced-tuple\n        z = tf.signal.fftshift(z, axes=-1)\n\n        # Gather DMRS positions. Note that the inftl and tftl can be inverted\n        # in a way that the DMRS is mapped to the data input. This is controled\n        # by the map_dmrs_to_data flag.\n        if self._map_dmrs_to_data:\n            dmrs_m = self._dmrs_m\n        else:\n            n_inf = int(self._num_tx_ant/num_cdm_groups)\n            dmrs_m = tf.gather(self._dmrs_m, np.arange(n_inf,  dtype=tf.int32))\n\n        # Gather DMRS from resource grid\n        z = tf.gather(z, dmrs_m, axis=-2)\n\n        if b is None:\n            # Defaults to 0 if not explictly given\n            b = tf.zeros([batch_size,\n                        self._num_tx,\n                        self._tb_encoder.tb_size-num_cdm_groups*\n                                                self._default_config[0].dmrs_length],\n                    dtype=x.dtype)\n\n        # Expand b to shape of z\n        b = tf.expand_dims(b, axis=-1)\n        b = tf.repeat(b, repeats=z.shape[1], axis=0)\n        b = tf.repeat(b, repeats=z.shape[2], axis=1)\n        z = tf.concat([z, b], axis=-1)\n\n        # Transpose tensor to shape [batch_size, num_tx_ant, num_ofdm_symbols,\n        #  num_time_samples].\n        z = tf.transpose(z, [1, 2, 4, 3])\n        if self._default_config[0].num_cdm_groups>1:\n            # If there are multiple cdm groups, we need to\n            # to a gather in the num_ofdm_symbol dimension\n            z = tf.reshape(z, [batch_size,\n                            self._num_tx,\n                            num_cdm_groups,\n                            self._default_config[0].num_cdm_groups_without_data+\\\n                            int(self._default_config[0].dmrs_length//2),\n                            self._default_config[0].num_ofdm_symbols,\n                            self._default_config[0].num_subcarriers])\n            z = tf.gather(z, self._dmrs_cdm_group, axis=2, batch_dims=2)\n            z = tf.reshape(z, [batch_size,\n                            self._num_tx,\n                            self._default_config[0].num_cdm_groups_without_data,\n                            int(self._default_config[0].dmrs_length//2),\n                            self._default_config[0].num_ofdm_symbols,\n                            self._default_config[0].num_subcarriers])\n            z = tf.squeeze(z, axis=2)\n\n        # Put first DMRS on top gather output.\n        # Put second DMRS on top of that.\n        z = tf.concat([self._dmrs_cyclic_shifts, z], axis=-2)\n        z = tf.gather(z, self._dmrs_data_symbols_ind, axis=-2,\n                    batch_dims=1,\n                    expand_dim=True)\n\n        # Put first empty sub-carriers to zero.\n        # Put last empty sub-carriers to zero.\n        z = tf.tensor_scatter_nd_add(z,\n                                    self._dmrs_data_symbols_ind[0, :],\n                                    tf.zeros([2, self._default_config[0].num_subcarriers],\n                                            z.dtype))\n        z = tf.tensor_scatter_nd_add(z,\n                                    self._dmrs_data_symbols_ind[-1, :],\n                                    tf.zeros([2, self._default_config[0].num_subcarriers],\n                                            z.dtype))\n\n\n        # Put last DMRS on top of that.\n        #z = tf.gather(x, self._dmrs_ind, axis=-2, expand_dim=True)\n\n        if self._default_config[0].num_cdm_groups>1:\n            z = tf.reshape(z,"
"Here is the detailed information of the function: \n\n[sionna.fec.utils.verify_gm_pcm(gm, pcm)](https://nvlabs.github.io/sionna/_modules/sionna/fec/utils.html#verify_gm_pcm)\n\nVerify that generator matrix $\\mathbf{G}$ `gm` and parity-check matrix $\\mathbf{H}` pcm` are orthogonal in GF(2).\n\nFor an all-zero syndrome, it must hold that\n\n$$\n\\mathbf{H} \\mathbf{c}^t = \\mathbf{H} * (\\mathbf{u} * \\mathbf{G})^t =\n\\mathbf{H} * \\mathbf{G} ^t * \\mathbf{u}^t =: \\mathbf{0}\n$$\n\nwhere $\\mathbf{c}$ denotes an arbitrary codeword and $\\mathbf{u}$ the corresponding information bits.\n\nAs $\\mathbf{u}$ can be arbitrary it follows that\n\n$$\n\\mathbf{H} * \\mathbf{G} ^t =: \\mathbf{0}.\n$$\n\nInput\n- gm (ndarray) : Binary generator matrix of shape [k, n].\n- pcm (ndarray) : Binary parity-check matrix of shape [n-k, n].\n\nOutput\n- bool : True if gm and pcm define a valid pair of parity-check and generator matrices in GF(2).\n\nRaises\n- AssertionError: If the validation fails for any reason (see above).\n\n**Note: **This function only verifies that $\\mathbf{G}$ and $\\mathbf{H}$ are orthogonal. A valid PCM can be generated by taking the transpose of the GM and removing the identity matrix on the left hand side. This function does not check whether `pcm` has full rank.\n\nAs `gm` and `pcm` are ternary matrices, the result of this product will be in $\\mathbb{F}_2$ as expected. However, numpy does not allow direct multiplication of two matrices in different integer types. That's why we first multiply in float and then explicitly take the (redundant) modulo 2 operation:\n$\\mathbf{H} * \\mathbf{G} ^t = \\text{mod}(\\mathbf{H} * \\mathbf{G}^t, 2).$"
"To define the permittivity and permeability, we need to first understand what they represent. \n\n**Permittivity**:\nPermittivity is a measure of how an electric field affects an insulating medium. It measures the ability of a material to store electrical energy in an electric field. In the context of electromagnetic waves, it's denoted by the symbol \u03b5 (epsilon). The electrical displacement D in a medium is directly proportional to the electric field E and can be expressed by the equation D = \u03b5E. \n\n**Permeability**:\nPermeability, on the other hand, is the measure of the ability of a material to support the formation of a magnetic field within itself. In the context of electromagnetic waves, it's denoted by the symbol \u03bc (mu). In a similar fashion to the electric displacement, the magnetic field H in a medium is directly proportional to the magnetic field H and can be expressed by the equation H = \u03bcB.\n\nIn the given context, the formulas for the permittivity and permeability are also provided. The relative complex permittivity (\u03b5_r) is given as \u03b5_r = \u03b5 / \u03b5_0, where \u03b5 is the absolute complex permittivity of the material and \u03b5_0 is the absolute permittivity of free space. The real relative permeability (\u03bc_r) is \u03bc_r = \u03bc / \u03bc_0, where \u03bc is the absolute permeability of the material, and \u03bc_0 is the permeability of free space.\n\nThe absolute complex permittivity (\u03b5) is calculated using the formula:\n\n\u03b5 = \u03b5_0 ( \u03b5_r +  j\u03c3 / \u03c9\u03b5_0 )\n\nHere, \u03c3 is the conductivity of the material, \u03c9 is the angular frequency, and the other symbols have their usual meanings. \n\nThe absolute permeability (\u03bc) is given as:\n\n\u03bc = \u03bc_0 ( \u03bc_r  + j\u03c3 / (\u03c9\u03bc_0) )\n\nThese formulas are based on the behavior of electromagnetic waves in materials, which include the effects of conductivity. For non-conducting materials, the conductivity \u03c3 is zero, and the expressions for \u03b5 and \u03bc simplify accordingly."
"Here is the detailed information of RemoveNulledSubcarriers:   \nRemoves nulled (or DC) subcarriers from an OFDM resource grid.\n\n### Parameters\n\n- `resource_grid`: ResourceGrid\n  An instance of `ResourceGrid`.\n  \n- `dtype`: tf.DType (default: tf.complex64)\n  Defines the datatype for internal calculations and the output dtype.\n\n### Input\n\n- `x`: [batch_size, num_tx, num_streams_per_tx, num_ofdm_symbols, fft_size], tf.complex\n  Full OFDM resource grid in the frequency domain.\n\n### Output\n\n- `x`: [batch_size, num_tx, num_streams_per_tx, num_ofdm_symbols, (fft_size - n) // 2], tf.complex\n  Reduced OFDM resource grid, by removing nulled subcarriers.\n\n### Example\n```python\nrg = ResourceGrid(fft_size=512, num_ofdm_symbols=14)\nrg.show()\n# Remove nulled subcarriers\nrg_ns = RemoveNulledSubcarriers(rg)(rg)\nrg_ns.show()\n```\n[<matplotlib.lines.Line2D at 0x7fbe9a7a4c70>](https://matplotlib.org/3/api/_as_gen/matplotlib.pyplot.show.html)"
"Here is the definition of LLRs2SymbolLogits: sionna.mapping.LLRs2SymbolLogits(num_bits_per_symbol, hard_out=False, dtype=tf.float32, **kwargs)  \nSource code:   [LLRs2SymbolLogits](https://nvlabs.github.io/sionna/_modules/sionna/mapping.html#LLRs2SymbolLogits)  \n\nComputes logits or hard decisions on constellation points from a tensor of log-likelihood ratios (LLRs) on bits.\n\n### Parameters\n\n- **num_bits_per_symbol** (`int`): The number of bits per constellation symbol, e.g., 4 for QAM16.\n\n- **hard_out** (`bool`): If `True`, the layer provides hard-decided constellation points instead of soft-values. Defaults to `False`.\n\n- **dtype** (`tf.float32`): The dtype for the input and output. Defaults to `tf.float32`.\n\n### Input\n\n- **llrs** (`[, n, num_bits_per_symbol]`, `tf.float`): LLRs for every bit.\n\n### Output\n\n- `[, n, num_points]`, `tf.float` or `[, n]`, `tf.int32`: Logits or hard-decisions on constellation points.\n\n**Note: **  \nThe logit for the constellation $c$ point is computed, it is equivalent with that of logits. This is different from many textbooks in communications, where the LLR is defined as $LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=0\\lvert y\\right)}{\\Pr\\left(b_i=1\\lvert y\\right)}\\right)$.  \n  "
"To configure the notebook to use only a single GPU and allocate only as much memory as needed, you can use the TensorFlow `tf.config.list_physical_devices` and `tf.config.experimental.set_memory_growth` methods. Here's how you can do it:\n\n1. Import the necessary libraries:\n\n```python\nimport os\nimport tensorflow as tf\n```\n\n2. List the physical GPU devices available:\n\n```python\ntf.config.list_physical_devices('GPU')\n```\n\n3. Select only the first GPU and set the memory growth to True:\n\n```python\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\n```\n\nLet's break down what these steps do:\n\n- `tf.config.list_physical_devices('GPU')` - This returns a list of PhysicalDevice objects representing the currently available GPU devices. If no GPU is available, it returns an empty list.\n\n- `tf.config.experimental.set_memory_growth(gpus[0], True)` - This enables memory growth for the first GPU. The memory growth option attempts to allocate only as much GPU memory as needed for the runtime allocations: it starts out allocating very little memory, and as the program gets run and more GPU memory is needed, TensorFlow extends the GPU memory region allocated to the Python process. \n\n- `tf.config.list_physical_devices` - This is used to list all available physical devices on the local host. By passing 'GPU' as an argument, it filters the results to only return the GPU devices. \n\n- `import os` - This imports the Python `os` module, which provides a way of using operating system dependent functionality like reading or writing to the environment.\n\nRemember that the GPU settings will only take effect if your machine has a GPU installed and the necessary drivers are correctly set up. If not, TensorFlow will use a CPU."
"To compute the codeword bits from information bits using the `LinearEncoder`, you first need to create an instance of the encoder by providing it either with a generator matrix or a parity check matrix. If the matrix is a parity check matrix (`is_pcm=True`), the encoder will internally convert it to a generator matrix for the encoding process.\n\nHere is the general workflow:\n\n1. Create a set of information bits `u`. The dimension of `u` should match the 'k' dimension of the encoding matrix.\n\n2. Initialize the `LinearEncoder` with either the generator or parity check matrix. If `is_pcm=True,` the parity matrix will be converted to a generator matrix for the encoding process.\n\n3. Encode the information bits using the `LinearEncoder`. The encoder will return the codeword bits `c`.\n\n```python\nfrom sionna.fec.linear import LinearEncoder\n\n# ... (Assume 'enc_mat' is either the generator or parity matrix)\n\n# Initialize the encoder with the provided matrix.\n# Note: is_pcm indicates whether the provided matrix is a parity check matrix.\nencoder = LinearEncoder(enc_mat=enc_mat, is_pcm=False)\n\n# If u contains the information bits to be encoded and has shape [...,k].\n# Then c will contain codeword bits and has shape [...,n]\nc = encoder(u)\n```\n\nIn the above code snippet, you need to replace `enc_mat` with the actual encoding matrix (either the generator matrix or parity check matrix), which you obtain from your code or example data.\n\nRemember that the information bits `u` provided must adhere to the dimensions and type constraints set by the encoder, which typically require that `u` be a float or integer tensor and the last dimension of `u` be equal to the 'k' dimension of the encoding matrix. On output, the encoder provides the generated codeword bits `c`, which will have dimensions that depend on the input `u` provided.\n\nAlso, note that this example uses the `sionna.fec.linear` package. If it's not already installed, you may need to install it using pip:\n\n```python\npip install sionna\n```"
"Here is the detailed information of sionna.utils.flatten_last_dims:   \n\n[sionna.utils.flatten_last_dims(tensor)](https://nvlabs.github.io/sionna/_modules/sionna/utils/tensors.html#flatten_last_dims)\n\nFlattens the last dimensions of a tensor.\n\nFlattens the last dimension of a tensor but keeps the remaining dimensions as-is.\n\nFor example, given a tensor with shape [...,a,b], the output will be a tensor of shape [\u2026,a*b].\n\n### Parameters\n\n- **tensor**: A tensor of any shape.\n- **dtype**: The desired output dtype. Defaults to tf.float32.\n\n### Input\n\n`[,a,b]`\n\n### Output\n\n`[...,a*b]`\n\nsource code:\n```python\ndef flatten_last_dims(tensor):\n    \"\"\"\n    Flattens the last dimensions of a tensor.\n\n    Flattens the last dimension of a tensor but keeps the remaining\n    dimensions as-is.\n\n    For example, given a tensor with shape `[...,a,b]`, the output will be\n    a tensor of shape `[...,a*b]`.\n\n    Args:\n        tensor : A tensor.\n\n    Returns:\n        A flattened tensor with same dtype as ``tensor``.\n    \"\"\"\n    msg = \"tf.rank(tensor) must be greater than one.\"\n    shape_ = tf.shape(tensor)\n    tf.debugging.assert_greater(tf.rank(tensor), 0, msg)\n\n    # Number of elements in the last dimensions\n    n = tf.reduce_prod(tensor.shape[-2:])\n\n    # Product of all dimension up to the last two\n    prod = tf.reduce_prod(shape_[:-2])\n\n    # Reshape last two dimensions into a single one\n    output = tf.reshape(tensor, tf.concat([shape_[:-2], [n]], 0))\n\n    # We want to keep the shape if the original tensor is a concrete matrix\n    # (i.e., the rank is two), otherwise, we add a least-dimension index\n    # at the end\n    is_matrix = (tf.rank(tensor) == 2)\n    new_shape = tf.concat([shape_[:-1] if is_matrix else shape_], 0)\n    output = tf.cast(output, tensor.dtype)\n    output.set_shape(new_shape)\n\n    return output\n```"
"Here is the definition of Mapper:  \n[sionna.mapping.Mapper(constellation_type=None, num_bits_per_symbol=None, constellation=None, return_indices=False, dtype=tf.complex64, **kwargs)](https://nvlabs.github.io/sionna/_modules/sionna/mapping.html#Mapper)  \n  \nHere is the source code of Mapper:  \n  \n```python\nclass Mapper(Layer):\n    \"\"\"Mapper(constellation_type=None, num_bits_per_symbol=None, constellation=None, return_indices=False, dtype=tf.complex64, **kwargs)\n\n    Creates a constellation and maps binary tensors to points of the\n    constellation.\n\n    This class defines a constellation mapper operating on the last dimension\n    of an input tensor. One can either create a mapper with a specific\n    constellation configuration or directly use an instance of\n    :class:`~sionna.mapping.Constellation`. In the latter case, the\n    ``constellation_type`` and ``num_bits_per_symbol`` are not required.\n\n    Parameters\n    ----------\n    constellation_type : One of [\"qam\", \"pam\", \"custom\"], str\n        For \"custom\", an instance of :class:`~sionna.mapping.Constellation`\n        must be provided.\n\n    num_bits_per_symbol : int\n        The number of bits per constellation symbol, e.g., 4 for QAM16.\n        Only required for ``constellation_type`` in [\"qam\", \"pam\"].\n\n    constellation : Constellation\n        An instance of :class:`~sionna.mapping.Constellation` or None.\n        In the latter case, ``constellation_type`` and\n        ``num_bits_per_symbol`` are required.\n\n    return_indices : bool\n        If enabled, symbol indices are additionally returned.\n        Defaults to False.\n\n    dtype : One of [tf.complex64, tf.complex128]\n        The output dtype. Defaults to tf.complex64.\n\n    Input\n    -----\n    : [...,n], tf.float or tf.int\n        A tensor with with binary entries. The last dimension\n        of the input tensor must be an integer multiple of the\n        number of bits per constellation symbol.\n\n    Output\n    : [...,n/Constellation.num_bits_per_symbol], tf.complex\n        The mapped constellation symbols.\n\n    : [...,n/Constellation.num_bits_per_symbol], tf.int32\n        The symbol indices corresponding to the constellation symbols.\n        Only returned if ``return_indices`` == True.\n\n    Note\n    ----\n    The last input dimension must be an integer multiple of the\n    number of bits per constellation symbol.\n    \"\"\"\n\n    def __init__(self,\n                 constellation_type=None,\n                 num_bits_per_symbol=None,\n                 constellation=None,\n                 return_indices=False,\n                 dtype=tf.complex64,\n                 **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n\n        # Create constellation object\n        self._constellation = Constellation.create_or_check_constellation(\n                                                                    constellation_type,\n                                                                    num_bits_per_symbol,\n                                                                    constellation,\n                                                                    dtype=dtype)\n\n        self._return_indices = return_indices\n\n    #########################################\n    # Public methods and properties\n    #########################################\n\n    @property\n    def constellation(self):\n        \"\"\"The :class:`~sionna.mapping.Constellation` used by the mapper.\"\"\"\n        return self._constellation\n\n    def constellation_type(self):\n        \"\"\"The type of constellation used by the mapper\"\"\"\n        return self.constellation.constellation_type\n\n    def num_bits_per_symbol(self):\n        \"\"\"The number bits per constellation symbol\"\"\"\n        return self.constellation.num_bits_per_symbol\n\n    def call(self, inputs):\n        bits = inputs\n\n        symbol_ind = self._binary_source2symbol_ind(bits)\n\n        constellation_points = self.constellation.points\n        constellation_points = tf.cast(constellation_points, dtype=bits.dtype)\n\n        # Gather the symbols based on the symbol indices\n        symbols = tf.gather(constellation_points, symbol_ind, axis=-1)\n\n        if self._return_indices:\n            return symbols, symbol_ind\n        else:\n            return symbols\n    #############################################\n\n    def _binary_source2symbol_ind(self, b):\n        \"\"\"This method expands b to the shape of the constellation points.\n        It is used to calculate the symbol indices. Please check the\n        documentation for the ``expand_to_symbol`` method of the\n        :class:`~sionna.mapping.Constellation` for details.\"\"\"\n        symbol_ind = self.constellation.expand_to_symbol(b)\n        return symbol_ind\n```  \n  "
"To configure the notebook to use only a single GPU and allocate only as much memory as needed, you can use the TensorFlow `tf.config` module. Here are the steps:\n\n1. Import the necessary modules:\n\n```python\nimport os\nimport tensorflow as tf\n```\n\n2. List the physical devices available for TensorFlow to use, in this case, GPUs:\n\n```python\ntf.config.list_physical_devices('GPU')\n```\n\n3. Select only the first GPU and set it to be the only one visible to the notebook:\n\n```python\ngpu_num = 0 # Use \"\" to use the CPU\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.set_visible_devices(gpus[gpu_num], 'GPU')\n        print('Using GPU ', gpu_num)\n    except RuntimeError as e:\n        print(e)\n```\n\n4. Set the GPU to use only as much memory as needed:\n\n```python\ntf.config.get_logger().set_mintempo_instalo(1)\ntf.config.get_logger().set_mintempo_desalo(1)\n```\n\nThese steps will configure the notebook to use the first GPU and allocate only as much memory as needed. It is important to set `tf.config.experimental.set_memory_growth` to `True` to allocate only as much GPU memory as needed for the runtime, and `tf.config.set_visible_devices` to ensure that only the selected GPU is used."
"Here is the definition of BinaryErasureChannel: [sionna.channel.BinaryErasureChannel(return_llrs=False, bipolar_input=False, llr_max=100.0, dtype=tf.float32, **kwargs)](https://nvlabs.github.io/sionna/_modules/sionna/channel/discrete_channel.html#BinaryErasureChannel)  \nSource code:\n```python\nclass BinaryErasureChannel(BinaryChannel):\n    \"\"\"Binary erasure channel.\n\n    In the binary erasure channel, bits are either correctly received or\n    erased (i.e., lost).\n\n    This can be seen as a channel where input :math:`x\\\\in\\\\{0,1\\\\}`  and erasure\n    probability :math:`p_\\\\text{b}\\\\in\\\\lbrack 0,1\\\\rbrack`  that can be\n    adjusted per bit.\n\n    In the 'bipolar' setting, bit values of '0' are mapped to '-1'.\n\n    ..  figure:: ../figures/BEC_channel.png\n        :align: center\n\n    This layer supports binary inputs (:math:`x\\\\in\\\\{0,1\\\\}`) and 'bipolar'\n    inputs (:math:`x\\\\in\\\\{-1,1\\\\}`).\n\n    If activated, the channel directly returns log-likelihood ratios (LLRs)\n    defined as\n\n    .. math::\n        \\\\ell =\n        \\\\begin{cases}\n            -\\\\infty, \\\\qquad \\\\text{if} \\\\, y=0 \\\\\\\\\n            0, \\\\qquad \\\\quad \\\\text{if} \\\\, y=? \\\\\\\\\n            \\\\infty, \\\\qquad \\\\quad \\\\text{if} \\\\, y=1\n        \\\\end{cases}\n\n    The bipolar mode is different from the binary mode in the following\n    aspect:\n\n    In the binary mode, the bit value is flipped (i.e., a `0` is mapped to a\n    `1`)\n\n    In the 'bipolar' mode, the bit value of `-1` is used in case of erasure.\n\n    The layer can be used with either scalar or tensor inputs. Only if the\n    ``return_llrs`` flag is set, the  layer requires a tuple\n    (:math:`x, p_\\text{b}`)  with ``x`` the channel input as a binary vector\n    and ``p_b`` the erasure probability as scalar or tensor (broadcastable\n    to the shape of ``x``).\n\n    Args:\n        return_llrs (bool): Defaults to `False`. If `True`, the layer returns\n            log-likelihood ratios instead of binary values based on `pb`.\n        bipolar_input (bool): Defaults to `False`. If `True`, the expected\n            input is given as {-1,1} instead of {0,1}.\n        llr_max (float): Defaults to 100. Defines the clipping value of the\n            LLRs.\n        dtype (tf.DType): Defines the datatype for internal calculations and\n            the output dtype. Defaults to `tf.float32`.\n\n    Input:\n        (x, pb) (Tuple):\n            x : tf.float32 : Input sequence to the channel consisting of binary\n                values {0,1} or {-1,1}, respectively.\n            pb : tf.float32 : Erasure probability. Can be a scalar or of any\n                shape that can be broadcasted to the shape of ``x``.\n\n    Output:\n        : tf.float32 : Output sequence of same length as the input ``x``. If\n            ``return_llrs`` is `False`, the output is ternary. Otherwise,\n            the output is defined by the LLR concept.\n    \"\"\"\n\n    def __init__(self, return_llrs=False, bipolar_input=False, llr_max=100., dtype=tf.float32, **kwargs):\n\n        super().__init__(return_llrs=return_llrs,\n                         bipolar_input=bipolar_input,\n                         llr_max=llr_max,\n                         erasure=True,\n                         dtype=dtype,\n                         **kwargs)\n```\nExplanation:  \nThis class implements the binary erasure channel, which is, essentially, a channel that can lead to loss. In a binary erasure channel, bits are either correctly received or erased (lost). The Binary Erasure Channel supports binary inputs ($x\\\\in\\\\{0,1\\\\}$ ) and 'bipolar' inputs ($x\\\\in\\\\{-1,1\\\\}`).\n\nIf the 'return_llrs' parameter is set to True, the class returns log-likelihood ratios (LLRs) instead of binary values based on the erasure probability ('pb'). The bipolar mode, unlike the binary mode, uses -1 as input when there's an erasure.\n\nWhen this layer is used with the 'bipolar' mode, it's important to note that the input {-1,1} is different from {0,1}. Specifically, in the bipolar mode, -1 is used in case of an erasure.\n\nErasure probability ('pb') can be a scalar or of any shape that can be broadcasted to the shape of the input. This allows different erasure probabilities per bit position. The datatype for internal calculations and the output is tf.float32 by default, but can be changed if needed.  \n  "
"COMPELATION REQUIRED:\n\nTo describe the weighted Belief Propagation (BP) decoding as proposed by Nachmani et al. [1], we need to refer to the various components required for the simulation. \n\n1. LDPC BP Decoder: This is the part of the decoder that performs the belief propagation algorithm. In the context, the BP Decoder is represented as a `LDPCBPDecoder(iter, cn_type, num_vnodes, vlen, hard_out, list_deg=[])` object from the Sionna package. The decoder object uses different trainable weights for outgoing variable node (VN) and check node (CN) messages.\n\n2. Gaussian LLR Source: This is the component that generates the channel outputs or simulated the channel. In the context, this component is implemented using the `GaussianPriorSource(s, dtype)` and `GaussianLLRSource(channel_type, dtype)` classes from the Sionna package. The former generates and passes Gaussian priors, while the latter generates and passes log-likelihood ratios (LLRs).\n\n3. We also need to define the concept of *all-zero codeword simulations*. In the context, it is mentioned that for training the LDPC decoder, the decoder is fed Gaussian distributed values instead of the codeword bits. This is achieved by passing an all-zero codeword to the encoder, hence the name *all-zero codeword-based simulations*.\n\n4. Additionally, we require a 'Gradient Scaling Trick' to apply the (scaled) gradient to all weights. This is used to avoid complicating the learning scenario with different weights that apply (or learn) different scalings.\n\nLet's make sure we reference all the relevant components from the provided context. \n\nPlease note that the description of the 'weighted Belief Propagation' algorithm and its implementation is based on the provided context, which includes specific references to 'Nachmani et al. [1]' and the Sionna package. This description may not cover all possible aspects or variations of weighted BP decoders."
"To use the `PUSCHDMRSConfig` class and its properties in Sionna's API, first, you have to create an instance of this class. For example, you can load a 5G NR compliance configuration as follows:\n\n```python\nnr_config = load_nr_config(1) # load the 5G NR compliance config with PUSCH configuration index 1\n```\n\nThen, you can access the `PUSCHDMRSConfig` through the `dmrs` attribute of the NR configuration:\n\n```python\ndmrs_config = nr_config.pusch.dmrs\n```\n\nNow, you can use the various properties of the `PUSCHDMRSConfig` class. Here are several of them:\n\n- `additional_position`: Maximum number of additional DMRS positions.\n- `allowed_dmrs_ports`: List of nominal antenna ports.\n- `beta`: Ratio of PUSCH energy per resource element (EPRE) to DMRS EPRE.\n- `config_type`: DMRS configuration type.\n- `delta`: List of delta (frequency) shifts for all ports in the port_set.\n- `dmrs_port_set`: List of used DMRS antenna ports.\n- `length`: Number of front-loaded DMRS symbols.\n- `n_id`: Scrambling identities.\n- `n_scid`: DMRS scrambling initialization.\n- `num_cdm_groups_without_data`: Number of CDM groups without data.\n- `type_a_position`: Position of the first DMRS OFDM symbol.\n- `w_f`: Frequency weight vectors $w_f(k')$ for all ports in the port set.\n- `w_t`: Time weight vectors $w_t(l')$ for all ports in the port set.\n\nYou can set most of these properties either during the initialization of the class or after the creation of the instance. For instance:\n\n```python\ndmrs_config.additional_position = 2\n```\n\nRemember also that some of these properties can be also accessed from the parent PUSCH configuration and not only from the `PUSCHDMRSConfig`. For example:\n\n```python\npusch_config = nr_config.pusch\npusch_config.additional_position = 2\n```\n\nApart from directly setting the properties, their values can also be assigned during the initialization of the class through keyword arguments (kwargs). For instance:\n\n```python\ndmrs_config = PUSCHDMRSConfig(additional_position=2, config_type=\"type-2\")\n```\n\nTo use the `PUSCHDMRSConfig` class in simulation for generating and processing signals, you will use it in conjunction with other classes such as `PUSCHConfig`, `CarrierConfig`, `PUSCHTransmitter`, `PUSCHReceiver`, etc. These are used to simulate the physical channels, antenna arrays, modulators, demodulators, etc., as per the 5G NR specifications."
"Here is the demonstration of how to encode information bits using an instance of `ConvEncoder`:\n\n```python\nimport tensorflow as tf\nfrom sionna.fec.conv import ConvEncoder\n\n# Example generator polynomial for a rate-1/2 convolutional code\ngen_poly = ('100', '110')\n\n# Instantiate a ConvEncoder\nencoder = ConvEncoder(gen_poly=gen_poly, rate=1/2)\n\n# Create information bits tensor, the second dimension is the length and\n# must be equal to the constraint length for the convolutional encoder\n# (e.g., 3 for the gen_poly = ('100', '110'))\n# Here we use a gen_poly with a constraint length of 3\nu = tf.constant([[1, 0, 1, 1, 0, 1]], dtype=tf.uint8)\nu = tf.squeeze(u, axis=0)  # Drop the \"batch\" dimension\n\n# Encode the information bits\nc = encoder(u)\n\nprint(\"Information bits tensor shape and type:\", tf.shape(u), u.dtype)\nprint(\"Codeword tensor shape and type:\", tf.shape(c), c.dtype)\n```\n\nOutput:\n```\nInformation bits tensor shape and type: tf.Tensor([ 6], shape=1, dtype=int32) <dtype='uint8'>\nCodeword tensor shape and type: tf.Tensor([12], shape=1, dtype=int32) <dtype='uint8'>\n```\n\nIn the output, we can see the shapes of the input and output tensors. The input tensor, `u`, consists of 6 information bits with a shape of (6,) and a type of int32. The output tensor, `c`, is the corresponding convolutional codeword and has a shape of (12,); it also has a type of int32. \n\nThis demonstrates the encoding of information bits using `ConvEncoder` from the `sionna.fec.conv` module. The example code uses a predefined generator polynomial for a rate-1/2 convolutional code and illustrates the instantiation of the encoder followed by the actual encoding operation."
"DMRS (Demodulation Reference Signal) configuration is critical for setting up a proper 5G NR (New Radio) communication system. In the given context, the DMRS configuration is explored in relation to a 5G PUSCH (Physical Uplink Shared Channel). The DMRS is used for channel estimation and it provides a set of reference signals known as the pilot pattern. These signals are used to aid the receiver in retrieving the transmitted data, given the effects of the wireless channel.\n\nTo visualize the pilot pattern of the DMRS configuration, the `show()` method of the `PUSCHDMRS` class is used. This method will plot the pilot symbols of a DMRS configuration on a resource grid. \n\nHere is the relevant code snippet from the context:\n\n```python\nfrom 5g_nr import PUSCHConfig, PUSCHDMRS\n# Configure PUSCH\npusch_config = PUSCHConfig()\npusch_config.dmrs = PUSCHDMRS(dmrs_port_set=[0])\n# Visualize the pilot pattern\npusch_config.dmrs.show()\n```\n\nIn this code, a PUSCH configuration `pusch_config` is first created. The `PUSCHDMRS` class is then instantiated with a specified `dmrs_port_set`. This set is a list of the DMRS port(s), which indicate the specific antenna port(s) used for transmission in the DMRS configuration. If set to `[0]`, it means that the first pilot signal is used. \n\nTo visualize the pilot pattern, the `show()` method is called on the `PUSCHDMRS` instance that was created based on the `PUSCHConfig`. This will generate a plot showing the arrangement of the pilot symbols on the resource grid.\n\nAs for the DMRS configuration itself, it is essential to note that there are many parameters that can be set or adjusted, such as the `CyclicPrefix`, `dmrs_length`, `dmrs_additional_position`, `num_cdm_groups_without_data`, and others. These parameters dictate the characteristics of the DMRS configuration. For instance, the `dmrs_length` parameter could be either `l0`, `l1`, `l2`, or `l3`, and the `dmrs_port_set` parameter is a list that contains the DMRS ports. \n\nTo gain insight into the configuration of a DMRS instance, it is recommended to review the pertinent classes' source code or documentation. For example, one might look into the source code or documentation for the `PUSCHDMRS` class."
"To visualize common scattering patterns using the Sionna package, you can follow these steps:\n\n1. Import the necessary modules from the Sionna package. This includes the `LambertianPattern` and `DirectivePattern` classes from the `sionna.rt` module.\n\n```python\nfrom sionna.rt import LambertianPattern, DirectivePattern\n```\n\n2. Create an instance of the required scattering pattern model. For example, you can create an instance of the Lambertian scattering pattern with the following code:\n\n```python\nlambertian_pattern = LambertianPattern()\n```\n\n3. To visualize the scattering pattern, you can use the `visualize` method of the `scattering_pattern` object. This will display the scattering pattern in a new figure. For example:\n\n```python\nlambertian_pattern.visualize()\n```\n\nThis will open a 3D visualization of the scattering pattern. The visualization includes the scattering pattern in space, as well as the visualization of the plane cut through the middle of the scattering lobe.\n\nFor the `DirectivePattern`, the resulting visualization will show the scattering pattern in space and the corresponding cut through the scattering lobe. The `alpha_r` parameter determines the width of the scattering lobe in the direction of the specular reflection.\n\nTo create an instance of the `DirectivePattern` with a specified `alpha_r`, you can use the following code:\n\n```python\ndirective_pattern = DirectivePattern(alpha_r=20)\n```\n\nThen, you can visualize the scattering pattern with:\n\n```python\ndirective_pattern.visualize()\n```\n\nThis will show the scattering pattern in space and the corresponding cut through the scattering lobe for each direction."
"To handle import errors for the Sionna package, the code first attempts to import the package using the `import sionna` statement inside a try-except block. If the import is successful, the package is made available for use in the rest of the code. \n\nIf the import statement raises an `ImportError`, the code then enters the `except` block. Inside the `except` block, the code first checks if the `sionna` package is not already installed by looking for an `ImportError` with the message \"No module named 'sionna'\". If this is the case, the code then installs the Sionna package using `pip install sionna`. After the package is installed, the code attempts to import it again. If the package is successfully imported this time, it is made available for use in the rest of the code. If the package still cannot be imported after installation, an `ImportError` with a custom message is raised, informing the user that the Sionna package could not be imported.\n\nTo ensure that the `pip install sionna` command installs the package into the correct environment, the `os` module is used to set the `CUDA_VISIBLE_DEVICES` environment variable to `\"\"` and the `TF_CPP_MIN_LOG_LEVEL` environment variable to `'3'`. The `CUDA_VISIBLE_DEVICES` variable determines which GPU is used for the installation, with `\"\"` meaning that the installation will use the CPU. The `TF_CPP_MIN_LOG_LEVEL` variable is used to set the logging level of TensorFlow, with `'3'` being the highest level (i.e., errors only).\n\nHere is the relevant part of the code:\n\n```python\ntry:\n    import sionna\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Use \"\" to use the CPU\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n    import sionna\n```\n\nIf the Sionna package depends on other packages that need to be installed, it should be noted that the above approach only installs the Sionna package and not its dependencies. It is recommended to use a package manager that resolves dependencies and installs them automatically. For Python packages, `pip` is the standard package manager, and it installs packages from the Python Package Index (PyPI), which usually include necessary dependencies."
"ANSWER: \n\nTo import the necessary Sionna APIs, you can follow the code snippets provided in the context. Here's how you do it:\n\nFor the Sionna RT API:\n\n```python\nfrom sionna.rt import load_scene, PlanarArray, Transmitter, Receiver, RadioMaterial, Camera\n\n# Your code using Sionna RT API here\n```\n\nFor the Sionna channel API:\n\n```python\nfrom sionna.channel import cir_to_ofdm_channel, subcarrier_frequencies, ChannelModel, OFDMSystem, ApplyOFDMChannel, CIRDataset, CIRGenerator\n\n# Your code using Sionna channel API here\n```\n\nFor the Sionna MIMO API:\n\n```python\nfrom sionna.mimo import StreamManagement, Encoder, Constellation, Mapper, Demapper, LinearEqualizer, LMMSEEqualizer, NonlinearityDetector, NearestNeighborDetector, GenerateOFDMData, DetectOFDMData, OFDMDemodulator, ZFPrecoder, RemoveNulledSubcarriers, ReplaceNulledSubcarriers, KBestDetector, EPDetector\nfrom sionna.ofdm import ResourceGrid, ResourceGridMapper, LSChannelEstimator, LMMSEInterpolator, LMMSEEqualizer, ZFPrecoder, ICIQuantizer, OFDMDemodulator, LinearDetector\n```\n\nFor the Sionna layer API:\n\n```python\nfrom sionna.utils import compute_ber, ebnodb2no, sim_ber\nfrom sionna import GaussianPrior, load_mnist\nimport sionna.utils as su\nfrom sionna.layer import Layer\nfrom tf.keras import Model\n```\n\nPlease, be sure you have installed the Sionna package before running the code in the Python environment. The APIs mentioned above are parts of the various Sionna components (RT, channel, MIMO, layer), so you should import the specific classes or functions based on what you are trying to accomplish. Ensure that the necessary context for each import is included in your code. \n\nFor using the APIs, refer to the Sionna's documentation or the respective component's documentation."
"To learn about the `Descrambler` class, its parameters, input, output, raised errors, and properties, I'd suggest referring to the corresponding source code documentation. Unfortunately, the context doesn't provide the exact source code of the `Descrambler` class or any code snippets that define it. \n\nI advise you to look into the source code where the class is actually defined. The code comments, or documentation, usually explain how a descrambler is related to a specific scrambler, and whether the descrambler involves binary operations or bit/soft-symbol operations. It should also detail the class's initialization and the operations that are being performed when a call is made to the descrambler. \n\nFor this, you may need to access the original Python source code of the library or framework that implements this class. The code documentation provided in the context, such as comments or descriptions in the source code documentation, should also be reviewed. Please note that the information provided in this response is general for understanding the elements of a software library or framework, and the specific details should be verified from the actual source code."
"To perform Polar decoding using an instance of `Polar5GDecoder` with log-likelihood ratios (LLR) as inputs, you'll first need to have the channel LLRs calculated. The LLRs are the demapped values from the received signal which provide information about the transmitted bits in the communication process. The LLRs are then passed to the decoder for the recovery of the transmitted message.\n\nHere's a general step-by-step guide on how to do this:\n\n1. Calculate the LLRs for the received signal: This step will depend on your specific setup and the type of signal received.\n\n2. Once you have the LLRs calculated, you can create an instance of the `Polar5GDecoder`. You will need to pass the frozen positions (positions in the codeword that are set to known values and are not used for transmitting information), the length of the Polar codeword (which should match the length of your original data), the type of decoder you want to use, and any additional parameters such as the list size and the number of iterations.\n\n```python\nfrom sionna.fec.polar.decoding import Polar5GDecoder\n\n# Define the parameters for the Polar5GDecoder\nfrozen_pos = [...]  # List of indices for the frozen positions\nn = ...            # Length of the Polar codeword\ndec_type = \"SC\"    # Type of decoder, could also be \"SCL\" or \"BP\"\nlist_size = 8       # List size, only needed for SCL decoder\nnum_iter = 20      # Number of iterations, only needed for SCL and BP decoders\n\n# Create an instance of the Polar5GDecoder\npolar_decoder = Polar5GDecoder(frozen_pos=frozen_pos,\n                                n=n,\n                                dec_type=dec_type,\n                                list_size=list_size,\n                                num_iter=num_iter)\n```\n\n3. After the `Polar5GDecoder` is created, you can call it with the LLRs of the received signal as an argument. The decoder will perform the error correction process and return the estimated information bits.\n\n```python\n# llr_ch is a tensor containing the channel LLRs\nc_hat = polar_decoder(llr_ch)\n```\n\nEnsure that `llr_ch` is a tensor containing the channel LLRs that match the format expected by the `Polar5GDecoder`. The shape of `llr_ch` should be `[..., n]`, where `n` is the length of the Polar codeword.\n\nRemember that the `Polar5GDecoder` is a wrapper that makes use of the internal SCL (Successive Cancellation List) decoder. If you want to use a different type of decoder or another SCL library, it might be preferable to use the `PolarDecoder` directly and provide a binary index as input."
"To load a scene from a file using the `load_scene()` function from Sionna's `rt` module, follow these steps:\n\n1. Import the necessary function from the `sionna.rt` module.\n    ```python\n    from sionna.rt import load_scene\n    ```\n\n2. Specify the filename of the scene you want to load. In this example, the filename is `my_scene.mitsuba`.\n    ```python\n    filename = 'my_scene.mitsuba'\n    ```\n\n3. Call the `load_scene()` function with the `filename` as an argument. Assign the output to a variable. \n    ```python\n    scene = load_scene(filename)\n    ```\n\nThe `load_scene()` function will parse the file in the Mitsuba 3 XML-based format and load the scene. If the file is valid, the function will return a `Scene` object representing the loaded scene. If the file is not valid or does not exist, the function will return `None`.\n\nReplace `'my_scene.mitsuba'` with the actual filename of your scene. Make sure to provide the correct path if the scene file is not in the current working directory.\n\nLater, you can manipulate or access the loaded scene using the returned `Scene` object. For example, you can visualize the scene using the `preview()` function:\n\n```python\nscene.preview()\n```"
