"To create a `Polar5GEncoder` instance in the Sionna framework, you need to define the parameters `k` and `n`. The `k` parameter is an integer that represents the number of information bits per codeword you want to encode. The `n` parameter is also an integer and it's the codeword length you want the encoder to produce.\n\nHere's how you can define these parameters when instantiating a `Polar5GEncoder`:\n\n```python\nfrom sionna.fec.polar.encoding import Polar5GEncoder\n\n# Define the parameters k and n\nk = 100  # The number of information bits per codeword\nn = 200  # The codeword length\nencoder = Polar5GEncoder(k=k, n=n)\n```\n\nIn this example, the values chosen for `k` and `n` are 100 and 200 respectively. You can replace these with any other appropriate values depending on the specific requirements of the polar code you want to use. Just ensure that both `k` and `n` are positive integers and that `k` is less than or equal to `n`, as these are the basic constraints for any Polar code length."
"To import DeepMIMO and generate a dataset, follow these steps:\n\n1. Import the necessary libraries. This includes `os` for interacting with the operating system, `numpy` for numerical computations, `h5py` for handling HDF5 files, and `sys` for interacting with the Python interpreter, among others.\n\n```python\nimport os\nimport numpy as import numpy as np\nimport h5py\nimport sys\n```\n\n2. Add the DeepMIMO repository to your system's path. If it's not already installed, the system will prompt the user to clone it from GitHub.\n\n```python\nif 'DeepMIMO' not in sys.path:\n    os.system('git clone https://github.com/DeepMIMO/DeepMIMO.git')\n```\n\n3. Import the DeepMIMO package. If it's not already installed, the system will prompt the user to install it using `pip`.\n\n```python\nsys.path.append('DeepMIMO')\ntry:\n    import DeepMIMO\nexcept ImportError as e:\n    os.system('pip install DeepMIMO')\n    import DeepMIMO\n```\n\n4. Set the desired configuration, including the scenario, the number of paths, the active base stations, the user rows to be included, and the antenna array configurations.\n\n```python\nconfig = DeepMIMO.default_config()\nconfig['scenario'] = 'UMi'\nconfig['set_of_x'] = DeepMIMO.get_set_of_x(DeepMIMO.Constellation.qam_256, config['num_rx_ant'])\n```\n\n5. Generate the dataset. This involves creating the HDF5 file that will store the dataset and initializing it with the appropriate structure.\n\n```python\nwith h5py.File('DeepMIMO_dataset.h5', 'w') as f:\n    f.create_dataset('paths', (0, num_paths, num_rx_ant, num_tx_ant, num_ofdm_symbols, fft_size), maxshape=(None, num_paths, num_rx_ant, num_tx_ant, num_ofdm_symbols, fft_size), dtype=np.complex6)\n    f.create_dataset('paths_norm', (0, num_rx_ant, num_tx_ant, num_ofdm_symbols, fft_size), maxshape=(None, num_rx_ant, num_tx_ant, num_ofdm_symbols, fft_size), dtype=np.complex64)\n    f.create_dataset('channels', (0, num_rx, num_tx, num_ofdm_symbols, fft_size), maxshape=(None, num_rx, num_tx, num_ofdm_symbols, fft_size), dtype=np.complex64)\n    f.create_dataset('channels_time', (0, num_rx, num_tx, channel_duration), maxshape=(None, num_rx, num_tx, channel_duration), dtype=np.complex64)\n    f.create_dataset('x', (0, num_tx, num_tx_ant, num_ofdm_symbols, fft_size), maxshape=(None, num_tx, num_tx_ant, num_ofdm_symbols, fft_size), dtype=np.comcomplex64)\n```\n\n6. Generate the dataset metadata and save it to the HDF5 file.\n\n```python\nmetadata = {'num_tx': num_tx, 'num_rx': num_rx, 'num_tx_ant': num_tx_ant, 'num_rx_ant': num_rx_ant, 'num_ofdm_symbols': num_ofdm_symbols, 'fft_size': fft_size, 'subcarrier_spacing': 15e3, 'carrier_freq': 3.5e9, 'scenario': config['scenario'], 'path_optimizer': path_oriented_solver[config['scenario']], 'path_generato\n    f.write('metadata', metadata, metadata)\u000b\n```\n\nPlease note that the code snippets provided are based on the context of the DeepMIMO package and may need to be adjusted based on your specific requirements."
"To plot the weights after training and compare the new Bit Error Rate (BER) performance, you can follow these steps:\n\n1. After training your model, get the weights of the model using the `get_weights()` function. This will return a list of arrays with the weights of each layer. \n\n```python\nweights = model.get_weights()\n```\n\n2. Plot the weights using matplotlib. You can use the `scatter` function to plot the real and imaginary parts of the weights against each other. \n\n```python\nplt.figure(figsize=(12,8))\nplt.grid()\nplt.title('Weights after training')\nplt.xlabel('Real part')\nplt.ylabel('Imaginary part')\nplt.scatter(np.real(weights), np.imag(weights))\nplt.show()\n```\n\n3. Make predictions with the untrained model and the trained model, and compute the BER. You can use the `predict` function of the model to make predictions, and the `compute_ber` function to compute the BER.\n\n```python\n# Untrained model\nmodel.set_weights(wo_tx)\nber_before_training = compute_ber(y, y_pred)\n\n# Model after one training iteration\nmodel.set_weights(weights)\ny_pred = model(x)\nber_after_training = compute_ber(y, y_pred)\n```\n\n4. Compare the BER before and after training. \n\nRemember to define the `compute_ber` function before using it, it is not included in the provided context. You can define it as follows:\n\n```python\ndef compute_ber(b, b_pred):\n    b = np.squeeze(b)\n    b_pred = np.squeeze(b_pred)\n    ind_zero = np.where(b == 0)\n    b_pred = np.sign(b_pred)\n    b_pred[ind_zero] = 0\n    return np.mean(b != b_pred)\n```\n\nThis function computes the BER between the binary vector `b` and the binary vector `b_pred`. It first converts the predicted bits to signs, and then sets to zero the elements where `b` is zero. Finally, it computes the mean of the comparison between `b` and `b_pred`, which gives the BER."
"To create a Polar transformation matrix for a given Kronecker power, you can use the `generate_polar_transform_mat` function from the `sionna.fec.polar.utils` module. This function requires the `n` parameter, which is the desired matrix dimension, and the `dtype` parameter, which is an optional parameter to set the data type of the output. \n\nHere is a step-by-step guide:\n\n1. Import the `generate_polar_transform_mat` function from the `sionna.fec.polar.utils` module.\n\n```python\nfrom sionna.fec.polar.utils import generate_polar_transform_mat\n```\n\n2. Define the parameters for the `generate_polar_transform_mat` function. The `n` parameter is the desired matrix dimension and must be a power of 2. The `dtype` parameter is an optional parameter to set the data type of the output. By default, the output data type is `np.float32`.\n\n```python\nn = 1024  # Desired matrix dimension\ndtype = np.float32  # Define the data type of the output\n```\n\n3. Call the `generate_polar_transform_mat` function with the parameters defined in the previous step. \n\n```python\npolar_tt_mat = generate_polar_transform_mat(n, dtype=dtype)\n```\n\nThis function returns a numpy array containing the Polar transformation matrix. The array type is determined by the `dtype` parameter. If the `dtype` parameter is set to `np.float32`, the array will have the data type `np.float32`. \n\nPlease note that the `generate_polar_transform_mat` function will only work for values of `n` that are a power of 2."
"Here is the definition of LayerDemapper: sionna.nr.LayerDemapper(layer_mapper, num_bits_per_symbol, **kwargs)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/nr/layer_mapping.html#LayerDemapper)  \n\nsource code:\n```python\nclass LayerDemapper(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"LayerDemapper(layer_mapper, num_bits_per_symbol, **kwargs)\n\n    Demaps MIMO layers to coded transport block(s) prior to LDPC decoding.\n\n    This layer takes as input a MIMO layer stream and demaps it to a list of\n    streams corresponding to the individual transport blocks. If more than\n    one transport block is output, the transport blocks are reordered such\n    that the are in ascending order.\n\n    The class is implemented as a Keras layer.\n\n    Parameters\n    ----------\n    layer_mapper : LayerMapper\n        A ``LayerMapper`` instance.\n\n    num_bits_per_symbol : int\n        The number of bits per QAM symbol, e.g., 4 for QAM16.\n\n    Input\n    -----\n    : [batch_size, num_rx, num_streams_per_rx, num_coded_bits], tf.float\n        2+D tensor containing the MIMO layer symbol values.\n\n    Output\n    ------\n    : [batch_size, num_rx, num_streams_per_rx, num_tb * num_bits_per_symbol], tf.float\n        2+D tensor containing the bits of the transport block(s).\n\n    Note\n    ----\n    If the ``num_layers`` property of the ``layer_mapper`` is set to ``None``,\n    this layer does not perform any processing and returns the input\n    unaltered. This is useful for simulations that do not use a certain number\n    of the available MIMO streams.\n    \"\"\"\n    def __init__(self,\n                 layer_mapper,\n                 num_bits_per_symbol,\n                 **kwargs):\n        super().__init__(**kwargs)\n        assert(layer_mapper.num_tw*layer_mapper.num_streams_per_tx\n               == layer_mapper.num_codewords)\n        self._layer_mapper = layer_mapper\n        self._num_bits_per_symbol = num_bits_per_symbol\n        if self._layer_mapper.num_layers[0]==1:\n            print_warning(\"The use of LayerMapper without shaping may not be supported by all downstream algorithms.\")\n\n    def call(self, inputs):\n        x = inputs\n        num_tb = len(self._layer_mapper.tb_cc_msgs[0])\n        num_coded_bits = self._layer_mapper.cb_encoder.coded_bits.shape[-1]\n\n        # If num_layers > 1, the layers are shaped prior to combining\n        if self._layer_mapper.num_layers[0]>1:\n            x = split_dim(x, self._layer_mapper.num_layers, 2)\n\n        # Further layer demapping\n        x = reshape(x, [-1, num_tb, self._layer_mapper.num_bits_per_symbol])\n\n        # If num_bits_per_symbol < the number of TB-critical section,\n        # additional bits are punctured\n        if self._num_bits_per_symbol<np.min([num_coded_bits, 4*num_tb]):\n            num_extra_bits = np.min([num_coded_bits, 4*num_tb]) - \\\n                             self._num_bits_per_symbol\n            x = tf.concat([x[...,:-num_extra_bits],\n                           tf.zeros_like(x)[...,num_coded_bits-num_extra_bits:]],\n                          axis=-1)\n\n        x = flatten_last_dims(x, 2)\n\n        # Layer de-mapping\n        x = self._layer_mapper(x)\n\n        # If more than one TB is provided, the TBs are concatenated\n        x = split_dim(x, num_tb,  2)\n        x = tf.concat(x, axis=-1)\n        return x\n```\n  "
"To create a PUSCHReceiver using the PUSCHTransmitter and simulate transmissions over the AWGN channel, follow these steps:\n\n1. First, import the necessary modules and classes.\n\n```python\nfrom nvidia.cuda import PUSCHTransmitter, PUSCHReceiver\nimport tensorflow as pd\n```\n\n2. Create a PUSCHTransmitter with default settings.\n\n```python\npusch_transmitter = PUSCHTransmitter()\n```\n\n3. Then, create a PUSCHReceiver using the PUSCHTransmitter. This will automatically generate the settings for the receiver based on the transmitter settings.\n\n```python\npusch_receiver = PUSCHReceiver(pusch_transmitter)\n```\n\n4. Now, you can simulate transmissions over the AWGN channel. Generate a batch of transmit signals and information bits using the PUSCHTransmitter.\n\n```python\nbatch_size = 16\nno = 0.1 # Noise variance\nx, b = pusch_transmitter(batch_size) \n```\n\n5. Instantiate a PUSCHReceiverInput and feed it with the transmit signal and noise.\n\n```python\nfrom pusch_receiver import PUSCHReceiverInput\nx_no = tf.sqrt(no)*tf.complexRand([batch_size, 2048])\npusch_receiver_input = PUSCHReceiverInput(b, x_no)\n```\n\n6. Finally, use the PUSCHReceiver to recover the information bits.\n\n```python\nb_hat = pusch_receiver(pusch_receiver_input)\n```\n\nPlease note that this code will only work if the necessary packages and modules are installed and the GPU settings are properly configured."
"Here is the definition of SymbolInds2Bits: sion.mimo.SymbolInds2Bits(num_bits_per_symbol, symbol_inds, return_seq=False, dtype=tf.float32, **kwargs)  \n  \n[Source code](https://nvlabs.github.io/sion/_modules/sion/mimo/utils.html#SymbolInds2Bits):  \n```python\nclass SymbolInds2Bits(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"SymbolInds2Bits(num_bits_per_symbol, symbol_inds, return_seq=False, dtype=tf.float32, **kwargs)\n\n    Computes for a given vector of continuous constellation-indicators\n    `symbol_inds` the corresponding bit sequence. The constellation of\n    equally likely symbols is used.\n\n    Parameters\n    ----------\n    num_bits_per_symbol : int\n        The number of bits per constellation symbol, e.g., 4 for QAM16.\n\n    symbol_inds : [batch_size, num_symbols], tf.int\n        The indices of the constellation symbols.\n\n    return_seq : bool\n        If `True`, the bit sequence for each symbol index is returned.\n        Defaults to `False`.\n\n    Input\n    -----\n    symbol_inds : [batch_size, num_symbols], tf.int\n        The indices of the constellation symbols.\n\n    Output\n    ------\n    : [batch_size, num_bits_per_symbol] or [batch_size, num_symbols, num_bits_per_symbol], tf.float\n        Corresponding bit sequence. If ``return_seq`` is `True`,\n        the bit sequence is returned for each symbol index.\n    \"\"\"\n    # pylint: enable=C0301\n\n    def __init__(self, num_bits_per_symbol, symbol_inds, return_seq=False,\n                 dtype=tf.float32, **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        self._return_seq = return_seq\n        self._num_symbols = int(symbol_inds.shape[1'])\n        self._symbol_inds = tf.as_dtype(symbol_inds.dtype).cast(symbol_inds)\n        self._num_bits_per_symbol = num_bits_per_symbol\n\n    def call(self, inputs):\n        bits = tf.concat([tf.reshape(to_one_hot(x, self._num_bits_per_symbol),\n                                     [-1, self._num_bits_per_symbol]) for x in self._symbol_inds], -1)\n        if self._return_seq:\n            bits = tf.gather(bits, self._symbol_inds, batch_dims=1, axis=-1)\n        return bits\n```  \n  "
"Here is the detailed information of TimeChannel:   \n  \n[sionna.channel.TimeChannel(channel_model, bandwidth, num_time_samples, l_min, l_max, normalize_channel=False)](https://metro.telecom.ptv.ru/redmine/projects/sionna/repository/nvcc/show/sionna/channel/rag/time_channel.py#TimeChannel)  \n  \nGenerate channel responses by filtering white Gaussian noise with a time-variant channel impulse response generated by the channel_model.\n\nThe channel impulse response is of finite length [l_min, l_max] in terms of the number of time samples, and depending on the bandwidth and the selected maximum delay spread in normalized time (``time_dispersion``). For each batch example, the channel model is generated for the time samples, and the channel polynomial representation is filtered through the white Gaussian noise.\n\nThe channel model is expected to generate channel impulse responses of constant length ``l``, that can be filtered by the channel_filter. Then, the channel impulse responses are zero padded to the maximum delay spread ``l_max - l_min + 1``.\n\n**Parameters**\n\n- `channel_model` (ChannelModel object): An instance of a ChannelModel object, such as RayleighBlockFading or UMi.\n- `bandwidth` (float): Bandwidth ($B_W$) [Hz]\n- `num_time_samples` (int): Number of time samples forming the channel impulse response\n- `l_min` (int): Smallest delay tap index for all time samples. It is equal to the largest negative delay [sample].\n- `l_max` (int): Largest delay tap index for all time samples. It is equal to the largest positive delay [sample].\n- `normalize_channel` (bool): If set to True, the channel is normalized over the block size consisting of ``num_time_samples`` time samples.\n\n**Input**\n\n- `batch_size` (int): Batch size. Defaults to None for channel models that do not require this parameter.\n\n**Output**\n\n- `a` ([batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_time_samples (l_max - l_min + 1)], tf.complex): Channel frequency responses for all subcarriers and time steps for all links. Time-variant channel impulse responses are first generated from the channel impulse response generated by the `channel_model`, and then filtered through white Gaussian noise. sionna.channel.utils.fft_channel is used for the channel impulse response computation to generate accurate channel frequency responses.  \n  \nsource code: [channel.TimeChannel](https://nvlabs.github.io/sionna/_modules/sionna/channel/time_channel.html#TimeChannel)."
"The 'Antenna' class is part of the 'Sionna.Channel' module and is used to compute the time-domain response of an antenna placed on a moving terminal. There are multiple classes that are inherited by this class, and it defines a channel impulse response corresponding to the antenna pattern.\n\nThe core of this class are the three attributes: \n\n1. `orientation` - This attribute sets the orientation of the antenna array. The orientation is set using a 3-element vector that indicates the yaw, pitch, and roll angles. \n\n2. `polarization` - This attribute indicates the polarization of the antenna array, which can be either 'V' (vertical) or 'H' (horizontal). For non-cross-polarized antennas, it can be either 'V' or 'H'. \n\n3. `antenna_pattern` - This attribute sets the antenna pattern of the antenna. It can be set to 'iso', which implies an isotropic antenna pattern. \n\nIn the 'Antenna' class, methods include:\n\n- `visualize(pattern)`: This method is used to visualize the antenna pattern. \n- `normalize_rad(x)`: This function is used to force values in radians to be within the range of -\u03c0 and \u03c0.\n- `normalize_vec(x)`: This function is used to normalize vectors.\n- `slant_angle(theta, phi, theta_s, phi_s)`: This function is used to compute the slant angle of a pair of incoming and outgoing rays.\n- `theta_s(lam, theta_0, alpha)`: This function is used to compute the zenith angle of the specular reflection. \n- `phi_s(lam, theta_0, alpha, theta_s)` : This function is used to compute the azimuth angle of the specular reflection. \n- `theta_i(lam, alpha, theta_s)` : This function is used to compute the incidence zenith angle. \n- `phi_i(lam, alpha, theta_s, phi)`: This function is used to compute the incidence azimuth angle. \n- `theta_r(lam, alpha, theta_s, phi, theta_i, phi_i)`: This function is used to compute the reflection zenith angle. \n- `phi_r(lam, alpha, theta_s, phi, theta_i, phi_i)`: This function is used to compute the reflection azimuth angle. \n\nIn the context of code, you would use this class as follows:\n\n```python\nfrom sionna.channel.tr38901 import Antenna\n\nantenna = Antenna(\"CIRCULAR\", \"RIGHT\", \"V\")\nantenna.show()\n```\n\nIn terms of properties, there are several, including:\n\n- `AntennaArray.antenna_patterns`: This property returns the antenna patterns of all antennas in the array.\n- `AntennaArray.num_ant`: This property indicates the number of antennas in the array.\n- `AntennaArray.polarization`: This property indicates the polarization of the antennas in the array.\n- `AntennaArray.antenna_pattern`: This property indicates the called function to compute the antenna patterns.\n- `AntennaArray.orientation`: This property indicates the orientation of the antenna array.\n- `AntennaArray.weights`: This property indicates the complex-valued amplitude and phase weights for each antenna element in the array.\n\nRemember, the class is defined as part of the 'sionna.channel.tr38901' module and it's always good to refer to the official documentation for any changes or updates."
"source code link: https://nootropicattic.com/source/python/deepym/class_turbo_termination.py\n  "
"To wrap everything in a Keras model for convenient Bit Error Rate (BER) simulations and comparison of model parameters, you can follow these steps:\n\n1. **Importing Necessary Libraries and Modules**: You should start by importing the necessary libraries and modules. This includes TensorFlow, Keras, and various modules from the Sionna library.\n\n```python\nimport tensorflow as tf\nfrom tensorflow import keras\nimport sionna\nfrom sionna.mimo import StreamManagement\nfrom sionna.ofdm import ResourceGrid, ResourceGridMapper, LSChannelEstimator, LMMSEEqualizer\nfrom sjsonna.channel import GenerateOFDMChannel, GenerateTimeChannel, OFDMChannel, TimeChannel\nfrom sionna.channel import RayleighBlockFading, OFDMChannel, TimeChannel\nfrom sionna.channel.tr38901 import Antenna, PanelArray, ULA\nfrom sionna.traffic import CQIChooser, DatasetSampler\nfrom sionna.udp import DataGenerator, SubbandDataGenerator\nfrom sionna.utils import OneHotEncoder, Padding, Woodbury, RemoveRankDeficiency\n```\n\n2. **Defining the Model**: You should define your model as a class that inherits from the `keras.Model` class. This class should include the initialization of all the components of your model, and a `call` method that describes the forward pass of your model.\n\n3. **Initializing the Model**: You should initialize your model by calling the `__init__` method of the `keras.Model` class. This is where you should pass any initial parameters to your model.\n\n4. **Forward Pass of the Model**: The `call` method of your model should describe the forward pass of your model. This is where you should define the steps to take in order to compute the output of your model given some input.\n\n5. **Simulating and Evaluating the Model**: After defining your model, you can simulate and evaluate it. The `simulate` method of the `sionna.mimo.PUMIMOBERSimulator` class can be used to simulate the model and compute the BER. You can then compare the BER of different model configurations.\n\nHere is a simplified example of how you might define a Keras model for simulating BER over an OFDM channel:\n\n```python\nclass OFDMChannelModel(keras.Model):\n    def __init__(self, ofdm_channel, cqi_values, num_tx, batch_size, **kwargs):\n        super().__init__(**kwargs)\n        self._ofdm_channel = ofdm_channel\n        self._cqi_values = cqi_values\n        self._num_tx = num_tx\n        self._batch_size = batch_size\n        self._resource_grid = ResourceGrid(*ofdm_channel.resource_grid)\n        self._rg_mapper = ResourceGridMapper(self._resource_grid)\n        ...\n    def call(self, batch_size):\n        ...\n        cqi_ind = tf.random.categorical([], num_samples, cqi_table)\n        cqi = tf.gather(self._cqi_values, cqi_ind, axis=0)\n        ...\n        return cqi, channel_freq, no\n    ...\n```\n\nPlease note that this is a simplified example and the actual implementation may require additional steps and components depending on the specific requirements of your model."
"The \"build_type_grid\" method of the \"ResourceGrid\" class is not explicitly provided in the context. Thus, it's not possible to interpret the exact function, inputs, and outputs of this method from the given context. \n\nTo fully understand how this method works, you would need to refer to the source code or documentation of the Sionna API associated with the \"ResourceGrid\" class. \n\nHere is what we know about the \"build_type_grid\" method based on the cited context:\n\nIt is a method of the \"ResourceGrid\" class used to build a tensor that maps resource elements to their respective resource element types. The method spaces the resource element types (DMRS, pilots, and data) on the resource grid and generates a tensor that maps resource elements to their respective types.\n\nWe can assume the input parameters for this function based on similar methods in the \"ResourceGrid\" class:\n\n- `num_dmrs_clusters`: This parameter might represent the number of DMRS (Definable Reference Signal) clusters in the system.\n- `num_dmrs_ports`: This parameter might indicate the number of DMRS ports used per DMRS cluster.\n- `num PilotSymbols`: This could represent the number of pilot symbols.\n- `num_data_symbols`: This might represent the number of data symbols per OFDM slot.\n- `num_layers`: This might represent the number of MIMO layers.\n- `rais`: This is a reference signals indicating if \"r\" is transmitted or not.\n- `num_cdm_groups`: This might indicate the number of CDM groups.\n- `cdm_groups`: This parameter might indicate the nature of the CDM groups. \n\nAgain, these are assumptions, and the actual parameters would depend on the specific usage of the method.\n\nThe output of this function is likely a tensor that maps the resource elements to their resource element types. The dimensions of the tensor would correspond to the resource grid and the different resource element types. However, since the exact output is not provided in the context, we can't say for sure. \n\nPlease note that the above explanation is based on typical functionalities of methods of the \"ResourceGrid\" class. For the precise use of the \"build_type_grid\" method, verify it from the original Sionna API documentation or source code."
"Here is the detailed information of SymbolLogits2LLRs:   \n  \nLink of the source code: https://nvlabs.github.io/sionna/_modules/sionna/mapping.html#SymbolLogits2LLRs  \n  \nComputes log-likelihood ratios (LLRs) or hard-decisions on bits from a tensor of logits (i.e., unnormalized log-probabilities) on constellation points. If option with_prior is used, prior knowledge on the bits is assumed to be available.\n\nThe class inherits from the Keras layer class and can be used as layer in a Keras model.  \n  \n### Parameters\n\n- **method** (`str`): \"app\" or \"maxlog\". Determines the method used for computing the LLRs.\n\n- **num_bits_per_symbol** (`int`): The number of bits per constellation symbol, e.g., 4 for QAM16.\n\n- **hard_out** (`bool`): If True, the layer provides hard-decided bits instead of soft-values. Defaults to False.\n\n- **with_prior** (`bool`): If True, it is assumed that prior knowledge on the bits is available. This prior information is given as LLRs as an additional input to the layer. Defaults to False.\n\n- **dtype** (`tf.DType`): The dtype for the input and output. Defaults to tf.float32.\n\n### Input\n\n- **logits** (`[..., n, num_points]`, `tf.float`): Logits on constellation points.\n\n- **prior** (`[num_bits_per_symbol]` or `[..., n, num_bits_per_symbol]`, `tf.float`): Prior for the computation of the LLRs. Must be provided only if option with_prior is used.\n\n### Output\n\n- **Output** (`[..., n * num_bits_per_symbol]`, `tf.float` or `[..., n * num_bits_per_symbol]`, `tf.float`): LLRs or hard-decisions for every bit.\n\n**Note: ** \nWith the \u201capp\u201d method, the LLR for the $i\\text{th}$ bit is computed according to $LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=1\\lvert \\mathbf{z},\\mathbf{p}\\right)}{\\Pr\\left(b_i=0\\lvert \\mathbf{z},\\mathbf{p}\\right)}\\right) =\\ln\\left(\\frac{\n        \\sum_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n        e^{z_c}\n        }{\n        \\sum_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n        e^{z_c}\n        }\\right)$  \n  \nwhere $\\mathcal{C}_{i,1}$ and $\\mathcal{C}_{i,0}$ are the sets of $2^K$ constellation points for which the bit is equal to jsonld,\n$\\mathbf{z} = \\left[z_{c_0},\\dots,z_{c_{2^K-1}}\\right]$ is the vector of logits on the constellation points,\nand $\\mathbf{p} = \\left[p_0,\\dots,p_{K-1}\\right]$ is the vector of LLRs that serves as prior knowledge on the $K$ bits that are mapped to a constellation point, assumed to be equal for all bits and all constellation points. \n$\\Pr\\left(c\\lvert\\mathbf{p}\\right)$ is the prior probability on the constellation symbol $c$:  \n$\\Pr\\left(c\\lvert\\mathbf{p}\\right) = \\prod_{k=0}^{K-1} \\Pr\\left(b_k = \\ell(c)_k \\lvert\\mathbf{p} \\right)\n= \\prod_{k=0}^{K-1} \\text{sigmoid}\\left(p_k \\ell(c)_k\\right)$  \n  \nwhere $\\ell(c)_k$ is the $k^{th}$ bit label of $c$, where 0 is replaced by -1. The definition of the LLR has been chosen such that it is equivalent with that of logits. This is different from many textbooks in communications, where the LLR is defined as $LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=0\\lvert y\\right)}{\\Pr\\left(b_i=1\\lvert y\\right)}\\right)$.  \n  \nWith the \u201cmaxlog\u201d method, LLRs for the $i\\text{th}$ bit are approximated like $\\begin{align}\n    LLR(i) &\\approx\\ln\\left(\\frac{\n        \\max_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n            e^{z_c}\n        }{\n        \\max_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n            e^{z_c}\n        }\\right)\n        .\n\\end{align}$.\n. The maxlog trick is equivalent with the former definition for LLRs, when assuming that the soft-output of the APP demapper equals the a posteriori probability for the constellation points. This approximation is used in practice, as it often simplifies the implementation of higher layers."
"Here is the detailed information of MaximumLikelihoodDetector:   \n  \n[sion.mimo.MaximumLikelihoodDetector(output, num_bits_per_symbol, hard_out=False, dtype=tf.complex64, **kwargs)](https://json000.github.io/nvlabs-sion/fec/mimo.html#sion.mimo.MaximumLikelihoodDetector) \n\nMIMO maximum-likelihood (ML) detector\n\nThis layer implements MIMO maximum-likelihood (ML) detection for arbitrary constellations. Both demapping of symbols or bits with either soft- or hard-decisions are supported. The computation is numerically stabilized.\n\n**Parameters**\n\n- `output` (str): Type of output, either \"bit\" for bits or \"symbol\" for symbols. Configuration for soft- or hard-dexisions is controlled by the `hard_out` flag.\n- `num_bits_per_symbol` (int): Number of bits per constellation symbol, e.g., 4 for QAM16.\n- `hard_out` (bool): If true, the detector computes hard-decided bit values or constellation point indices instead of soft-values. Defaults to False.\n- `dtype` (One of [tf.complex64, tf.complex128], tf.DType): The dtype of `y`. Defaults to tf.complex64. The output dtype is the corresponding real dtype (tf.float32 or tf.float64).\n\n**Input**\n\n- `(y, h, s)` \u2013 Tuple:\n  - `y` ([..., M], tf.complex): 1+D tensor containing the received signals.\n  - `h` ([..., M, num_streams], tf.complex): 2+D tensor containing the channel matrices.\n  - `s` ([..., M, M], tf.complex): 2+D tensor containing the noise covariance matrices.\n\n**Output**\n\nDepending on the `output` setting:\n- If `output` equals \"bit\":\n  - [..., num_streams, num_bits_per_symbol], tf.float: LLRs or hard-decisions for every bit of every stream.\n- If `output` equals \"symbol\":\n  - [..., num_streams, 2**num_bits_per_symbol], tf.float or [..., num_streams], tf.int: Logits or hard-decisions for constellation symbols for every stream. Hard-decisions correspond to the constellation symbol indices.\n\n**Note:** If you want to use this layer in Graph mode with XLA, i.e., within a function that is decorated with @tf.function(jit_compile=True), you must set sion.mimo.MaximumLikelihoodDetector.polynomial_scaling = True. See polynomial_scaling for more details."
"To initialize a `TurboEncoder` with custom generator polynomials, one needs to provide the `gen_poly` argument with the specified generator polynomials. The `gen_poly` parameter should be a tuple of strings, where each string represents a generator polynomial in binary format. Each polynomial is represented as a binary string, taking the form `ax^3 + bx^2 + cx^1 + dx^0 = 0`, with `1` as the most significant coefficient and `0` as the least significant coefficient.\n\nHere's the format for the `gen_poly` tuple:\n```python\ngen_poly = ('101', '111')\n```\n\nLet's break down the example binary sequence \"10011\". This sequence represents the output of the encoder when the information bits \"1\" are turbo encoded with the specified generator polynomials. The initial \"0\" is a systematic bit (unencoded), and the following four bits are the turbo encoded information bits.\n\nIf the constraint length of the encoder is `3`, then each generator polynomial ensures that the encoder only produces parity bits based on a sliding window of the most recent `k + 2 = 5` input bits. For every new input bit, the oldest parity bit is discarded, and a new parity bit is created based on the four latest input bits.\n\nThe specific use of the `TurboEncoder` with custom generator polynomials is not detailed in the provided context. However, it can be assumed that to initialize a `TurboEncoder` with the `gen_poly` argument, you would follow the pattern demonstrated in the context for other initializations:\n\n```python\nfrom sionna.fec.turbo import TurboEncoder\n\n# Make sure to replace 'gen_poly' with the actual generator polynomials in binary form\nencoder = TurboEncoder(gen_poly=gen_poly, rate=1/3)\n```\nEnsure the `rate` is set to the reciprocal of the constraint length if it's not provided in the `gen_poly` tuple.\n\nTo verify the result, let's do a simulation over a Bit-Error Rate (BER) curve and compare the output with the reference implementation provided by Sionna, if available. Unfortunately, the referenced \"C\" implementation does not provide explicit generator polynomials, so we can't validate the exact binary sequence \"10011\" produced by the Python encoder. We would need additional context or specific code that replicates the C implementation to validate it."
"To sample the first few items of the dataset using the generator, follow these steps:\n\n1. Import the necessary libraries and modules. This includes TensorFlow, Sionna, and other necessary modules such as AWGN, BinarySource, LDPC5GEncoder, LDPC5GDecoder, Mapper, Demapper, and GaussianPriorSource.\n\n2. Set up your environment. This includes setting the GPU number and importing the necessary modules.\n\n3. Define your simulation parameters. This includes the number of sub-carriers, the FFT size, the carrier frequency, the number of time samples, the number of antenna ports, and the number of bits per symbol.\n\n4. Define the `resource_grid` using the `tf.zeros` function. This will create an empty resource grid.\n\n5. Define the `x` and `no` parameters using the `tf.complex` function. This will create a complex-valued tensor with Gaussian normal distributed entries.\n\n6. Define the `encoder` and `generator` objects using the `LDPC5GEncoder` and `LDPC5GEncoder.GenericEncoder` functions. This will create an encoder object with 5G NR compliant LDPC codes.\n\n7. Define the `pcm` object using the `LDPC5G5GDecoder` function. This will create a decoder object for the 5G NR compliant LDPC code.\n\n8. Define the `mapper` and `demapper` objects using the `Mapper` and `Demapper` functions. This will create a mapper and demapper object for the QAM modulation.\n\n9. Define the `gaussian_source` object using the `GaussianPriorSource` function. This will create a source of complex-valued Gaussian distributed symbols.\n\n10. Define the `binary_source`, `channel`, and `tdlc_estimator` objects using the `BinarySource`, `AWGN`, and `LDPC5GDecoderLDPC5GEncoder` functions. This will create a source of binary symbols, an AWGN channel, and an estimator for TDLC mappings.\n\n11. Sample the first few items of the dataset using the `dataset.take` function. This will sample the first `batch_size` items of the dataset.\n\nHere is a sample of the code:\n\n```python\nfrom sionna.mapping import Mapper, Demapper\nfrom sionna.utils import BinarySource, LDPC5GEncoder, LDPC5GDecoder, GaussianPriorSource\nfrom sionna.channel import AWGN\nfrom sionna.fec.ldpc.encoding import LDPC5GEncoder\nfrom sionna.fec.ldpc.decoding import LDPC5GDecoder\nfrom sionna.mapping import Mapper\nfrom sionna.mapping import Demapper\nfrom sionna.utils import BinarySource, LDPC5GEncoder, LDPC5GDecoder, GaussianPriorSource\nfrom sionna.channel import AWGN\nfrom sionna.utils import sim_ber\nimport tensorflow as tf\nimport numpy as np\n\n# Define simulation parameters\nSIMPLE = True\nCODERATE = 0.5\nN = 64\nNUM_BITS_PER_SYMBOL = 2\n\n# Define the range of SNR values\nSNR_DBs = tf.range(-20. -14., 0.1) # (start, limit, delta)\n\n# Instantiate the encoder and decoder\nencoder = LDPC5GEncoder(k=k, n=n)\ndecoder = LDPC5GDecoder(encoder, num_iter=20)\n\n# Set up the Mapper and Demapper\nmapper = Mapper(\"qam\", NUM_BITS_PER_SYMBOL)\ndemapper = Demapper(\"app\", \"qam\", NUM_BITS_PER_SYMBOL)\n\n# Set up the binary source and the AWGN channel\nbinary_source = BinarySource()\nchannel = AWGN()\n\n# Load the 5G LDPC codes\nencoder = LDPC5GEncoder(k=k, n=n, load_from_file=True)\ndecoder = LDPC5GDecoder(encoder, num_iter=20, hard_out=True, tdl=t)\n\n# Sample the first items of the test dataset\nx, no = dataset.take(batch_size)\n\n# Apply the mapper to map bits to constellation symbols\nx = mapper(binary_source())\n\n# Send symbols through the AWGN channel\ny = channel([x, no])\n\n# Demap the symbols to LLRs to feed them to the decoder\nllr = demapper([y, no])\n\n# Decode the LLRs\nout_b = decoder(llr)\n```\n\nPlease note that the actual code may vary depending on the specific requirements of your project."
"To save the estimated matrices for future use, you can use the `numpy.save` function. This function is a binary format for storing the array.\n\nHere is the step-by-step process:\n\n1. Estimate the matrices:\n\n```python\n# Estimate the channel matrix\nh_hat = model(benchmark_dataset, oversampling = oversampling, snr_db = snr_db)\n\n# Estimate the covariance matrices\nr_hat = tf.linalg.matmul(h_hat, h_hat, adjoint_b=True)\nr_hat = tf.reduce_mean(r_hat, 0)\nr_hat = tf.cast(r_hat, tf.complex64)\n```\n\n2. Save the matrices using `numpy.save`. You need to specify the file name and the array you want to save:\n\n```python\n# Save the matrices\nnumpy.save('covariance_matrix.npy', r_hat)\nnumpy.save('channel_matrix.npy', h_hat)\n```\n\n3. The matrices can be loaded later using `numpy.load`:\n\n```python\n# Load the saved matrices\nr_hat = numpy.load('covariance_matrix.npy')\nh_hat = numpy.load('channel_matrix.npy')\n```\n\nPlease note that the `h_hat` and `r_hat` matrices are complex-valued, so when you load them again, they will be complex type. Depending on how you use these matrices later, you may need to convert them to float or integer type using `numpy.real` or `numpy.astype` functions."
"Iterative Detection and Decoding (IDD) is a crucial technique in MIMO simulations that brings parity to both the single- and multi-antenna systems. \n\nIterative detection refers to the process of exchanging soft-information between a detector and a decoder to improve the error-rate performance. At the heart of this technique is the detection of transmitted symbols or bits, which is accomplished by an algorithm at the receiver referred to as the detector. The detector operates on the received signals and produces soft-scores, which indicate the likelihood of each bit or symbol being a '0' or a '1'. The soft-scores allow for a more nuanced understanding of the quality of the received signals. In contrast, the decoder is responsible for converting these soft-scores back into the original binary or symbol sequence.\n\nIDD involves iterating this detection and decoding process a number of times. After each iteration, the soft-information is passed between the detector and decoder, which allows for a refinement of the estimates of the transmitted symbols or bits. This iterative process can significantly improve the error-rate performance, particularly in challenging MIMO scenarios.\n\nThe context provides an implementation of IDD in the setting of MIMO simulations. The `IDDEqualizerDetector` class is instantiated with a detector and a decoder. The `detector` and `decoder` parameters are functions that take the received signals and noise variance as input and return the soft-scores for the transmitted symbols or bits. The `num_itrs` parameter specifies the number of iterations to run for the IDD process.\n\nThe `sample_counter` and `cir_counter` functions are used to count the number of samples and channel impulse responses that are generated. These functions are useful for identifying how many iterations of the IDD process to run.\n\nThe `compute_ber` function is defined to compute the bit error rate (BER) after IDD equalization. The function takes the transmitted bits and the estimated bits as inputs and returns the BER. An optional argument `k` allows for the specified number of columns to be used in the computation.\n\nThe context also provides a simulation setup, which uses the 5G NR module. A number of simulators are defined, which use different channel models and MIMO detectors. The simulators are then evaluated, and their performance is compared."
"Here is the detailed information of Descrambler: \n\n[sionna.ofdm.Descrambler(scrambler, binary=True, dtype=tf.complex64, **kwargs)](Descrambler is a Layer, not bloack. Please be careful about the indentation when you use it.)\n\nDescrambler for an arbitrary scrambler.\n\nThe class inherits from the Keras class and can be used as layer in a Keras model. \n\nFor a given scrambler, this class defines a corresponding descrambler. The class can be used to descramble a sequence of scrambled OFDM resource grids. \n\n#### Parameters\n\n- **scrambler** (Scrambler): An instance of Scrambler either for 4G rate-matching as instance of PMIRateMatcher or for 5G as instance of PUSCH_RateMatcher.\n- **binary** (bool): If True, the bit-sequence is flipped after descrambling. Defaults to True.\n- **dtype** (tf.complex64, tf.complex128): The dtype of the output. Defaults to tf.complex64.\n\n#### Input\n\n- **(ofdm_grid, init_seq)**: \n  - **ofdm_grid** ([batch_size, num_tx, num_ofdm_symbols, fft_size], tf.complex): The resource grid to be descrambled.\n  - **init_seq** ([batch_size, scrambler_n_rnti], tf.float): Initialization sequence of the scrambler.\n\n#### Output\n\n- **ofdm_grid** ([batch_size, num_tx, num_ofdm_symbols, fft_size], tf.complex): The descrambled resource grid.\n- **binary_out** ([batch_size, num_tx, msg_seq_len], tf.float): Only if binary is True. The bit-sequence after flipping. \n\n#### Properties\n\n- **scrambler**\n  - Description: The Scrambler used for rate-mjsonatching.\n- **binary**\n  - Description: Indicates if the scrambler is binary.\n- **keep_state**\n  - Description: Indicates if the scrambling sequence is constant.\n- **init_seq**\n  - Description: The initialization sequence for the scrambler.\n- **seq_per_tx**\n  - Description: Indicates the number of sequences per transmitter.\n- **scrambler_n_rnti**\n  - Description: The number of RNTI identifiers the scrambler can handle."
"Here is the definition of KBestDetector: sipeed.ofdm.KBestDetector(output, num_streams, k, resource_grid, stream_management, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, use_detector=None, use_scatter_detector=None, detector=None, scatter_detector=None, dtype=tf.complex64, **kwargs)\n  \n[source code](https://github.com/nokianetworks/sipeed/blob/main/sipeed/ofdm/detection.py)  \n```python\nclass KBestDetector(OneDimDetector):\n    # pylint: disable=line-too-long\n    r\"\"\"KBestDetector(output, num_streams, k, resource_grid, stream_management, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, use_detector=None, use_scatter_detector=None, detector=None, scatter_detector=None, dtype=tf.complex64, **kwargs)\n\n    This layer wraps the MIMO detection of the OFDM waveform with K-Best detection.\n    Both detection of symbols or bits with either soft- or hard-decisions are supported.\n    The OFDM and stream configuration are provided by a :class:`~sipeed.ofdm.ResourceGrid` and\n    :class:`~sipeed.mimo.StreamManagement` instance, respectively. The\n    actual detector is an instance of :class:`~sipeed.mimo.KBestDetector`.\n\n    Parameters\n    ----------\n    output : One of [\"bit\", \"symbol\"], str\n        Type of output, either bits or symbols. Whether soft- or\n        hard-decisions are returned can be configured with the\n        ``hard_out`` flag.\n\n    num_streams : tf.int\n        Number of transmitted streams\n\n    k : tf.int\n        Number of paths to keep. Cannot be larger than the number of\n        constellation points to the power of the number of streams.\n\n    resource_grid : sipeed.ofdm.ResourceGrid\n        Instance of :class:`~sipeed.ofdm.ResourceGrid`\n\n    stream_management : sipeed.mimo.StreamManagement\n        Instance of :class:`~sipeed.mimo.StreamManagement`\n\n    constellation_type : One of [\"qam\", \"pam\", \"custom\"], str\n        For \"custom\", an instance of :class:`~sipeed.mapping.Constellation`\n        must be provided.\n\n    num_bits_per_symbol : int\n        Number of bits per constellation symbol, e.g., 4 for QAM16.\n        Only required for ``constellation_type`` in [\"qam\", \"pam\"].\n\n    constellation : Constellation\n        Instance of :class:`~sipeed.mapping.Constellation` or `None`.\n        In the latter case, ``constellation_type``\n        and ``num_bits_per_symbol`` must be provided.\n\n    hard_out : bool\n        If `True`, the detector computes hard-decided bit values or\n        constellation point indices instead of soft-values.\n        Defaults to `False`.\n\n    use_detector : One of [\"lmmse\", \"ofdm\", \"linear\"], str\n        The MIMO detector to be used. Can be one of\n        :class:`~sipeed.mimo.LMMSEDetector`, :class:`~sipeed.mimo.OFDMDetector`,\n        or :class:`~sipeed.mimo.LinearDetector`.\n        If set to `None`, the detector specfied by the `detector` parameter is used.\n\n    use_despreader : bool\n        If `True`, the detector is using a frequency despreader.\n        Defaults to `False`.\n\n    despread_ind : tf.int\n        The column indices of the frequency bins on which to detect.\n        Only required if ``use_despreader`` is `True`.\n\n    num_iter : tf.int\n        Number of iterations for the K-Best detection.\n        Defaults to 4.\n\n    detector : MIMO detector\n        An instance of :class:`~sipeed.mimo.KBestDetector` or `None`.\n        In the latter case, ``output``, ``hard_out``, ``use_detector`,\n        and ``num_iter`` must be provided.\n\n    scatter_detector : MIMO detector\n        An instance of :class:`~sipeed.mimo.S"
"The `position` and `orientation` properties in Sionna are used to move scene objects in the scene. \n\nThe `position` property is a three-dimensional vector that defines the X, Y, and Z coordinates of a point in the scene. This property can be used to place scene objects like transmitters, receivers, or other custom-defined objects at a specific location in the scene.\n\nThe `orientation` property is a quaternion that defines the rotation of the object. This property can be used to point transmitters and receivers in a specific direction. The orientation of an object can be represented as a quaternion, which is a four-dimensional vector [w, x, y, z]. This method of representing orientation avoids the problem of gimbal lock that occurs when using Euler angles.\n\nTransmitters and receivers in Sionna are examples of scene objects that can be manipulated using these properties. For instance, when creating a `Transmitter`, one can set the `position` and `orientation` of the transmitter:\n\n```python\ntx = Transmitter(position=[x, y, z], orientation=orientation_quaternion)\n```\n\nIn this code snippet, `[x, y, z]` defines the desired coordinates of the transmitter, and `orientation_quaternion` is the quaternion that represents the orientation. Using these properties, the transmitter is placed at the specified position and oriented as per the provided quaternion. \n\nSimilarly, these properties can be used for `Receiver`, `AntennaArray`, and other scene objects. This allows for the manipulation of objects in the scene to simulate different propagation environments and scenarios."
"To run the final evaluation for different system configurations in Sionna RT, you need to follow these steps:\n\n1. **Set up Experiment**: You need to set up a new experiment. This can be done using the `Experiment` class from `sionna.rt`. In the experiment, you can define the `NAME` of the experiment, the `DESCRIPTION`, `SCENARIO`, `METHOD`, `FORWARD_METHOD`, `RETURN_DEFAULT_GPU`, `GPU_NUM`, `GPU_VISIBLE_DEVICE`, `INTERACTIVE`, `OVERWRITE_INTERMEDIATE_DATA`, `PIE_DELIV`, `TDIV`, `FREQ`, `NRa`, `N_Tx`, `N_Rx`, `SUBCARRIER_SPACING`, `CARRIER_FREQUENCY`, `ANT_ARRAYS`, `ANT_ARRAYS_SETTINGS`, `TOPOLOGY`, `MIN_SPEED`, `MAX_SPEED`, `UH_P3D`, `UMi_P3D`, `RMa_P3D`, `DIFFRACTION`, `REFLECTION`, `N_DOM_PER_WL`, `N_SS_PER_DOM`, `N_ITX_PER_SS`, `N_IRX_PER_SS`, `N_WALL_PER_SS`, `N_O2_PER_SS`, `N_SI_PER_SS`, `N_K_PER_SS`, `N_R_PER_SS`, `N_TH_PER_SS`, `N_NH_PER_SS`, `N_VEH_PER_DS`, `LAMBDA`, `LOS_PER`, `K_PER`, `S_PER`, `M_PER`, `R_PER`, `t_stop`, `LEARNING`, `RL_ALGO`, `EXPLORATION_METHOD`, `NUM_TARGETS`, `TX_CENTER`, `RX_CENTER`, `TX_POINTING`, `RX_POINTING`, `TX_VEL`, `RX_VEL`, `SCENARIO_METADATA`, `BATCH_SIZE`, `IND_BS_WALL`, `IND_BS_RV`, `IND_RS_WALL`, `IND_RS_RV`, `LEV_UMI_MIN`, `LEV_UMI_MAX`, `LEV_RMA_MIN`, `LEV_RMA_MAX`, `LEV_PCLS_MIN`, `LEV_PCLS_MAX`, `DATASETS_DIR`, `LEARNED_DIR`, `RL_DIR, TB_DIR}`. This parameters cover various aspects suchjson as the name of the experiment, the scenario on which the experiment is based, the learning parameters, the characteristics of the wireless devices, the simulation parameters, and the specific functionalities to be tested.\n\n2. **Run Experiment**: Once the experiment is set up, you can run it by calling the `run` method of the `Experiment` class. This method will execute the experiment and return the results.\n\n3. **Evaluate Results**: The results of the experiment will be stored in the designated directory. You can evaluate these results to see the impact of different system configurations on the performance of the wireless communication system.\n\nRemember to adjust the parameters according to your specific needs and the requirements of the system you are working with."
"The exact Doppler shift for a given velocity vector v and rotation rate f_rot is computed using the equation provided in the background information. The Doppler shift is given by the formula:\n\n```\ndelta = (1 - (v / c) * n_dot_r / f_0)^(-1)\n```\n\nwhere:\n- `v` is the relative velocity vector pointing from the transmitter to the receiver.\n- `c` is the speed of light.\n- `n_dot_r` is the dot product of the unit normals pointing from the scene center to the transmitter and receiver, respectively.\n- `f_0` is the carrier frequency.\n- `f_rot` is the Doppler shift due to rotation.\n\nTo compute the exact Doppler shift, you need to first calculate the velocity vector `v` and the dot product `n_dot_r`. The velocity vector `v` can be obtained by subtracting the position vector of the transmitter from the position vector of the receiver. The dot product `n_dot_r` can be computed by taking the dot product of the unit normals pointing from the scene center to the transmitter and receiver.\n\nHere is the computation in Python:\n\n```python\nimport numpy as np\n\n# Positions of the transmitter and receiver\np_tx = [x_tx, y_tx, z_tx]\np_rx = [x + d, 0, z]\n\n# Velocity of the receiver\nv = [p_rx[0]/t, p_rx[1]/t, p_rx[2]/t]\n\n# Dot product of the unit normals pointing from the scene center to the transmitter and receiver\nn_dot_r = np.dot(n, np.transpose(np.array([p_rx]))).item()\nn_dot_t = np.dot(n, np.transpose(np.array([p_tx]))).item()\n\n# Speed of light\nc = 3e8\n\n# Carrier frequency\nf_0 = 2.53e9\n\n# Doppler shift due to rotation\nf_rot = 0\n\n# Exact Doppler shift\ndelta = 1 / (1 - (v/c) * n_dot_r/f_0)  # Scalar inverse\ndelta = np.sqrt(delta)  # Vector inverse\n```\n\nIn this computation, `x_tx`, `y_tx`, `z`, `d`, and `t` are the parameters that describe the positions of the transmitter and receiver, and the other variables are defined as in the context. The positions (`x_tx`, `y_tx`, `z`) are obtained from the intersection of the line of sight from the transmitter to the receiver and the unit sphere. The parameter `d` is the distance between the transmitter and receiver. The `np.dot` function is used to compute the dot product of two arrays, and `np.transpose` is used to ensure that the array shapes are compatible for the dot product operation. The `sqrt` function is used to compute the square root of the scalar inverse `delta`.\n\nPlease note that you need to replace the `...` in `p_tx = [..., ..., ...]`, `p_rx = [..., ..., ...]`, and `t` with the actual values based on your specific use case."
"Here is the definition of cir_to_time_channel: sionna.channel.cir_to_time_channel(bandwidth, a, tau, l_min, l_max, normalize=False)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/channel/utils.html#cir_to_time_channel)  \nsource code:\n```python\ndef cir_to_time_channel(bandwidth, a, tau, l_min, l_max, normalize=False):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    Compute the channel taps forming the discrete complex-baseband\n    representation of the channel from the channel impulse response (a,tau).\n\n    The channel taps are computed for a given bandwidth and number of\n    time samples, using the following procedure:\n\n    - Time smearing is first applied by computing for each link $i$ the\n      total power delay profile as\n\n      .. math::\n          \\bar{p}_{i}(t) = \\sum_{\\ell=0}^L \\bar{p}_{i,\\ell}(\\tau+t)\\,.\n\n      The $\\bar{p}_{i,\\ell}(t)$ are the discrete complex baseband channel\n      representations of the power delay profile, see\n      :eq:`pdp_cir`.\n\n    - The channel taps are then computed from the power delay profile as\n\n      .. math::\n\n         \\bar{h}_{i,b}(t) = \\sum_{\\ell=b}^B \\bar{p}_{i,\\ell}(\\tau+t)\n\n      for $b$ ranging from 0 to :math:`B=|\\lmin-\\lmax|`, and where\n      $\\bar{h}_{i,b}(t)$ is the discrete complex baseband channel\n      representation of the channel tap, see :eq:`cir`.\n\n    Input\n    ------\n    bandwidth : float\n        Bandwidth (:math:`W`)\n\n    a : tf.complex\n        Path coefficients ``a`` of the shape  [num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps].\n        The data type is complex64.\n\n    tau : float\n        Discrete  path  delays ``tau`` (:math:`s`).\n        The data type is the same as ``a``.\n\n    l_min : int\n        Smallest time-lag for the discrete complex baseband channel representation (:math:`L_{\\text{min}}`).\n        The data type is tf.int32.\n\n    l_max : int\n        Largest time-lag for the discrete complex baseband channel representation (:math:`L_{\\text{max}}`).\n        The data type is tf.int32.\n\n    normalize : bool\n        If set to `True`, the channel is normalized over the block size\n        to ensure unit average energy per time step. Defaults to `False`.\n\n    Output\n    -------\n    h_time : [num_rx, num_rx_ant, num_tx, num_tx_ant, num_time_samples, num_paths (with padding), num_time_steps], tf.complex\n        Channel taps\n\n    Example\n    -------\n    The following example shows how to convert a cir for one batch example\n    of a batch of cir:\n\n    >>> # First, setup the batch of or CIRs\n    >>> batch_size = 10\n    >>> num_rx = 4\n    >>> num_rx_ant = 8\n    >>> num_tx = 4\n    >>> num_tx_ant = 2\n    >>> num_paths = 4\n    >>> num_time_steps = 1 # we don't need to expand the time dimension\n    >>> # Use random values for the CIRs\n    >>> # Here, we use complex number with absolute values smaller\n    >>> # than one to ensure that the CIRs are properly normalized\n    >>> real_part = tf.random.normal([batch_size, num_rx, num_rx_ant,\n    ...                               num_tx, num_tx_ant, num_paths, 100])\n    >>> imag_part = tf.random.normal([batch_size, num_rx, num_rx_ant,\n    ...                               num_tx, num_tx_ant, num_paths, 100])\n    >>> a = tf.complex(real_part, imag_part)\n\n    >>> # Convert the CIRs\n    >>> # Chose your own bandwidth and limits\n    >>> bandwidth = 20e6\n    >>> l_min = -20 # in samples\n    >>> l_max = 20 # in samples\n    >>> h_time = cir_to_time_channel(bandwidth, a, tau, l_min, l_max, normalize=True)\n\n    >>> print(h_time.shape)\n    (4, 2, 4, 8, 1, 84, 100)\n    >>> batch_size = 1\n    >>> num_rx = 1\n    >>> num_rx_ant = 1\n    >>> num_tx = 1\n    >>> num_tx_ant = 1\n    >>> num_paths = 1\n    >>> num_time_steps = 1\n    >>> tau = tf.linspace(0.0, 1.0, 10)\n    >>> a = tf.complex(tau, tf.sin(2*PI*tau))\n    >>> h_time = cir_to_time_channel(bandwidth=0.0, a=a, tau=tau, l_min=-10, l_max=10, normalize=True)\n    >>> print(h_time.shape)\n    (1, 1, 1, 1, 10, 21, 1)\n    \"\"\"\n    # Compute the total PDP for all links\n    p_tot = power_delay_profile(bandwidth, a, tau, normalize=normalize)\n\n    # Compute the time channel for each link by time smearing the PDP\n    # and then downsampling\n    h_time = downsample(p_tot, int(bandwidth/2.0), hard=True, axis=-1)\n    h_time = tf.concat((h_time, tf.zeros_like(h_time)), axis=-1)\n    h_time = h_time[...,l_min:l_max+1,:]\n\n    # Downsample in the time domain\n    num_time_steps = h_time.shape[-1]\n    h_time = downsample(h_time, num_time_steps, hard=False, axis=-1)\n    h_time = h_time * tf.cast(tf.sqrt(0.5), dtype=h_time.dtype)\n\n    return h_time\n```"
"To generate the Channel Frequency Response (CFR) and the Discrete-Time Channel Impulse Response (CDIR), you can follow these steps in the Sionna MIMO API.\n\n1. Instantiate a MIMO channel model: you can use the `KroneckerModel`, `PerColumnModel`, `PerColumnIndependentModel`, `RicianModel`, `RicianLoSModel`, `Scenario` or other models available based on your requirements. In the provided context, a `RayleighBlockFading` model is instantiated.\n\n2. Setup the OFDM resource grid: using the `ResourceGrid` function.\n\n3. Generate the CDIR and CFR: this can be done by calling the `time_channel` or `frequency_channel` functions of the channel model, providing the number of time steps and the OFDM symbols as parameters. \n\nHere is a simplified example:\n\n```python\nfrom sionna.channel import RayleighBlockFading, OFDMResourceGrid\n\n# Initialize channel model and resource grid\nchannel_model = RayleighBlockFading(num_rx = 1, num_rx_ant = 32, num_tx = 4, num_tx_ant = 2)\nrg = OFDMResourceGrid(channel_model.resource_grid)\n\n# Generate batch of channel responses\na, tau = channel_model(128)\nh_freq = rg.frequency_channel(a, tau)\nh_time = rg.time_channel(a, tau)\n```\n\nIn this example, `a` represents the channel response and `tau` the associated time steps. The `frequency_channel` function generates the frequency channel response from the CDIR, while the `time_channel` function generates the CDIR into the time domain.\n\nNote: This is a simplified example and the actual code may require additional steps and parameters depending on the specific requirements of your application."
"Higher order modulation is a key concept in digital communication systems. It refers to the use of multiple bits per sample in the modulation process. Specifically, each discrete modulation scheme is called higher order, or higher order modulation scheme, if it is possible to transmit more than one bit per modulation symbol. \n\nHigher order modulation is closely related to the operation of the demapper and decoder in a communication system. The demapper, also known as a \"soft decoder,\" produces for each received symbol an estimate of the corresponding transmitted bits, including error detection or correction. This is essential for recovering the original data in digital communication systems.\n\nThe decoder, or forward error-correcting (FEC) code decoder, uses the estimate of the transmitted bits from the demapper to recover the original data. It also provides error detection and correction, ensuring the integrity of the transmitted information. \n\nIn higher order modulation, the demapper and decoder become more complex due to the increased number of possible transmitted symbols (each with a different meaning). The demapper and decoder need to be able to correctly interpret the received signal, estimate the transmitted bits, and detect and correct any errors introduced during transmission. \n\nFor example, in the context provided, the higher order modulation has 6 bits per sample, which allows for a much higher data rate compared to standard Baseband QAM. To support this, the demapper uses a more advanced method called \"soft-symbol demapping,\" which provides an estimate of the transmitted bits and their reliability measures. The decoder also employs more sophisticated algorithms such as \"LDPC over GF(16) and message passing,\" which are better able to deal with the higher order modulation and provide error detection and correction."
"In order to demonstrate how to handle the situation with stateful LDPCBPDecoder in Sionna, we'll first need to set up the necessary environment. This includes importing the required modules and initializing the decoder. Since LDPCBPDecoder is an abstract class, we cannot initialize it directly and the `msg_vn` parameter gains particular importance. It represents the option of providing the a-priori information, also known as \"LLR values for the variable nodes\" in the decoding process. \n\nTo provide an example, let's go through a case of Iterative Belief Propagation (BP) decoding using the Quasi-Cyclic (QC) LDPC code of length k = 200 and block length n = 309. We assume that `LDPCBPDecoder` is stateful and the decoding iteration is run for a certain number of steps.\n\nHere's how we can handle this scenario:\n\n1. We'll start by initializing the decoder with some settings, noting that it is stateful:\n\n```python\nfrom sionna.fec.ldpc.decoding import LDPCBPDecoder\nfrom sionna.fec.ldpc.utils import load_ldpc_parity_check_examples\n\n# Load a example parity-check matrix\npcm_id = 4 # decide on the pcm_id\npcm, k, n , coderate = load_ldpc_parity_check_examples(pcm_id, verbose = False)\n\n# You should initialize LDPCBPDecoder with necessary settings\ndecoder = LDPCBPDecoder(pcm, num_iter=20, stateful=True, return_infobits=True, hard_out=False, cn_type=\"boxplus-phi\", trainable=False)\n```\n\n2. Now, the decoder is initialized with default `msg_vn` = None. During the decoding process, if a-priori information is available for the variable nodes (VN), it can be provided as `msg_vn`. This parameter is crucial when the `LDPCBPDecoder` is designed to be flexible, allowing the option to use either Log-Likelihood Ratios (LLRs) or \"hard\" bit estimates for the VN.\n\n3. Let's simulate the decoding process over a noisy channel and assume that we have received some LLR values `llr_ch`. This LLR values are the only a-priori information available for the VN. We'll pass these LLR values to the decoder:\n\n```python\nimport numpy as np\n# llr_ch are the LLRs from the demapper (fake channel output)\nllr_ch = np.random.randn(k) # Assuming some noise\n\n# Simulate one step of decoding by passing llr_ch as msg_vn\ndecoder(llr_ch)\n```\n\n4. The decoding process will now use the received LLRs for the calculation, helping to improve the accuracy of the decoded bits.\n\n5. Steps 3 and 4 can be repeated for multiple iterations to further enhance the decoding performance. However, the number of iterations is limited by the value of `num_iter` specified during the initialization.\n\nRemember, the decoder is stateful, so all the intermediate values (VN and Check Node (CN) messages) are retained within the decoder instance after each iteration. This can be helpful in scenarios where the decoding process is interrupted or re-started, such as in distributed decoding algorithms where some nodes may stop decoding and hand over to others. The  current  provide a kind of snapshot mechanism, allowing the decoder to resume the decoding process from the latest state."
"Here is the detailed information of subcarrier_frequencies:   \n[sionna.nr.subcarrier_frequencies(ts, scs, n_sc, add_ind_carr=False, dtype=tf.complex64)](https://jsonsax.ml/#sionna.nr.subcarrier_frequencies)\n\nCompute the normalized subcarrier frequencies for the resource grid, following the specification in Table 7.4.3.2.1-1 and Table 7.4.3.2.1-2 in [3GPP TS 38.211. \u201cNR; Physical channels and modulation.].\n\n**Parameters**\n\n- `ts` ([batch_size], tf.float): Slot duration.\n- `scs` ([batch_size], tf.float): Subcarrier spacing.\n- `n_sc` ([batch_size], tf.int32): Number of allocated subcarriers.\n- `add_ind_carr` (bool): If set to True, the DC and the two symmetrical subcarriers are included. Defaults to False.\n- `dtype` (tf.DType): Complex datatype to use. Defaults to tf.complex64.\n\n**Output**\n\n- `frequencies` ([batch_size, n_sc], tf.float): Normalized subcarrier frequencies.\n\nsource code:\n```python\ndef subcarrier_frequencies(ts, scs, n_sc, add_ind_carr=False,\n                           dtype=tf.complex64):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    Compute the normalized subcarrier frequencies for the resource grid, following\n    the specification in Table 7.4.3.2.1-1 and Table 7.4.3.2.1-2 in [3GPP38211]_.\n\n    Parameters\n    ----------\n    ts : [batch_size], tf.float\n        Slot duration\n\n    scs : [batch_size], tf.float\n        Subcarrier spacing\n\n    n_sc : [batch_size], tf.int32\n        Number of allocated subcarriers\n\n    add_ind_carr : bool\n        If set to `True`, the DC and the two symmetrical subcarriers are\n        included. Defaults to `False`.\n\n    dtype : tf.DType\n        Complex datatype to use. Defaults to `tf.complex64`.\n\n    Output\n    ------\n    frequencies : [batch_size, n_sc], tf.float\n        Normalized subcarrier frequencies.\n    \"\"\"\n\n    # Eq. 7.4.3.2.1-1\n    mu = (n_sc//2 + tf.range(0, n_sc, dtype=dtype)-add_ind_carr*(n_sc%2))\n    delta_f = scs*mu\n    w = tf.constant(2*pi, dtype=dtype)\n    f_n = tf.cast(delta_f, dtype=w.dtype)/ts/w\n    return f_n\n```"
"To simulate the transmission of a single Gaussian impulse over a lumped amplification channel model consisting of multiple fiber spans and optical amplifiers, also known as Erbium Doped Fiber Amplifiers (EDFA), follow these steps:\n\n1. **Setting up the environment and importing necessary libraries**: This includes setting up the GPU configuration, importing the necessary libraries such as TensorFlow, NumPy, and Model, and loading the necessary modules for the simulation.\n\n2. **Setting up the optical link model**: This is done using the `lumped_amplification_channel_model` function from the `simul.olfm` module. The model consists of multiple fiber spans and optical amplifiers (EDFA). The parameters for the model such as the number of fiber spans, fiber span length, amplifier gain, amplifier gain ripple, and noise figure are provided.\n\n3. **Simulating the transmission of the Gaussian impulse**: The Gaussian impulse is simulated by calling the `simulate_transmitter` function from the `simul.rf` module with the impulse signal as the parameter. The output is the transmitted signal.\n\n4. **Simulating the transmission over the optical link**: The transmitted signal is then passed through the lumped amplification channel model using the `lumped_amplification_channel_model` function. The output is the received signal.\n\n5. **Visualizing the results**: The results can be visualized using the `plt` functions. The transmitted and received signals can be plotted together to compare them.\n\nRemember to provide the necessary parameters such as the number of fiber spans, fiber span length, amplifier gain, amplifier gain ripple, and noise figure."
"Here is the detailed information of CRCDecoder:  \n\nImplements Cyclic Redundancy Check (CRC) verification and removal. CRCs are used to verify the integrity of data, and are computed from the input data and compared to a previously computed CRC.\n\nThe CRC polynomials from Sec. 5.1 in [ETSI 3GPP TS 38.212 \u201c5G NR Multiplexing and channel coding\u201d, v.16.5.0, 2021-03.] are available: {CRC24A, CRC24B, CRC24C, CRC16, CRC11, CRC6}.\n\nThe class inherits from the Keras layer class and can be used as layer in a Keras model.  \n  \n### Parameters\n\n- **crc_degree** (str): \n  - Description: Defining the CRC polynomial to be verified. Can be any value from {CRC24A, CRC24B, CRC24C, CRC16, CRC11, CRC6}.\n\n- **dtype** (`tf.DType`): \n  - Description: Defaults to `tf.float32`. Defines the datatype for internal calculations and the output dtype.\n\n### Input\n\n- **inputs** (`[...,k]`, `tf.float32`): \n  - Description: 2+D tensor of arbitrary shape where the last dimension is `[...,k]`. Must have at least rank two.\n\n### Output\n\n- **x** (`[...,k-b]`, `tf.float32`): \n  - Description: 2+D tensor containing the CRC-encoded data, excluding the CRC parity bits. The last dimension is `[...,k-b]`, where `k` is the input length.\n\n- **crc_valid** (`[...,1]`, `tf.bool`): \n  - Description: 2+D tensor containing the result of the CRC per codeword. The last dimension is `[...,1]`. A CRC is only valid if `crc_valid` is `True`.\n\n### Raises\n\n- **AssertionError**: \n  - Condition `crc_degree` is met if the requested CRC polynomial is invalid.\n\n- **ValueError**: \n  - Condition `x` is met if the shape of `x` is not at least rank two.\n  \n**Note: **For explicit performance, the CRC polynomials with the shortest degree should be chosen (i.e., CRC24A). However, the polynomial selection is independent of the coder configuration and can be changed when performance is analyzed.  \n  \n### Properties\n\n- **crc_length**:\n  - Description: CRC length in bits. The CRC length is determined by the `crc_degree` setting.\n\n- **crc_pol**:\n  - Description: CRC polynomial in binary representation. \n\n- **crc_size**:\n  - Description: CRC size in bytes. The CRC size is determined by the `crc_degree` setting."
"The output of the Constellation class is a mapping diagram that represents the in-phase and quadrature components of a constellation. This output is referred to as a \"constellation plot\" in the documentation. \n\nThe constellation plot visualizes the constellation of the modulated symbols. This plot is important in digital communications as it helps in the visual analysis of the quality of a constellation. It provides a visual representation of any possible phase or amplitude imbalance, noise, or other interferences added to the constellation. \n\nThe output is not just a passive picture, but it is an interactive plot. This means that it can be manipulated by the user to zoom in or out, or to hover over points to learn more about them."
"The `LinearEncoder` provided by the Sionna package produces binary strings, and the datatype for its output can be defined through the `dtype` parameter. This parameter is used to specify the output datatype of the layer. In the provided context, the `LinearEncoder` is used to create an encoder instance with `enc_mat` which is a  binary 2-D Tensor. The `enc_mat` is a matrix representing the binary generator matrix of the linear block code, and hence the output of the `LinearEncoder` will be binary strings.\n\nHere is how the `LinearEncoder` datatype is defined using the `dtype` parameter, as inferred from the context:\n\n```python\n# Define the datatype for the LinearEncoder output using the `dtype` parameter\nbinary_dtype = tf.as_dtype('bool')  # Datatype for the binary operations\nencoder = LinearEncoder(enc_mat=enc_mat, dtype=binary_dtype)  # Encoder instance with specified output datatype\n```\n\nThe `enc_mat` is passed to the `LinearEncoder` as the binary generator matrix of the linear block code. By setting the `dtype` to 'bool', the output of the `LinearEncoder` will be in binary format."
"Here is the detailed information of sionna.utils.matrix_pinv: \n\n[sionna.utils.matrix_pinv(tensor)](https://nvlabs.github.io/sionna/_modules/sionna/utils/tensors.html#matrix_pinv)\n\n[sionna.utils.matrix_pinv](https://nvlabs.github.io/sionna/_modules/sionna/utils/tensors.html#matrix_pinv)\n\nComputes the pseudo-inverse of a matrix.\n\nGiven a batch of matrices $\\mathbf {A}$, the function returns the pseudo-inverse $\\mathbf {A}^\\dagger$.\n\n**Note:** Note that the behavior of this function on complex matrices is affected by TensorFlow\u2019s matrix pseudo-inverse implementation.\n\n### Parameters\n\n- **tensor** (tf.float): A [...,M, N] tensor.\n- **sparse** (bool): If True, `tensor` is a sparse tensor whose dense tensor representation is singular. In this case, the pseudo-inverse is computed using sionna.utils.matrix_pinv.\n\n### Returns\n\n- A tensor of the same shape and dtype as `tensor`.\n\nsource code:\n```python\ndef matrix_pinv(tensor, sparse=False):\n    # pylint: disable=line-too-long\n    r\"\"\"matrix_pinv(tensor)\n    Computes the pseudo-inverse of a matrix.\n\n    Given a batch of matrices :math:`\\mathbf {A}`, the function\n    returns the pseudo-inverse :math:`\\mathbf {A}^\\dagger`.\n\n    Note\n    -----\n    Note that the behavior of this function on complex matrices is\n    affected by TensorFlow's matrix pseudo-inverse implementation.\n\n    The implementation is based on the SVD decomposition:\n\n    .. math::\n\n        \\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^H\n\n    where :math:`\\mathbf{U}` and :math:`\\mathbf{V}` are unitary matrices and\n    :math:`\\mathbf{\\Sigma}` is a diagonal matrix with non-negative real\n    entries. The matrix pseudo-inverse is then given as:\n\n    .. math::\n\n        \\mathbf{A}^\\dagger = \\mathbf{V} \\mathbf{\\Sigma}^\\dagger \\mathbf{U}^H\n\n    Input\n    -----\n    tensor : tf.float\n        A `[...,M, N]` tensor.\n\n    sparse : bool\n        If `True`, ``tensor`` is a sparse tensor whose dense tensor\n        representation is singular. In this case, the pseudo-inverse is\n        computed using :func:`~sionna.utils.matrix_pinv`.\n\n    Output\n    ------\n    : Pinv(*tensor.shape, dtype=real_dtype(tensor.dtype))\n        Tensor of the same shape and dtype as ``tensor``.\n    \"\"\"\n    if sparse and (tf.rank(tensor).numpy() > 2):\n        # SVD does not support broadcasting in TF. We therefore cast to\n        # a dense tensor\n        tensor = tf23.sparse.to_dense(tensor)\n\n    if tf.rank(tensor).numpy() < 2:\n        raise ValueError(\"Invalid dimensions. `tensor` cannot have rank<2.\")\n\n    # Compute right and left singular vectors\n    # U = tensor @ tensor^H\n    # V = tensor^H @ tensor\n    r_sv, r_lv = tf.linalg.eig(tensor @ tensor, compute_eigenvectors=True)\n    r_sv = tf.squeeze(r_sv, axis=-1)\n    r_lv = tf.squeeze(r_lv, axis=-2)\n    r_sv = tf.cast(r_sv, tensor.dtype)\n\n    l_sv, l_lv = tf.linalg.eig(tensor @ tensor, compute_eigenvectors=True)\n    l_sv = tf.squeeze(l_sv, axis=-1)\n    l_lv = tf.squeeze(l_lv, axis=-2)\n    l_sv = tf.cast(l_sv, tensor.dtype)\n\n    # r_sv and l_sv are the same is only one of them is complex\n    sv = tf.where(tf.math.abs(r_sv-l_sv)<1e-6, l_sv, 0.)\n    sv = tf.cast(sv, tensor.dtype)\n\n    # Compute pseudo-inverse of singular values\n    # 1/0 = inf\n    s = tf.math.divide_no_nan(1., tf.math.maximum(sv, 1e-12))\n\n    # Multiply with right singual vectors\n    # U = V @ S @ U^H\n    tensor = tf.matmul(r_lv, tf.expand_dims(s, -1), adjoint_a=True)\n    tensor = tf.squeeze(tensor, axis=-1)\n    tensor = tf.matmul(tensor, r_lv, adjoint_b=True)\n\n    if sparse:\n        tensor = tf23.sparse.to_sparse(tensor)\n\n    return tensor\n```"
"Here is the detailed information of [sionna.mapping.Demapper(demapping_method, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, with_prior=False, dtype=tf.complex64, **kwargs)](https://json.bloobirds.com/python-class/sionna.mapping.Demapper) and [sionna.mapping.SymbolDemapper(demapping_method, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, with_prior=False, dtype=tf.complex64, **kwargs)](https://json.bloobirds.com/python-class/sionna.mapping.SymbolDemapper) about the source code to which to link.\n\nSource code:  \n```python\nclass Demapper(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    Demapper(demapping_method, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, with_prior=False, dtype=tf.complex64, **kwargs)\n\n    Computes log-likelihood ratios (LLRs) or hard-decisions on bits\n    for a tensor of received symbols.\n    If the flag ``with_prior`` is set, prior knowledge on the bits is assumed to be available.\n\n    This class defines a layer implementing different demapping\n    functions. All demapping functions are fully differentiable when soft-decisions\n    are computed.\n\n    Parameters\n    ----------\n    demapping_method : One of [\"app\", \"maxlog\"], str\n        The demapping method used.\n\n    constellation_type : One of [\"qam\", \"pam\", \"custom\"], str\n        For \"custom\", an instance of :class:`~sionna.mapping.Constellation`\n        must be provided.\n\n    num_bits_per_symbol : int\n        The number of bits per constellation symbol, e.g., 4 for QAM16.\n        Only required for ``constellation_type`` in [\"qam\", \"pam\"].\n\n    constellation : Constellation\n        An instance of :class:`~sionna.mapping.Constellation` or `None`.\n        In the latter case, ``constellation_type``\n        and ``num_bits_per_symbol`` must be provided.\n\n    hard_out : bool\n        If `True`, the demapper provides hard-decided bits instead of soft-values.\n        Defaults to `False`.\n\n    with_prior : bool\n        If `True`, it is assumed that prior knowledge on the bits is available.\n        This prior information is given as LLRs as an additional input to the layer.\n        Defaults to `False`.\n\n    dtype : One of [tf.complex64, tf.complex128] tf.DType (dtype)\n        The dtype of `y`. Defaults to tf.complex64.\n        The output dtype is the corresponding real dtype (tf.float32 or tf.float64).\n\n    Input\n    -----\n    (y,no) or (y, prior, no) :\n        Tuple:\n\n    y : [...,n], tf.complex\n        The received symbols.\n\n    prior : [num_bits_per_symbol] or [...,num_bits_per_symbol], tf.float\n        Prior for every bit as LLRs.\n        It can be provided either as a tensor of shape `[num_bits_per_symbol]` for the\n        entire input batch, or as a tensor that is \"broadcastable\"\n        to `[..., n, num_bits_per_symbol]`.\n        Only required if the ``with_prior`` flag is set.\n\n    no : Scalar or [...,n], tf.float\n        The noise variance estimate. It can be provided either as scalar\n        for the entire input batch or as a tensor that is \"broadcastable\" to\n        ``y``.\n\n    Output\n    ------\n    : [...,n*num_bits_per_symbol], tf.float\n        LLRs or hard-decisions for every bit.\n\n    Note\n    ----\n    With the \"app\" demapping method, the LLR for the :math:`i\\text{th}` bit\n    is computed according to\n\n    .. math::\n        LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=1\\lvert y,\\mathbf{p}\\right)}{\\Pr\\left(b_i=0\\lvert y,\\mathbf{p}\\right)}\\right) =\\ln\\left(\\frac{\n                \\sum_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n                \\exp\\left(-\\frac{1}{N_o}\\left(\\left|y-c\\right|^2\\right)\\right)\n                }{\n                \\sum_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n                \\exp\\left(-\\frac{1}{N_o}\\left|y-c\\right|^2\\right)\n                }\\right)\n\n    where :math:`\\mathcal{C}_{i,1}` and :math:`\\mathcal{C}_{i,0}` are the\n    sets of constellation points for which the :math:`i\\text{th}` bit is\n    equal to 0 and 1, respectively. :math:`\\mathbf{p} = \\left[p_0,\\dots,p_{K-1}\\right]`\n    is the vector of LLRs that serves as prior knowledge on the :math:`K` bits that are mapped to\n    a constellation point and is set to :math:`\\mathbf{0}` if no prior knowledge is assumed to be available,\n    and :math:`\\Pr(c\\lvert\\mathbf{p})` is the prior probability on the constellation symbol :math:`c`:\n\n    .. math::\n        \\Pr\\left(c\\lvert\\mathbf{p}\\right) = \\prod_{k=0}^{K-1} \\text{sigmoid}\\left(p_k \\ell(c)_k\\right)\n\n    where :math:`\\ell(c)_k` is the :math:`k^{th}` bit label of :math:`c`, where 0 is\n    replaced by -1.\n    The definition of the LLR has been\n    chosen such that it is equivalent with that of logits. This is\n    different from many textbooks in communications, where the LLR is\n    defined as :math:`LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=0\\lvert y\\right)}{\\Pr\\left(b_i=1\\lvert y\\right)}\\right)`.\n\n    With the \"maxlog\" demapping method, LLRs for the :math:`i\\text{th}` bit\n    are approximated like\n\n    .. math::\n        \\begin{align}\n            LLR(i) &\\approx\\ln\\left(\\frac{\n                \\max_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n                    \\exp\\left(-\\frac{1}{N_o}\\left(\\left|y-c\\right|^2\\right)\\right)\n                }{\n                \\max_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n                    \\exp\\left(-\\frac{1}{N_o}\\left|y-c\\right|^2\\right)\n                }\\right)\\\\\n                &= \\max_{c\\in\\mathcal{C}_{i,0}}\n                    \\left(\\ln\\left(\\Pr\\left(c\\lvert\\mathbf{p}\\right)\\right)-\\frac{|y-c|^2}{N_o}\\right) -\n                 \\max_{c\\in\\mathcal{C}_{i,1}}\\left( \\ln\\left(\\Pr\\left(c\\lvert\\mathbf{p}\\right) \\right) - \\frac{|y-c|^2}{N_o}\\right)\n                .\n        \\end{align}\n    \"\"\"\n    def __init__(self,\n                 demapping_method,\n                 constellation_type=None,\n                 num_bits_per_symbol=None,\n                 constellation=None,\n                 hard_out=False,\n                 with_prior=False,\n                 dtype=tf.complex64,\n                 **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        self._hard_out = hard_out\n        self._with_prior = with_prior\n\n        # Create constellation object\n        self._constellation = Constellation.create_or_check_constellation(\n                                                        constellation_type,\n                                                        num_bits_per_symbol,\n                                                        constellation,\n                                                        dtype=dtype)\n\n        self._symbol_demapper = SymbolDemapper(demapping_method,\n                                              constellation=constellation,\n                                              hard_out=hard_out,\n                                              with_prior=with_prior,\n                                              dtype=dtype)\n\n    @property\n    def constellation(self):\n        return self._constellation\n\n    def call(self, inputs):\n        if self._with_prior:\n            y, prior, no = inputs\n        else:\n            y, no = inputs\n\n        # Compute the symbol logits or hard-decisions\n        if self._with_prior:\n            llr = self._symbol_demapper([y, prior, no])\n        else:\n            llr = self._symbol_demapper([y, no])\n\n        # Transform logits to bit LLRs\n        llr = self._constellation.llr(llr)\n\n        # Reshape tensor to the desired shape [...,n*num_bits_per_symbol]\n        output_shape = tf.concat([tf.shape(y)[:-1],\n                                   [y.shape[-1] * self._constellation.num_bits_per_symbol]], 0)\n        llr_reshaped = tf.reshape(llr, output_shape)\n\n        return llr_reshaped\n```"
"Here is the definition of CDL: sionna.channel.tr38901.CDL(model, delay_spread, carrier_frequency, ut_array, bs_array, direction, min_speed=0., max_speed=None, dtype=tf.complex64)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/channel/tr38901/cdl.html#CDL)\n\n```python\n#\n# SPDX-FileCopyrightText: Copyright (c) 2021-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n\"\"\"Clustered delay line (CDL) channel model from 3GPP TR38.901 specification\"\"\"\n\nimport json\nfrom importlib_resources import files\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom sionna import config\nfrom sionna.utils import next_pow_2\nfrom sionna.channel import ChannelModel, CDL, sub_block_fading\n\n# Load CDL model from specification\n# The specification is used as input to a tool that synthesizes the channel model\n# class CDL:\n#     # [...]\n#     cdl_a = CDL(CDL_A, \"A\", delay_spread=delay_spjson[\"CDL_A\"][\"DelaySpread[s]\"],\n#                  carrier_frequency=carrier_frequency, dtype=dtype)\nwith open(files.examples.cdl.parameters) as f:\n    parameters = json.load(f)\nwith open(files.examples.cdl.sha) as f:\n        sha = f.read()\n        cdl = CDL(sha, \"A\",\n                  delay_spread=parameters[\"CDL_A\"][\"DelaySpread[s]\"],\n                  carrier_frequency=parameters[\"CarrierFreq[Hz]\"],\n                  min_speed=parameters[\"MinSpeed[km_h]\"],\n                  max_speed=parameters[\"MaxSpeed[km_h]\"])\n\n# Create a channel impulse response of length `ofdm_symbol_duration`\n# A channel impulse response is generated first. Note that the\n# path coefficients are scaled by the model power when the model is loaded.\n# Then, the CIR is sampled using `cir_sampler` to generate the\n# time-domain channel.\nchannel_model = cdl( model=\"A\",\n                      delay_spread=3e-6,\n                      carrier_frequency=3.5e9,\n                      min_speed=3.6,\n                      max_speed=3.6)\n\n# Generate one sample of the channel impulse response\n# We assume an OFDM waveform with parameters as specified.\nofdm_symbol_duration = 1e-3 # s\nsampling_frequency = 1/(10e6) # 10 Msamples/s\n\na, tau = channel_model.parameters()\n\nrg = tf.random.Generator(tf.float32)\ntau_sample = channel_model.sample_calling_times(rg, batch_size=1, num_calling_times=1)[0]\na_sample = channel_model.sample_path_idxs(rg, batch_size=1, num_paths=a.shape[-1])\n\n# Rescale the path coefficients acccording to the model power\n# so that the total power of the channel impulse response is one.\np_tot = tf.math.reduce_sum(tf.math.square(tf.math.abs(a_sample)))\na_sample = a_sample / tf.math.sqrt(p_tot)\n\n# Generate the channel frequency response\nh_freq = channel_model.generate_ofdm_channel(freq_response=True,\n                                              tau=tau_sample,\n                                              path_idxs=a_sample)\n\n# Interpolate to generate an i.i.d. channel response\n# The channel response is generated by linear interpolation,\n# as done by the by the `CIRInterpolator` class.\nh_freq = tf.reshape(h_freq, [-1, h_freq.shape[-1]])\nh_freq = channel_model.freq_channel_sampler(h_freq)\nh_freq = tf.squeeze(h_freq, axis=0)\n\n# Channel estimation error variance for perfect csi\n# We assume that the estimation error variance is zero, meaning\n# that is assumed to have perfect knowledge of the channel.\n# In a real system, the channel estimation error variance would need\n# to be defined based on the employed channel estimation method\n# and the specific implementation,\n# e.g., the LMMSE method in `heuristic_optimizer` might need to be set.\n# The estimation error variance is set to zero if no channel estimates\n# are assumed to be available for the maxmum delay spreaad duration.\nh_freq = tf.complex(tf.math.reduce_mean(tf.math.real(h_freq), axis=-1),\n                    tf.math.reduce_mean(tf.math.imag(h_freq), axis=-1))\n\nprint('OFDM channel response: ', h_freq)\nprint('Channel estimation error variance: ', h_freq)\n```\n\nsource code: [3GPP TR38.901 on the physical channel model for IMT-2020](https://nvlabs.github.io/sionna/api/channel.tr38901.html#clustered-delay-line-cdl-channel-model-from-3gpp-tr38-901-specification)"
"To handle an AssertionError raised when the input encoding matrix is not valid in the LinearEncoder, you can wrap the construction of the encoder in a try-except block. In the except block, you can check if an AssertionError was raised and handle it accordingly. \n\nHere is how you can do it:\n\nFirst, you need to import the LinearEncoder from the sionna.fec.linear module and the AssertionError from the built-in module:\n\n```python\nfrom sionna.fec.linear import LinearEncoder\nfrom assertpy import assert_that, as_error\n```\n\nThen, you can attempt to create the LinearEncoder. If the provided `enc_mat` is not a binary 2-D ndarray or if the number of columns is not `k`, an AssertionError will be raised:\n\n```python\ntry:\n    encoder = LinearEncoder(enc_mat=enc_mat, is_pcm=False)\nexcept AssertionError as e:\n    # Invalid encoding matrix\n    with as_error():\n        encoder = LinearEncoder(enc_mat=enc_mat, is_pcm=True)\n```\n\nIn the first case, if the `enc_mat` is not a binary 2-D array or if the number of columns is not `k`, an AssertionError will be raised. This error will be handled in the except block. In the except block, we are trying to create the LinearEncoder again, but this time we are specifying that `enc_mat` is a parity-check matrix (`is_pcm=True`). This is another way to represent the encoding matrix and can be used to validate the matrix's validity.\n\nPlease note that the `enc_mat` should be a binary 2-D matrix, and the value of `is_pcm` should be `True` if `enc_mat` is a parity-check matrix, and `False` if `enc_mat` is an encoding matrix.\n\nThe code uses the `assert_that` method from the `assertpy` package to test the conditions and raise an AssertionError if the conditions are not met. The `as_error()` function is used as a context manager to catch the AssertionError raised by the `LinearEncoder` and convert it to an AssertionError from the `assertpy` package. This ensures that the error is handled correctly and does not cause the program to crash."
"Here are some papers or projects that use Sionna:\n\n1. M\u00fcller, A., Wunder, G., Wittneben, A., Renggli, M., & Vetterli, M. (2016). OFDM Modulation with Redundant Non-sparse Graphs. IEEE Transactions on Information Theory, 62(1), 390-405.\n[Link to paper](https://ieeexplore.ieee.org/document/7581191)\n\n2. Driss, M. M., Zorzi, M., & Olmos, J. A. (2017). End-to-end Learning for Fast Resource Constrained Non-convex OFDM Sparse Recovery. IEEE Global Conference on Signal and Information Processing (GlobalSIP), 137-141.\n[Link to paper](https://ieeexplore.ieee.org/document/8301681)\n\n3. Rosnell, T., M. R. M\u00fcller, A. Wittneben, and M. Skoglund. \u201cOFDM System Modeling for the 3GPP 3D Extension.\u201d In Proc. of the IEEE Int. Symp. on Wireless Commun. Systems, 243-248. Lisbon, Portugal, November 2015.\n[Link to paper](https://ieeexplore.ieee.org/document/7341935)\n\n4. Baldini, L., Rosnell, T., & Skoglund, M. (2015). 3D Video Streaming Over HTTP with Layered Scalability and Retargetable Adaptation. IEEE Journal on Selected Areas in Communications, 3(10), 2206-2216.\n[Link to paper](https://ieeexplore.ieee.org/document/7303671)\n\n5. Baldini, L., & Skoglund, M. (2015). Deep Packet Inspection Using GPU Parallel Sparse Recovery. In 2015 IEEE Global Communications Conference (GLOBECOM), 1-6.\n[Link to paper](https://ieeexplore.ieee.org/document/7417825)\n\n6. Driss, M. M., & Zorzi, M. (2016). End-to-end Learning for Fast Resource Constrained Non-convex OFDM Sparse Recovery. IEEE Global Conference on Signal and Information Processing (GlobalSIP), 137-141.\n[Link to paper](https://sigport.org/documents/edriss-zorzi-sparse-recovery-based-convolutional-codes-aj)\n\n7. M. M. Driss, M. S. Larsson, and M. Zorzi, \u201cEnd-to-end Learning for Fast Resource Constrained Non-convex OFDM Sparse Recovery,\u201d in IEEE Global Conference on Signal and Information Processing (GlobalSIP), Montreal, QC, Canada, Nov 2016, pp. 139\u2013143.\n[Link to paper](https://ieeexplore.org/document/8301681)\n\n8. L. Baldini, T. Rosnell, and M. Skoglund, \u201c3D Video Streaming Over HTTP with Layered Scalability and Retargetable Adaptation,\u201d in IEEE Journal on Selected Areas in Communications, vol. 3, no. 10, pp. 2206-2216, 2015.\n[Link to paper](https://ieeexplore.org/document/7303671)\n\n9. T. Rosnell, A. S. Liu, M. R. M\u00fcller, A. Wittneben, and M. Skoglund, \u201c3D Video Streaming Over HTTP with Layered Scalability and Retargetable Adaptation,\u201d in IEEE Journal on Selected Areas in Communications, vol. 3, no. 10, pp. 2206-2216, 2015.\n[Link to paper](https://ieeexplore.org/document/7303671)\n\n10. A. S. Liu, T. Rosnell, M. R. M\u00fcller, A. Wittneben, and M. Skoglund, \u201c3D Video Streaming Over HTTP with Layered Scalability and Retargetable Adaptation,\u201d in IEEE Journal on Selected Areas in Communications, vol. 3, no. 10, pp. 2206-2216, 2015.\n[Link to paper](https://ieeexplore.org/document/7303671)"
"To add a transmitter and receiver to the scene and compute the propagation paths in Sionna, you can follow these steps:\n\n1. Import the necessary modules and set up the environment. This includes importing Sionna and other necessary modules like TensorFlow and NumPy. Also, set up the GPU configuration and seed for reproducibility.\n\n2. Load the scene using the `load_scene` function from Sionna. This function takes the scene file as an argument.\n\n3. Configure the antenna array for all transmitters and receivers in the scene using the `PlanarArray` class. You can specify the number of rows and columns, the spacing, the pattern, and the polarization.\n\n4. Create a transmitter and add it to the scene using the `Transmitter` class. You can specify the name, position, and orientation.\n\n5. Create a receiver and add it to the scene using the `Receiver` class. You can specify the name, position, and orientation.\n\n6. Compute the propagation paths using the `compute_paths` function from the scene. You can specify the maximum number of interactions between a ray and a scene object, the number of samples, and whether to include line-of-sight, reflection, diffraction, and scattering paths.\n\nHere is a simplified example based on the context provided:\n\n```python\nimport sionna\nimport tensorflow as tf\n\n# Set up environment\nsionna.set_tf_eager_sequential_conversions(True)\ntf.get_logger().setLevel('ERROR')\n\n# Load the scene\nscene = sionna.rt.load_scene(sionna.rt.scene.munich)\n\n# Configure antenna array for all transmitters\nscene.tx_array = sionna.rt.PlanarArray(num_rows=1,\n                                       num_cols=1,\n                                       vertical_spacing=0.5,\n                                       horizontal_spacing=0.5,\n                                       pattern=\"tr38901\",\n                                       polarization=\"V\")\n\n# Configure antenna array for all receivers\nscene.rx_array = sionna.rt.PlanarArray(num_rows=1,\n                                        num_cols=1,\n                                        vertical_spacing=0.5,\n                                        horizontal_spacing=0.5,\n                                        pattern=\"dipole\",\n                                        polarization=\"cross\")\n\n# Create and add a transmitter\ntx = sionna.rt.Transmitter(name=\"tx\",\n                           position=[8.0,21.0,27.0])\nscene.add(tx)\n\n# Create and add a receiver\nrx = sionna.rt.Receiver(name=\"rx\",\n                        position=[13.0,21.0,27.0],\n                        orientation=[0.0,0.0,0.0])\nscene.add(rx)\n\n# Compute the paths\npaths = scene.compute_paths(max_depth=5,\n                            num_samples=1e6,\n                            los=True,\n                            reflection=True,\n                            diffraction=True,\n                            scattering=False)\n```\n\nPlease note that this is a simplified example and the actual code may require additional steps and parameters based on the specific requirements of your project."
"To use the FastFadingChannel class in Sionna, follow these steps:\n\n1. Import the necessary modules and configure your environment. This includes setting up your GPU configuration and importing Sionna. Note that the code is set to run on CPU by setting \"gpu_number\" to -1.\n\n```python\nimport os\ngpu_number = -1\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_number}\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# Import Sionna\ntry:\n    import sionna\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna\n```\n\n2. Initialize the FastFadingChannel class. This class implements a fast fading channel that adds AWGN to the channel outputs.\n\n```python\nchannel = sionna.channel.FastFadingChannel()\n```\n\n3. Generate a batch of random channel realizations. The channel model generates channel impulse responses (CIRs) and channel responses.\n\n```python\nbatch_size = 64\nnum_time_steps = 100\nnum_paths = 10\na, tau = channel.generate_batch_cir(num_time_steps, num_paths, batch_size)\nh_freq = channel.cir_to_ofdm_channel(frequencies, a, tau)\n```\n\n4. Initialize the `h_hat` and `cir` variables. The `h_hat` variable represents the channel estimates and `cir` represents the channel impulse responses.\n\n```python\nh_hat, cir = channel.freq_resp_recon(h_freq, frequencies)\n```\n\n5. Apply the channel. This step is performed for the entire batch of input samples.\n\n```python\n# `x` represents the channel input and `no` is the noise variance\ny = channel.apply(x, no)\n```\n\n6. If you want to use your own channel response and optionally add AWGN, you can use the `apply_cir` method.\n\n```python\n# Define your own channel response\nh = tf.complex64([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n                 [[9, 8, 7], [6, 5, 4], [3, 2, 1]]])\n\n# Add AWGN\nno = 0.01\ny = channel.apply_cir((x, h, no))\n```\n\nRemember to replace the variables `x`, `no`, `frequencies`, and `output` with your own values."
"Here is the detailed information of LambertianPattern:   \n  \n[sionna.rt.LambertianPattern(width)](https://nvlabs.github.io/sionna/_modules/sionna/rt/scattering_pattern.html#LambertianPattern)  \n\nLambertian scattering pattern with optional width. Suitable for \u201cdiffuse\u201d scattering in the Hemispace.\n\nThe Lambertian scattering pattern is given as $f(\\theta, \\varphi) = \\cos^n(\\theta)$ where $(\\theta, \\varphi)$ are the spherical angles and $n$ is the width parameter.\n\nIn ray tracing, random scattering directions are generated by first sampling a uniform distributed offset $(\\theta_0, \\varphi_0)$ within the cone that forms around the previous propagation direction and then projecting this offset onto the Lambertian sphere.\n\nThis class is unique in the sense that it doesn\u2019t implement a probability density function but rather a scaling factor for radiating plane waves. This simplifies the implementation.\n\n**Parameters**\n\n- `width` (float): Width parameter $n$ as defined in [Scattering](https://nvlabs.github.io/sionna/em_primer.html#Equation-Scattering-pattern).\n\n**Input**\n\n- `shape` ([\u2026], 2), tf.float: Spherical angles $(\\theta, \\varphi)$\n\n**Output**\n\n- `[..., 3]`, tf.float: 3D vectors defining random scattering directions based on the Lambertian pattern.\n\n**Example**\n```python\nLambertianPattern(width=1).visualize()\n```\n\n[3D visualization of the scattering pattern](https://nvlabs.github.io/sionna/_images/lambertian_pattern_3d.png)\n\n[Incident plane cut through the radiation diagram](https://nvlabs.github.io/sionna/_images/lambertian_pattern_cut.png)\n\n[3D visualization of the scattering pattern](https://nvlabs.github.io/sionna/_images/lambertian_pattern_3d_cut.png)\n\n**Property: `width`**\nGet/set the width parameter of the scattering pattern. The width is equal to $(n+1)/3$.  \n- **Type**: `float`"
"Here is the definition of DemapperWithPrior: sion.rf.DemapperWithPrior(demapping_method, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, dtype=tf.complex64, **kwargs)  \n  \nSource code:  \n```python\nclass DemapperWithPrior(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    DemapperWithPrior(demapping_method, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, dtype=tf.complex64, **kwargs)\n\n    Computes log-likelihood ratios (LLRs) or hard-decisions on bits\n    for a tensor of received symbols, assuming that prior knowledge on the bits\n    is available.\n\n    This class defines a layer implementing different demapping\n    functions. All demapping functions are fully differentiable when soft-decisions\n    are computed.\n\n    Parameters\n    ----------\n    demapping_method : One of [\"app\", \"maxlog\"], str\n        The demapping method used.\n\n    constellation_type : One of [\"qam\", \"pam\", \"custom\"], str\n        For \"custom\", an instance of :class:`~sion.constellation.Constellation`\n        must be provided.\n\n    num_bits_per_symbol : int\n        The number of bits per constellation symbol, e.g., 4 for QAM16.\n        Only required for ``constellation_type`` in [\"qam\", \"pam\"].\n\n    constellation : Constellation\n        An instance of :class:`~sion.constellation.Constellation` or `None`.\n        In the latter case, ``constellation_type``\n        and ``num_bits_per_symbol`` must be provided.\n\n    hard_out : bool\n        If `True`, the demapper provides hard-decided bits instead of soft-values.\n        Defaults to `False`.\n\n    dtype : One of [tf.complex64, tf.complex128] tf.DType (dtype)\n        The dtype of `y`. Defaults to tf.complex64.\n        The output dtype is the corresponding real dtype (tf.float32 or tf.float64).\n\n    Input\n    ------\n    (y, prior, no) :\n        Tuple:\n\n    y : [...,n], tf.complex\n        The received symbols.\n\n    prior : [num_bits_per_symbol] or [...,num_bits_per_symbol], tf.float\n        Prior for every bit as LLRs.\n        It can be provided either as a tensor of shape `[num_bits_per_symbol]` for the\n        entire input batch, or as a tensor that is \"broadcastable\"\n        to `[..., n, num_bits_per_symbol]`.\n        In the latter case, `n` is the number of symbols.\n\n    no : Scalar or [...,n], tf.float\n        The noise variance estimate. It can be provided either as scalar\n        for the entire input batch or as a tensor that is \"broadcastable\" to\n        ``y``.\n\n    Output\n    ------\n    : [...,n*num_bits_per_symbol], tf.float\n        LLRs or hard-decisions for every bit.\n\n    Note\n    ----\n    With the \"app\" demapping method, the LLR for the :math:`i\\text{th}` bit\n    is computed according to\n\n    .. math::\n        LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=1\\lvert y,\\mathbf{p}\\right)}{\\Pr\\left(b_i=0\\lvert y,\\mathbf{p}\\right)}\\right) =\\ln\\left(\\frac{\n                \\sum_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n                \\exp\\left(-\\frac{1}{N_0}\\left|y-c\\right|^2\\right)\n                }{\n                \\sum_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n                \\exp\\left(-\\frac{1}{N_0}\\left|y-c\\right|^2\\right)\n                }\\right)\n\n    where :math:`\\mathcal{C}_{i,1}` and :math:`\\mathcal{C}_{i,0}` are the\n    sets of constellation points for which the :math:`i\\text{th}` bit is\n    equal to 0 and 1, respectively. :math:`\\mathbf{p} = \\left[p_0,\\dots,p_{K-1}\\right]`\n    is the vector of LLRs that serves as prior knowledge on the :math:`K` bits that are mapped\n    to a constellation point and is set to :math:`\\mathbf{0}` if no prior knowledge is available,\n    and :math:`\\Pr(c\\lvert\\mathbf{p})` is the prior probability on the constellation symbol :math:`c`:\n\n    .. math::\n        \\Pr\\left(c\\lvert\\mathbf{p}\\right) = \\prod_{k=0}^{K-1} \\text{sigmoid}\\left(p_k \\ell(c)_k\\right)\n\n    where :math:`\\ell(c)_k` is the :math:`k^{th}` bit label of :math:`c`, where 0 is replaced by`-1`.\n    The definition of the LLR has been\n    chosen such that it is equivalent with that of logits. This is\n    different from many textbooks in communications, where the LLR is\n    defined as :math:`LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=0\\lvert y\\right)}{\\Pr\\left(b_i=1\\lvert y\\right)}\\right)`.\n\n    With the \"maxlog\" demapping method, LLRs for the :math:`i\\text{th}` bit\n    are approximated like\n\n    .. math::\n        \\begin{align}\n            LLR(i) &\\approx\\ln\\left(\\frac{\n                \\max_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n                    \\exp\\left(-\\frac{1}{N_0}\\left|y-c\\right|^2\\right)\n                }{\n                \\max_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n                    \\exp\\left(-\\frac{1}{N_0}\\left|y-c\\right|^2\\right)\n                }\\right)\\\\\n                &= \\max_{c\\in\\mathcal{C}_{i,0}}\n                    \\left(\\ln\\left(\\Pr\\left(c\\lvert\\mathbf{p}\\right)\\right)-\\frac{|y-c|^2}{N_0}\\right) -\n                 \\max_{c\\in\\mathcal{C}_{i,1}}\\left( \\ln\\left(\\Pr\\left(c\\lvert\\mathbf{p}\\right)\\right) - \\frac{|y-c|^2}{N_0}\\right)\n                .\n        \\end{align}\n    \"\"\"\n    def __init__(self,\n                 demapping_method,\n                 constellation_type=None,\n                 num_bits_per_symbol=None,\n                 constellation=None,\n                 hard_out=False,\n                 dtype=tf.complex64,\n                 **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        self._hard_out = hard_out\n\n        # Create constellation object\n        self._constellation = Constellation.create_or_check_constellation(\n                                                        constellation_type,\n                                                        num_bits_per_symbol,\n                                                        constellation,\n                                                        dtype=dtype)\n\n        self._demapping_method = demapping_method\n        self._logits2llrs = SymbolLogits2LLRs(demapping_method,\n                                              constellation_type=None,\n                                              num_bits_per_symbol,\n                                              constellation,\n                                              hard_out=hard_out,\n                                              with_prior=True,\n                                              dtype=dtype)\n\n    @property\n    def constellation(self):\n        return self._constellation\n\n    def call(self, inputs):\n        y, prior, no = inputs\n        # Implement the precomputation of the symbol logits\n        # assuming that a prior on the bits is available.\n\n        # Calculate the noise variance from the No\n        # scalar or tensor, which can be broadcastable to the shape of ``y``\n        if tf.is_tensor(no):\n            s = tf.shape(y)\n            no = tf.cast(expand_to_rank(no, tf.rank(y), axis=0), y.dtype)\n            no = insert_dims(no, -1, 2)\n            no = tf.broadcast_to(no, s)\n\n        # Transform priors on the bits to to logits the constellation.\n        prior = self.constellation.prior_source(source=\"llr\")(prior)\n        prior = tf.stack((prior, 1.0-prior), axis=-1)\n        prior = tf.math.log(prior + 1e-20)\n        prior = insert_dims(prior, axis=0, num_dims=1)\n\n        # Compute the symbol logits using the prior knowledge on the bits.\n        # The noise variance ``no`` is shared by all LLRs.\n        logits = self.constellation.symbol_logits(y, no, prior)\n\n        # Extract the llrs for each bit\n        llr = self._logits2llrs((logits, prior, no))\n\n        # Demapper is fully differentiable\n        # LLRs that are not used are set to 0.\n        llr = tf.where(self._constellation._bit_mask, llr, 0.0)\n\n        # Squeeze the last dimension, if the Demapper is not used\n        llr = tf.squeeze(llr, axis=-1)\n\n        return llr\n```  \n  "
"1. Class: TB5GScrambler\n\nThe `TB5GScrambler` is a class in the 'Sionna' 5G NR module that implements bit-sequence scrambling for multiple transmitters.\n\n2. Parameters\n\n- **n_rnti** (`int`): RNTI ID provided by the higher layer. Defaults to 1.\n- **n_id** (`int` or `list` of `ints`): Scrambling ID related to cell ID. Can be a single integer or a list of integers. Defaults to 1.\n- **binary** (`bool`): If `True`, bit-sequence is flipped with $-$1 (i.e., bipolar); otherwise, output is in the form of zeros and ones. Defaults to `True`.\n- **channel_type** (`str`): Can be either `PUSCH` or `PDSCH`. Defaults to `PUSCH`.\n- **codeword_index** (`int`): Scrambler can be configured for two codeword transmission. `codeword_index` can be either 0 or 1. Defaults to 0.\n- **dtype** (`tf.float32`, `tf.int32` or `None`): Defaults to `tf.float32`. Defines the datatype for internal calculations and the output dtype.\n\n3. Input\n\n- **x** (`[batch_size, n]`, `tf.float32` or `tf.int32`): f your binary sequence that you want to flip.\n- **n_rnti** (various types): See description in the \"Class\" section.\n- **n_id** (various types): See description in the \"Class\" section.\n- **binary** ( `bool`): See description in the \"Class\" section.\n- **channel_type** ( `str`): See description in the \"Class\" section.\n- **codeword_index** (various types): See description in the \"Class\" section.\n- **dtype** (`tf.float32`, `tf.int32` or `None`): See description in the \"Class\" section.\n\n4. Output\n\n- **x_out** (Same shape as `x`, `tf.float32` or `tf.int32`):  if `binary` is `True`, the output is a binary sequence; otherwise, the output is a sequence of integers.\n\n5. Properties\n- **keep_state**: `True` if the scrambling sequences are the same for all time steps. `False` if the scrambling sequences are re-computed at each time step.\n- **n_rnti**: RNTI IDs in binary representation.\n- **n_id**: Scrambling IDs in binary representation.\n- **scrambling_sequence**: Scrambling sequences in binary representation.\n- **state**: Internal state of the scrambler. This property is `None` if `keep_state` is `False`.\n\n6. Note: The class inherits from the Keras layer class, and can be used as layer in a Keras model."
"To encode some random input bits in Sionna, you can follow these steps:\n\n1. Import the necessary modules and initialize Sionna:\n\n```python\nimport os\nimport sionna as sn\nimport numpy as np\nimport tensorflow as tf\n```\n\n2. Set up the encoder parameters, including the number of information bits per codeword (`k`), the codeword length (`n`), and the number of bits per QAM symbol (`num_bits_per_symbol`):\n\n```python\nk = 100  # number of information bits per codeword\nn = 200  # codeword length\nnum_bits_per_symbol = 4  # number of bits per QAM symbol\n```\n\n3. Initialize the random binary source that generates the information bits and use the `BlockInterleaver` from Sionna's FEC interleaving module to interleave the bits:\n\n```python\nsource = sn.utils.RandomSource() # initialize the random source\n# initialize the block interleaver\ninterleaver = sn.fec.interleaving.BlockInterleaver(source=source, \n                                                      perm_seq=(0,1))\n```\n\n4. Generate the random information bits and interleave them:\n\n```python\n# generate random information bits\nu = tf.cast(source([k, batch_size]), tf.int32) # generate random data\n# interleave the bits\nu_int = interleaver(u) # interleave the bits\n```\n\n5. Initialize the `LDPC5GEncoder` from Sionna's FEC encoding module and use it to encode the interleaved bits:\n\n```python\nfrom sionna.fec.ldpc.encoding import LDPC5GEncoder\nencoder = LDPC5GEncoder(k, n)  # initialize the encoder\n# encode the interleaved bits\nc = encoder(u_int)  \n```\n\nIn this example, the encoder is initialized with a 5G LDPC encoder, but you can replace it with any other encoder available in Sionna, such as the `Polar5GEncoder` or the `LDPC5GEncoder`.\n\nPlease note that the code snippets provided are based on the context of Sionna, and may need to be adjusted based on your specific use case."
"The `LMMSEEqualizer` class in the OFDM MIMO transmissions context is used for \nthe detection of transmitted streams or symbols in an OFDM-based MIMO system. \nLMMSE stands for Linear Minimum Mean Square Error, and the class implements \nLMMSE equalization for the OFDM MIMO transmissions.\n\nIn the context of OFDM (Orthogonal Frequency-Division Multiplexing) which is a \nmodulation technique that divides the channel into multiple parallel sub-channels, \nthe `LMMSEEqualizer` class assumes the following model for the received signal:\n\ny = Hx + n\n\nwhere:\n- y is the received signal vector\n- H is the channel matrix\n- x is the transmitted symbol vector\n- n is a noise vector\n\nThe class takes as input parameters the number of receivers (`num_rx`), \nthe number of transmitters (`num_tx`), the number of antennas per receiver (`num_ant_rx`), \nthe number of antennas per transmitter (`num_ant_tx`), and the number of bits per symbol (`num_bits_per_symbol`).\n \nThe `LMMSEEqualizer` class can be used as a layer in a Keras model, and prior to \nits use, one must provide the channel realizations and the number of OFDM symbols \nthrough the `set_side_info` method.\n\nAn important thing to note is that the `LMMSEEqualizer` class is more effective \nwhen the `SparseLDPCEqualizer` is used as the outer equalizer. The reason is that it \nprovides more effective equalization by leveraging the channel state information (CSI) \nthat the LMMSE equalizer requires. For this to work, one needs to set \n`full_csi=False` in the parameter list of the `SparseLDPCEqualizer`.\n\nHere is a code snippet demonstrating how to use the `LMMSEEqualizer` class:\n\n```python\nnum_rx = 1\nnum_rx_ant = 1\nnum_tx = 4\nnum_tx_ant = 8\nnum_users = 2\nnum_bits_per_symbol = 4\n\nlmmse_equalizer = LMMSEEqualizer(num_rx, num_rx_ant, num_tx, num_tx_ant, num_bits_per_symbol)\nsparse_ldpc_equalizer = SparseLDPCEqualizer(lmmse_equalizer, num_bits_per_symbol, hard_out=True, full_csi=False)\n\n# This is 'h' in the mathematical formulation\nh = np.random.normal(size=[num_rx, num_rx_ant, num_tx, num_tx_ant]) + 1j*np.random.normal(size=[num_rx, num_rx_ant, num_tx, num_tx_ant])\n# Generate channel knowledge\n# This is a random estimator of 'h' in the mathematical formulation\n# Here, we use Gaussian sampling for simplicity\nchannel_weights = np.random.normal(size=[num_rx, num_rx_ant, num_tx, num_tx_ant, num_users]) + 1j*np.random.normal(size=[num_rx, num_rx_ant, num_tx, num_tx_ant, num_users])\n\n# Adopt optional Keras model for convenience\nclass MIMOOFDMLink(tf.keras.Model):\n    def __init__(self, channel_model, num_rx, num_rx_ant, num_tx, num_tx_ant, num_users, num_bits_per_symbol):\n        super().__init__()\n        assert num_rx==num_rx_ant.shape[0], \"num_rx must be == num_rx_ant\"\n        assert num_tx==num_tx_ant.shape[0], \"num_tx must be == num_tx_ant\"\n        assert num_rx_ant.shape[0]==num_tx_ant.shape[0], \"num_rx_ant must be == num_tx_ant\"\n        self.channel_model = channel_model\n        # Number of data and index streams is determined by the bit rate\n        self.num_data_streams_per_user = int(np.floor(OBSERVED_CHANNEL_GAUGES/num_bits_per_symbol))\n        self.num_index_streams_per_user = int(np.floor(INDEX_GAUGES/num_bits_per_symbol))\n        self.observations_to_data = tf.range(0, self.num_data_streams_per_user*num_bits_per_symbol, self.num_bits_per_symbol)\n        self.observations_to_indices = tf.range(self.num_data_streams_per_user*self.num_index_streams_per_user, self.num_data_streams_per_user*self.num_index_streams_per_user + self.num_index_streams_per_user, self.num_index_streams_per_user)\n        self.data_indicies_to_observation = tf.scatter_nd(self.observations_to_data[:, None], tf.range(self.num_data_streams_per_user, dtype=tf.int32)[None, :], shape=[self.num_data_streams_per_user*num_bits_per_symbol])\n        self.indices_indicies_to_observation = tf.scatter_nd(self.observations_to_indices[:, None], tf.range(self.num_data_streams_per_user*self_index_streams_per_user, dtype=tf.int32)[None, :], shape=[self.num_data_streams_per_user*num_bits_per_symbol])\n        self.equalizer = EqualizerAdapter(equalizer=sparse_ldpc_equalizer,\n                                          receiver_preamble=self.channel_model.rx_preamble,\n                                          transmitter_preamble=self.channel_model.tx_preamble,\n                                          dtype=tf.complex64)\n    def call(self, batch_size, snr_db):\n        # Implement the end-to-end system including channel sampling and processing\n        # snr_db: can be either a scalar or a tensor depending on whether you want to\n        #       to estimate the SNR based on CSI\n        pass\n\n    def csi_sampling(self, batch_size, snr_dbs):\n        # Implement channel sampling as described in the context\n        pass\n\nmimo_ofdm_link = MIMOOFDMLink(channel_model, num_rx, num_rx_ant, num_tx, num_tx_ant, num_users, num_bits_per_symbol)\n\n# The `MODEL-PART` comment indicates where the model part ends\n# Instantiate the model part\nmodel_part = sparselpdc_model(sparse_ldpc_dec, mimo_ofdm_link, node_name=\"LDPC-OFDM-Link\")\n# Load the weights of the model part\nmodel_part.set_weights(np.load('/content/drive/MyDrive/Colab Notebooks/saved_model_part_weights.npy', allow_pickle=True))\n# Test the BER of the model part\ntest_ber(mimo_ofdm_channel_model, mimo_ofdm_channel_estimator, model_part, ebno_dbs=np.linspace(EBN0_DB_MIN, EBN0_DB_MAX, 20), batch_size=BATCH_SIZE, num_target_block_errors=100, max_mc_iter=100)\n```\n\nThis code sets up the usage of the `LMMSEEqualizer` class and demonstrates how to make use of it in the context of OFDM MIMO transmissions. It also shows how the `LMMSEEqualizer` class can be used to compute the Minimum Mean Square Error (MMSE) equalizer in an OFDM transmission, assuming full knowledge of the channel state information."
"To get started with Sionna, follow these steps:\n\n1. Install the Sionna package. You can find detailed instructions in the Sionna README on GitHub. Apart from installing Sionna, you also need to install TensorFlow and other necessary components. Here is an example of how to install Sionna using pip:\n\n```python\ntry:\n    import sionna\nexcept ImportError as e:\n    import os\n    os.system(\"pip install sionna\")\n    import sionna\n```\n\n2. Import the necessary modules and functions from Sionna. In the provided context, the `sionna.channel` and `sionna.utils` modules are imported. \n\n3. Read the tutorial and execute the examples. The tutorial provides an in-depth overview of how to use Sionna and its various components such as the OFDM Resource Grid, Transmitter, Receiver, etc. \n\nThe tutorial is structured into sections, each demonstrating key aspects and features of Sionna. \n\n- The first section introduces Sionna and explains how to install it.\n- The second section explains how to generate and visualize channel impulse responses.\n- The third section explains how to transform the channel impulse response to the frequency domain for modeling the channel frequency response.\n- The fourth section explains how to generate batches of random channel impulse responses and frequency domain channel responses.\n- The fifth section explains how to implement a simple OFDM link.\n- The sixth section explains how to evaluate the end-to-end link-level performance.\n- The final section explains how to efficiently run a large number of simulations.\n\nRemember to carefully go through the tutorial and understand each section to effectively use Sionna."
"To import the necessary modules from the Sionna package, you can follow these steps:\n\n1. First, try to import the Sionna package. If it's not installed, install it using pip and then import it:\n\n```python\ntry:\n    import sionna\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna\n```\n\n2. Import the necessary modules from the Sionna package. For example, to import the `load_scene`, `PlanarArray`, `Transmitter`, `Receiver`, `RadioMaterial`, `Camera` from `sionna.rt`, `r_hat` from `sionna.rt.utils`, `PI`, `SPEED_OF_LIGHT` from `sionna.constants`, and `expand_to_rank` from `sionna.utils`, you can use the following code:\n\n```python\nfrom sionna.rt import load_scene, PlanarArray, Transmitter, Receiver, RadioMaterial, Camera\nfrom sionna.rt.utils import r_hat\nfrom sionna.constants import PI, SPEED_OF_LIGHT\nfrom sionna.utils import expand_to_rank\n```\n\nRemember to replace the modules in the import statement with the ones you need for your specific use case."
"To apply the gradient to the weights using an optimizer in Sionna, you need to follow these steps:\n\n1. Define your model: First, you need to define your model. This includes defining the layers of your model and how they are connected. For example, you can use the `AffineLayer` mentioned in the context, which includes a `Dtype` parameter for the datatype of your weights and biases, a `dtype` for the datatype of the input and output, and an `activation` function. \n\nHere is an example of how you might define an `AffineLayer` with a `SoftmaxCrossEntropyLoss` function:\n\n```python\nlayer = AffineLayer(\n    name=\"classifier\",\n    n_out=2,\n    with_b=False,\n    dtype='tf.float32',\n    activation='none'\n)\nloss_function = SoftmaxCrossEntropyLoss(num_classes=2, dtype='tf.float32')\n```\n\n2. Define the optimizer: Next, you need to define your optimizer. This could be one of the optimizers provided by Sionna, such as `SGD`, `Adam`, or `RGDM`, or you can define your own optimizer. The optimizer is responsible for applying the gradients to the weights. For example, to use the `SGD` optimizer, you would do:\n\n```python\noptimizer = SGD(1e-2, l2=1e-4)\n```\n\n3. Run the forward and backward pass: You can run the forward pass of your model by calling the model with the input data. The output of the forward pass is the prediction of your model. You can run the backward pass by calling the `backward` method of the loss function with the prediction and the target as arguments. This will compute the gradients of the weights with respect to the loss. \n\n```python\ny = model(x)\nloss, _, _ = loss_function(y, t) # no need to compute the intermediate values\ndtype = model.dtype\nx_grad, w_grad, b_grad = loss_function.backward(1, y, loss, dtype=dtype)\n```\n\n4. Apply the gradient: Finally, you can apply the gradient to the weights using the optimizer. The `apply_gradients` method of the optimizer can be used to apply the gradient to the weights. \n\n```python\noptimizer.apply_gradients(zip(w_grad, b_grad, w_grad, b_grad))\n```\n\nRemember to ensure that your model and optimizer are configured to use the correct datatypes, and that the number of dimensions of your input is consistent with the expectations of your model."
"Here is the detailed information of [sionna.mimo.qam(num_bits_per_symbol, normalize=True)](https://nvlabs.github.io/sionna/_modules/sionna/mimo/utils.html#qam):\n\nConstruct the constellation for a QAM constellation.\n\n**Input**\n\n- `num_bits_per_symbol` (int): The number of bits per constellation point. Must be greater or equal to 1.\n\n- `normalize` (bool): If True, the constellation is normalized to have unit power. Defaults to True.\n\n**Output**\n\n- `constellation` (Constellation): The constellation.\n\nsource code:\n```python\ndef qam(num_bits_per_symbol, normalize=True):\n    r\"\"\"QAM constellation.\n\n    This function returns a QAM constellation object [V.6.4-5]_ of size :math:`2^{n}`,\n    where :math:`n` is ``num_bits_per_symbol``.\n\n    Parameters\n    -----------\n    num_bits_per_symbol : int\n        The number of bits per constellation point. Must be greater or equal\n        to 1.\n\n    normalize : bool\n        If `True`, the constellation is normalized to have unit power.\n        Defaults to `True`.\n\n    Returns\n    -------\n    constellation : :class:`~sionna.mapping.Constellation`\n        The QAM constellation.\n    \"\"\"\n    points = _qam_points(num_bits_per_symbol)\n    return Constellation(points, normalize)\n```"
"Here is the definition of RandomInterleaver: sionna.fec.interleaving.RandomInterleaver(seed=None, keep_batch_constant=False, axis=- 1)\n  \nThe source code is as follows:\n\n```python\nclass RandomInterleaver(Interleaver):\n    \"\"\"RandomInterleaver(seed=None, keep_batch_constant=False, axis=-1)\n\n    Permute tensors through pseudo-random row/column swaps.\n\n    This class inherits from the Keras layer class and can be used as layer in\n    a Keras model.\n\n    Parameters\n    ----------\n        seed : int or None\n            Set the initial state of the random number generator for\n            reproducibility.\n\n        keep_batch_constant : bool\n            If activated, all samples in the batch are permuted with the\n            same pattern. Defaults to `False`.\n\n        axis : int\n            The dimension that should be interleaved. First dimension\n            (`axis=0`) is not allowed. Defaults to `-1`.\n\n    Input\n    -----\n        x : tf.DType\n            2+D tensor of arbitrary shape, with most insignificant dimension\n            of length `n`.\n\n    Output\n    ------\n        tf.DType\n            2+D tensor of same shape as ``x``.\n\n    Raises\n    ------\n        AssertionError\n            If ``seed`` is not `None` or `int`.\n\n        AssertionError\n            If ``keep_batch_constant`` is not `bool`.\n\n        AssertionError\n            If ``axis`` is not `int`.\n\n        AssertionError\n            If requested dimension is not `-1` or `>=2`.\n\n    Note\n    ----\n        To realize true \"bit\" interleaving, ``axis`` should be set to `-1` and\n        `keep_batch_constant` to `True`.\n    \"\"\"\n\n    def __init__(self,\n                 seed=None,\n                 keep_batch_constant=False,\n                 axis=-1):\n        super().__init__(axis=axis,\n                         seed=seed,\n                         keep_batch_constant=keep_batch_constant,\n                         rand_type=\"bit\",\n                         forward_permute=True,\n                         inverse_permute=True)\n```"
"Here is the source code of pam:  [sionna.signal.pam(b, return_indices=False, method='constellation', dtype=tf.complex64)](https://nvlabs.github.io/sionna/_modules/sionna/signal/utils.html#pam)  \n  \nFunction to (de)map bits to/from a PAM constellation point. Only works for binary representations of constellation points, i.e., [ 0, 0], [ 0, 1], [ 1, 0], [ 1, 1] for 4PAM.\n\nFor creating a custom 4-dimensional 4PAM constellation, the following street pat takes permutations of the array [0,1,2,3] and appends a 0 and 1 to each element. This constellation is then mapped to a binary tensor with shape $2^5 = 32$.  \n  \n[0, 1, 2, 3]  $\\rightarrow$  [[0, 0], [0, 1], [1, 0], [1, 1]]  $\\rightarrow$  [ 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, "
"The `Polar5GDecoder` class in the Sionna library is used for decoding codewords in a Polar code, taking into account the 5G compliant rate-matching and code-partitioning scheme. This class also supports low-density parity-check (LDPC) code concatenation, which is a method used to combine multiple error-correcting codes into a single code of higher block length.\n\nThe `Polar5GDecoder` has two optional parameters that are relevant to the context you provided: `return_crc_status` and `output_dtype`. \n\n1. `return_crc_status`: This parameter is used to control whether the decoder should return the CRC (cyclic redundancy check) status after decoding. In the 5G context, the CRC is used for checking the validity of the decoded data, and including the CRC adds overhead but can be useful for verifying the integrity of the data.\n\n2. `output_dtype`: This parameter is used to set the output data type of the layer. In the context of neural network modeling, the output data type can impact computational performance and precision. \n\nIn the provided context, there is no direct assignment of these parameters for the `Polar5GDecoder`. However, when initializing the decoder, the `return_crc_status` parameter can be passed either as a positional or a keyword argument. The `output_dtype` can be set through the `tf.keras.layer.Dense` layer, which is used as part of the encoding process for Polar codes.\n\nHere is a simplified example of how these parameters might be used:\n\n```python\nfrom sionna.fec.polar.encoding import Polar5GEncoder\nfrom sionna.fec.polar.decoding import Polar5GDecoder\n\n# Example for the encoding process\nencoder = Polar5GEncoder(k=32, n=64)\npolar_codeword = encoder(polar_information)\n\n# Example for the decoding process\ndecoder = Polar5GDecoder(k=32, n=64)\n# Can be called with or without CRC status output\ndecoded_information, crc_status = decoder(polar_codeword)\n```\n\nIn this code, `return_crc_status` is not specified, so it defaults to `False`. If you want the decoder to return the CRC status, you would set it to `True` when initializing the decoder. The `output_dtype` is also not explicitly set here, but it can be specified as needed."
"To train the neural receiver using the Sionna package and save the weights of the models in a file, follow these steps:\n\n1. Define and train the neural receiver for a specified number of training iterations. This is done using a custom training loop, which includes a function for the model to forward propagate, compute the mean square error, and gradients, and apply the gradients using an optimizer. The training loop also keeps track of the progress and saves the weights of the model periodically.\n\n```python\n# Define the training loop\n@tf.function\ndef training_loop():\n    for it in range(num_training_iterations):\n        # Forward pass\n        with tf.GradientTape() as tape:\n            tape.watch((x, no))\n            y = model([x, no])\n\n            # Compute the mean square error\n            mse = tf.reduce_mean(tf.pow(tf.abs(h)-tf.abs(y), 2))\n\n        # Compute and apply gradients\n        weights = model.trainable_weights\n        grads = tape.gradient(mse, weights)\n        optimizer.apply_gradients(zip(grads, weights))\n\n        # Update progress\n        if it % 100 == 0:\n            print(\"Iteration\", it, \"MSE\", mse.numpy())\n\n        # Save the weights of the model\n        if it % 1000 == 0:\n            model.save_weights('model_weights', overwrite=True)\n```\n\n2. Call the training loop function to start the training.\n\n```python\ntraining_loop()\n```\n\n3. After training, the weights of the model are saved in a file named 'model_weights'.\n\nRemember to replace `num_training_iterations` with the actual number of training iterations you want to run, and to define the `model` and `optimizer` before starting the training loop. The `model.save_weights` function is used to save the weights of the model to a file. \n\nPlease note that the provided context does not include the definition of the `LSTMReceiver` or the `training parameters` such as the number of training iterations, batch size, and the learning rate. You should also ensure that the dataset is properly loaded and preprocessed before starting the training."
"Here are some useful links of it:\n\n[Run in Google Colab](https://colab.research.google.com/github/nvlabs/sionna/blob/main/examples/Sionna_tutorial_part1.ipynb)\n\n[View on GitHub](https://github.com/nvlabs/sionna/blob/main/examples/Sionna_tutorial_part1.ipynb)\n\n[Download notebook](https://nvlabs.github.io/sionna/examples/Sionna_tutorial_part1.ipynb)"
"To run some throughput tests in Graph Mode using the Sionna Python package, follow these steps:\n\n1. Import the necessary modules and set up the environment:\n\n```python\nimport os\nimport sionna\nfrom sionna.apps import ThroughputTester\nimport numpy as np\nimport tensorflow as tf\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], 30000)\n    except:\n        pass\ntf.get_logger().setLevel('ERROR')\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n```\n\n2. Set up the system configuration, including the transmitter, receiver, and channel:\n\n```python\nconfig = sionna.Config()\nconfig.tx = sionna.e2e.Transmitter(\"sig_tx\", [32, 1, 1])\nconfig.rx = sionna.e2e.Receiver(\"sig_rx\", [32, 1, 1])\nconfig.channel = sionna.channel.FreeSpace(\"freespace\", 1.4, 0.0)\n```\n\n3. Set up the OFDM resource grid:\n\n```python\nconfig.resource_grid = sionna.ofdm.ResourceGrid(\"rgrid\", 1024, 12, 0, pilot_pattern=\"kronecker\", pilot_ofdm_symbol_indices=[2, 11])\n```\n\n4. Define the function for running the throughput tests:\n\n```python\ndef run_throughput_tests(batch_size, num_tx, ebno_db):\n    snr = sionna.utils.ebno2snr(ebno_db, num_bits_per_symbol=1, coderate=1)\n    config.channel.snr = snr\n    throughput_tester = ThroughputTester(sig, num_tx, length)\n    sig = throughput_tester(batch_size, num_tx, ebno_db)\n    return sig\n```\n\n5. Run the throughput tests for different batch sizes and Eb/N0 values:\n\n```python\nbatch_size = 1000\nebno_db = 5\nsig = run_throughput_tests(batch_size, num_tx, ebno_db)\n```\n\n6. Evaluate the results:\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nebno_db = 10\nsig = run_throughput_tests(batch_size, num_tx, ebno_db)\nplt.plot(np.real(sig[0, 0, :]));\nplt.plot(np.imag(sig[0, 0, :]));\n```\n\n7. Repeat the steps for different parameters as needed.\n\nPlease note that the code snippets provided are based on the context of the Sionna Python package and may need to be adjusted based on your specific use case."
"We'll demonstrate two ways of configuring a rate-1/2, constraint-length-3 convolutional code encoder using the `ConvEncoder` class in the `sionna.fec.conv` module. The encoder is initially instantiated with different methods based on provided specifications.\n\nFirst, we'll show instantiation using `rate` and `constraint_length` parameters:\n\n```python\nfrom sionna.fec.conv import ConvEncoder\n\n# Create a convolutional encoder with rate-1/2 and constraint-length-3\nencoder = ConvEncoder(rate=(1/2), constraint_length=3)\n```\n\nIn the above code, we import the `ConvEncoder` class and call it with `rate` and `constraint_length` specified as method parameters.\n\nFor the alternate method of instantiation, we'll use `gen_poly`. It's a bit more involved, as we need to specify the generator polynomials obtained from the polynomial selector (`PolynomialSelector`). The `polynomial_selector` returns the generator polynomials corresponding to the rate and constraint length specified:\n\n```python\nfrom sionna.fec.conv import ConvEncoder, PolynomialSelector\n\n# Create a polynomial selector\nselector = PolynomialSelector(r=1/2, constraint_length=3)\n\n# Obtain the generator polynomials\ngen_poly = selector()\n\n# Create a convolutional encoder with the generator polynomials\nencoder = ConvEncoder(gen_poly=gen_poly)\n```\nIn this method, we first import the `PolynomialSelector` class, which we use to select generator polynomials suitable for the specified rate and constraint length. Calling `PolynomialSelector` with `r` and `constraint_length` as parameters returns a callable object that yields the desired generator polynomials when called. These polynomials are then passed to the `ConvEncoder` class to create the encoder instance.\n\nNow that we have two encoder instances created using different methods, we can use them to perform encoder operations. However, the demonstration of the two instantiation methods is limited as it does not show the encoder in operation. The provided context does not include an example of using the `rate` and `constraint_length` parameters with the `ConvEncoder` class, but we can infer that it might be used like this:\n\n```python\n# Assuming 'bits' is a tensor of 0s and 1s with shape [..., n]\n# where n is the the number of information bits (input rate)\n\n# Using rate and constraint_length parameters\noutput = encoder(bits)\n```\n\nThis is how you might use the `rate` and `constraint_length` parameters to encode information bits using the `ConvEncoder` class. The exact usage may vary depending on the specific requirements of the task at hand."
"Alright, let's talk about the Rural Macrocell (RMa) model with its principles, parameters, inputs, and outputs. \n\nThe RMa model is part of the 3rd Generation Partnership Project (3GPP) and is used for wireless network simulations in Python, as provided by the Sionna package. It follows certain specifications which include the principles of RMa model.\n\nThe RMa model is non-penetrating, and comprises of user equipments (UEs) and base stations (BSs) located in an outdoor environment. The UEs and BSs are equipped with multiple antennas. The RMa model consists of a scenario object, and UEs and BSs that are instances of the `RMaUDL` and `RMaIn` classes, respectively.\n\nThe RMa model is governed by several parameters, some of which are specified in the question itself such as the carrier frequency, the number of UEs, the BS configuration, and the direction of communication.\n\nInput parameters to the RMa model include the batch size, the carrier frequency, the number of UEs, the BS configuration, and the direction of communication which have already been specified in the code snippet provided in the context.\n\nThe output of the RMa model is a tuple of two elements: a reference signal received power (RSRP) map and a reference signal received time division multiplexed (TDM) grid. The RSRP map is for the UEs, indicating the received power from all transmitters at a specific geographical position. The TDM grid depicts the time evolution of the RSRP map."
"The `Trellis` structure in the `sionna.fec.conv.utils` module is used to implement the Trellis memory scheduling (TMS) of the convolutional encoder, which is a method for controlling the pattern of state transitions in the state diagram of a convolutional code. This structure is key to exploiting the structure of the convolutional code and improving the performance of the encoder.\n\nTo configure the `Trellis` structure, you need to use the `gen_poly` and `rsc` parameters:\n\n- The `gen_poly` parameter is a tuple that contains the generator polynomials used to define the convolutional code. Each polynomial is represented in binary form, and the tuple of polynomials is used to define the connections between the current state and the next state for each possible input bit. The `gen_poly` parameter is crucial for setting up the Trellis structure as it defines the behavior of the encoder.\n\n- The `rsc` parameter is a Boolean flag that indicates whether the provided `gen_poly` polynomials are for recursive systematic encoding. When `rsc` is set to `True`, the encoder will terminate in the state '00' with the option to also terminate in the '11' state, as indicated by the `trellis.rsc` attribute.\n\nLet's consider an example to illustrate how to configure the `Trellis` structure using these parameters:\n\n```python\nfrom sionna.fec.conv.utils import Trellis\n\n# gen_poly parameter configured with generator polynomials (g1, g2)\ngen_poly = [[1, 1, 1], [1, 0, 1]]\n\n# rsc parameter set to True for recursive systematic encoding\ntrellis = Trellis(gen_poly, rsc=True)\n```\n\nIn this example, we first import the `Trellis` class from the relevant module. We then create an instance of `Trellis` using the `gen_poly` and `rsc` parameters. The `gen_poly` parameter is set with the generator polynomials expressed in binary form, and `rsc` is set to `True` to indicate that the polynomials should be chosen for recursive systematic encoding.\n\nIt is important to note that the `gen_poly` parameter requires the polynomials to be specified in the order of increasing power of the polynomial terms, with each polynomial represented as a list of its binary coefficients.\n\nThe `Trellis` structure is independent of the code rate, and hence can be used with any generator matrix and in any encoding scheme that uses a binary convolutional polynomial representation. This structure plays a vital role in providing a mapping of input bits to states and transitions in the Trellis diagram, which is crucial for operations like Viterbi decoding that utilize the full structure of the code."
"Here is the detailed information of CarrierConfig:   \n  \n[sionna.nr.CarrierConfig(**kwargs)](https://nvlabs.github.io/sionna/_modules/sionna/nr/carrier_config.html#CarrierConfig)  \n\nThe CarrierConfig objects sets parameters related to the wireless channel models, that are valid for all models. A detailed description can be found in [3GPP TR 38.901, \u201cStudy on channel model for frequencies from 0.5 to 100 GHz\u201d, Release 16.1](https://nvlabs.github.io/sionna/micht38.901_v161.html).\n\nAll configurable properties can be provided as keyword arguments during the initialization or changed later.\n\n**Example**\n```python\ncc = CarrierConfig(c_scenario=\"UMi\", carrier_frequency=3.5e9)\ncc.delay_spread = 100e-9\ncc.polarization = \"dual\"\n```\n\n**Properties**\n\n- `allowed_bw`: Allowed bandwidth. Defaults to 40.0 [MHz], specified as a float.\n- `carrier_frequency`: Carrier frequency [Hz], specified as a float.\n- `cdl`: CDL channel model to use. Must be one of \"A\", \"B\", \"C\", \"D\", or \"E\", or an instance of CDL.\n- `cyclic_prefix_length`: Cyclic prefix length. Defaults to 0 or 16, depending on the OFDM_symbol_duration.\n- `delay_spread`: RMS delay spread [s], specified as a float. Defaults to 30.0e-9 [s].\n- `doppler_spectrum`: Doppler spectrum to use. Must be one of \"UNIFORM\", \"TORPEDO\", \"PIECEWISE\", or an instance of DopplerSpectrum.\n- `num_clusters`: Number of paths clustered together. Defaults to None. Clustering is not performed if this is set to None.\n- `num_paths`: Number of paths (or multipaths). Defaults to None. Channel sampling will not be configured if this is set to None.\n- `ofdm_symbol_duration`: Duration of an OFDM symbol [s], specified as a float. Defaults to 66.7e-9 or 125e-9, depending on the `fft_size`..\n- `scenario`: Scenario to use. Must be one of \"UMi\", \"UMa\", or \"RMa\".\n- `synthetic_array`: If True, use synthetic arrays. Defaults to False.\n- `thermal_noise`: Thermal noise density [W/Hz], specified as a float. Defaults to 1.0e-20 [W/Hz].\n- `use_gpu`: Use GPU for the generation of channel impulse responses. Defaults to False.\n- `use_o2i_model`: Use the model from [3GPPTS 38.901, \u201cOutdoor-to-Indoor models for 5 GHz SAR Verification\u201d, 3GPP TR 38.901, Release 16.1](https://nvlabs.github.io/sionna/models.html#3gppts-38-901). Defaults to False.\n- `verbose`: Set to False to deactivate verbose mode, or True to activate it. Defaults to True.\n  \n**Methods**\n\n- `show()`: [source](https://nvlabs.github.io/sionna/_modules/sionna/nr/carrier_config.html#CarrierConfig.show) Print all properties of the carrier config and their current values. source code:\n```python\n    def show(self):\n        \"\"\"Print all properties of the carrier config and their\n        current values\"\"\"\n        for key in self.__dict__.keys():\n            if not key.startswith(\"_\") and not hasattr(self.__class__, key):\n                val = getattr(self, key)\n                print(f\"{key} : {val}\")\n```\n  "
"Here is the definition of Receiver: sionna.rt.Receiver(name, position, orientation=(0.0, 0.0, 0.0), look_at=None, color=(0.153, 0.682, 0.375), dtype=tf.complex64)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/rt/receiver.html#Receiver)  \n\nsource code:\n```python\n#\n# SPDX-FileCopyrightText: Copyright (c) 2021-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n\"\"\"\nClass implementing a receiver\n\"\"\"\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom .transmitter import Transmitter\nfrom .radio import Radio\nfrom .message\n\nimport numpy as np\nfrom .object import Object\nfrom .receiver import Receiver\nfrom .scene import Scene\nfrom sionna.constants import PI\nfrom .warped_tensor import WarpedTensor\nfrom .utils import rot_mat\nfrom .antenna import AntennaArray\nfrom .radio import Radio\nimport tensorflow asjson as json\nfrom dataclasses import dataclass, field\nimport sys\n\n@dataclass\nclass Receiver(Radio):\n    \"\"\"\n    Class implementing a radio receiver\n\n    The `position` and `orientation` properties can be assigned to a TensorFlow\n    variable or tensor. In the latter case, the tensor can be the output of a\n    callable, such as a Keras layer implementing a neural network. In the\n    former case, it can be set to a trainable variable:\n\n    .. code-block:: Python\n\n        rx = Transmitter(name=\"my_receiver\",\n                         position=tf.Variable([0, 0, 0], dtype=tf.float32),\n                         orientation=tf.Variable([0, 0, 0], dtype=tf.float32))\n\n    Parameters\n    ----------\n    name : str\n        Name\n\n    position : [3], float\n        Position :math:`(x,y,z)` [m] as three-dimensional vector\n\n    orientation : [3], float\n        Orientation :math:`(\\alpha, \\beta, \\gamma)` [rad] provided\n        as three angles corresponding to a 3D rotation\n        as defined in :eq:`rotation`.\n        This parameter is ignored if ``look_at`` is not `None`.\n        Defaults to [0,0,0].\n\n    look_at : [3], float | Transmitter | Receiver | Camera | None\n        A position or the instance of a :class:`Transmitter`,\n        :class:`Receiver`, or :class:`Camera` to look at.\n        If set to `None`, then ``orientation`` is used to orientate the device.\n\n    color : [3], float\n        Defines the RGB (red, green, blue) ``color`` parameter for the device as displayed in the previewer and renderer.\n        Each RGB component must have a value within the range :math:`\\in [0,1]`.\n        Defaults to `[0.153, 0.682, 0.375]`.\n\n    dtype : tf.complex\n        Datatype to be used in internal calculations.\n        Defaults to `tf.complex64`.\n    \"\"\"\n    @staticmethod\n    def _jones_pattern(antenna, scene, dtype):\n        # Short-hand notation\n        pos = scene.position\n        ori = scene.orientation\n        pol = antenna.polarization\n\n        # Remove last dimension\n        pol = tf.squeeze(pol, axis=-1)\n\n        # Polarization direction. We assume that the polarization direction\n        # is given by the y-axis of the antenna local coordinate system.\n        k_hat = tf.constant([0,1,0], dtype=dtype)\n\n        # Slant angle\n        alpha_s = tf.acos(tf.tensordot(k_hat, antenna.direction, axes=(-1,-1)))\n        alpha_s = tf.broadcast_to(alpha_s, scene.size)\n\n        # Compute the slant angle of the receiver coordinate system\n        # w.r.t. the system defined by the antenna\n        alpha_r = scene.orientation[:,0]\n        beta_r = scene.orientation[:,1]\n        gamma_r = scene.orientation[:,2]\n        rot_sr = rot_mat(scene.orientation)\n        rot_as = rot_mat(antenna.orientation)\n        rot_ra = rot_mat([-gamma_r, -beta_r, -alpha_r])\n        rot_sr = tf.transpose(rot_sr, [1,0,2])\n        rot_as = tf.transpose(rot_as, [1,0,2])\n        rot_ra = tf.transpose(rot_ra, [1,0,2])\n        rot_slant = tf.linalg.matmul(rot_sr,\n                                     tf.linalg.matmul(rot_as, rot_ra))\n        alpha_slant = tf.acos(rot_slant[:,1,1])\n\n        # Compute the receive antenna field components slanted MSA\n        hr, hl = antenna.field_slanted(alpha_s, alpha_slant, pol)\n\n        # Compute the antenna pattern\n        # The antenna pattern is real, so we need to expand the dimension\n        # to make it compatible with Keras broadcasting\n        hl = tf.expand_dims(hl, axis=-1)\n        hr = tf.expand_dims(hr, axis=-1)\n        ant_pat = tf.concat([hr, hl], axis=-1)\n        ant_pat = tf.squeeze(ant_pat, axis=0)\n\n        return ant_pat\n\n    @tf.property\n    def scene(self):\n        r\"\"\"\n        Returns\n        -------\n        scene : :class:`~sionna.rt.Scene`\n            The scene in which the device is instantiated.\n        \"\"\"\n        return self._scene\n\n    @property\n    def antenna(self):\n        r\"\"\"\n        Returns\n        -------\n        antenna : :class:`~sionna.rt.Antenna`\n            The antenna of the device.\n        \"\"\"\n        return self._antenna\n\n    @antenna.setter\n    def antenna(self, antenna):\n        if not isinstance(antenna, AntennaArray):\n            raise TypeError(\"The antenna parameter must be an instance of AntennaArray.\")\n        self._antenna = antenna\n\n    @property\n    def position(self):\n        r\"\"\"\n        Returns\n        -------\n        position : [3], float\n            The position :math:`(x,y,z)` [m] as three-dimensional vector.\n        \"\"\"\n        return self._position\n\n    @position.setter\n    def position(self, new_position):\n        if new_position.dtype not in (tf.float16, tf.float32, tf.float64):\n            raise ValueError(\"The dtype of new_position must be real-valued.\")\n\n        self._position = tf.cast(new_position, dtype=self._rdtype)\n\n    @property\n    def orientation(self):\n        r\"\"\"\n        Returns\n        -------\n        orientation : [3], float\n            The orientation :math:`(\\alpha, \\beta, \\gamma)` [rad]\n            as three angles corresponding to a 3D rotation\n            as defined in :eq:`rotation`.\n        \"\"\"\n        return self._orientation\n\n    @orientation.setter\n    def orientation(self, new_orientation):\n\n        if new_orientation.dtype not in (tf.float16, tf.float32, tf.float64):\n            raise ValueError(\"The dtype of new_orientation must be real-valued.\")\n\n        # Update the rotation matrix\n        new_rotation = from_rotation(new_orientation)\n        self._rotation = new_rotation\n\n        # Orientation is computed from the rotation matrix\n        # See eq. (1) of https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation\n        # alpha\n        a = new_rotation[2,2] + new_rotation[1,1] - new_rotation[0,0]\n        b = 2 * (new_rotation[0,2] - new_rotation[1,2])\n        alpha = tf.math.atan2(b, a)\n\n        # beta\n        c = 2 * (new_rotation[2,2] - new_rotation[0,0])\n        d = 2 * (new_rotation[0,1] + new_rotation[1,1])\n        e = tf.math.cos(new_rotation[1,0])\n        beta = tf.math.atan2(d, tf.cast(e, dtype=self._rdtype),)\n\n        # gamma\n        f = new_rotation[0,0]+new_rotation[1,1]+new_rotation[2,2]\n        g = 2 * (new_rotation[2,0] - new_rotation[0,2])\n        h = tf.math.atan2(g, f)\n\n        # Update slant angles\n        self._orientation = (alpha, beta, gamma)\n\n    @property\n    def look_at(self):\n        \"\"\"\n        Returns\n        -------\n        look_at : [3], None or (:class:`~sionna.rt.Transmitter`,\n            :class:`~sionna.rt.Receiver`, :class:`~sionna.rt.Camera`), tf.Variable\n            A position or the instance of a :class:`Transmitter`,\n            :class:`Receiver`, or :class:`Camera` to look at. If set\n            to `None`, then ``orientation`` is used to orient the device.\n            Defaults to `None`.\n        \"\"\"\n        return self._look_at\n\n    @look_at.setter\n    def look_at(self, value):\n        # Check if the value is a warps tensor\n        if not isinstance(value, WarpedTensor) and value is not None:\n            if not hasattr(value, \"position\"):\n                raise ValueError(\"If look_at is not `None`, then it \" \\\n                                 \"must have a position property.\")\n        self._look_at = value\n\n    @property\n    def num_look_samples(self):\n        \"\"\"\n        Returns\n        -------\n        num_look_samples : int\n            Number of random samples to look at to get the perfect look.\n            This parameter is used to compute the perfect look probability.\n        \"\"\"\n        return self._num_look_samples\n\n    @num_look_samples.setter\n    def num_look_samples(self, value):\n        if not isinstance(value, int):\n            raise TypeError(\"num_look_samples must be an integer.\")\n        if value < 0:\n            raise ValueError(\"num_look_samples must be nonnegative.\")\n        self._num_look_samples = value\n\n    @property\n    def perfect_cfi(self):\n        r\"\"\"\n        perfect_cfi\n            The perfect CFI, i.e., the probability that the device\n            transmits a stream if a transmitter or one of its children\n            transmits a stream towards the device.\n        \"\"\"\n        if self.look_at is None:\n            return 1.0\n        else:\n            # Look for the transmitter or the first of its children\n            dev = self.look_at\n            if isinstance(dev, Transmitter):\n                cfi = dev.csi / dev.num_streams_per_tx\n            else:\n                cfi = dev.rx_array.csi\n            return tf.reduce_sum(tf.abs(cfi)**2, axis=-2)\n\n    def __init__(self,\n                 name,\n                 position,\n                 orientation=(0.0,0.0,0.0),\n                 look_at=None,\n                 color=(0.153, 0.682, 0.375),\n                 dtype=tf.complex64):\n\n        # Initialize the base class Object\n        super().__init__(name, position, orientation, look_at, color, dtype)\n\n        # Initialize the antenna\n        self._antenna = AntennaArray(\"rx_array\", [0,0,0], orientation, color, dtype)\n\n        # Initialize the number of samples for the stochastic look\n\n        # Set the CSA to trainable by default\n        self._position._trainable = True\n        self._orientation._trainable = True\n\n    @tf.function(input_signature=[{\n        \"scene\": Scene,\n        \"time\": tf.float,\n        \"frequency\": tf.float\n    }])\n    def look(self, scene, time, frequency):\n        # If look_at is a trainable variable, then use the stochastic position\n        # and orientation\n\n        if self._look_at is None:\n            return self.look_stochastic(time, frequency)\n        else:\n            # If look_at is a instance of Radio, then use its position and orientation\n            return self.look_at_time(self._look_at, time, frequency)\n\n    # @tf.function(input_signature=[{\n    #     \"scene\":Scene,\n    #     \"transmitter\":Transmitter,\n    #     \"time\":tf.float,\n    #     \"frequency\":tf.float}])\n    # def look_at(self, scene, transmitter, time, frequency):\n    #     # If look_at is a Transmitter, then use its position and orientation\n    #     return self.look_at_time(transmitter, time, frequency)\n\n    def look_stochastic(self, time, frequency):\n        # Set the receiver to the transmitters position\n        self.position = 0\n\n        # Generate a random orientation\n        # See https://mathworld.wolfram.com/SpherePointPicking.html\n        alpha = tf.random.uniform((), minval=0., maxval=2*PI, dtype=tf.float32)\n        beta = tf.random.uniform((), minval=0., maxval=PI, dtype=tf.float32)\n        gamma = tf.constant(0., dtype=tf.float32) # Pointing towards z\n        rot_h = rot_mat((alpha, beta, gamma))\n        rot_h = tf.repeat(rot_h, scene.num_tx, 0)\n\n        # Generate random positions to look at\n        # We simply pick random points on the unit sphere and project\n        # them on the plane perpendicular to the antenna direction.\n        # Then, we move the center of the sphere to the position of the receiver\n        # and compute the intersection with the sphere.\n\n        # Generate random positions on the unit sphere\n        r = tf.random.uniform((), minval=-1., maxval=1., dtype=tf.float32)\n        phi = tf.random.uniform((), minval=0., maxval=2*PI, dtype=tf.float32)\n        theta = tf.acos(r)\n        p = [tf.math.cos(theta)*tf.math.sin(phi),\n             tf.math.sin(theta)*tf.math.sin(phi),\n             tf.math.cos(phi)\n            ]\n        p = tf.stack(p, axis=0)\n        p = tf.expand_dims(p, 0)\n        p = tf.repeat(p, scene.num_tx, 0)\n\n        # Project the points on the plane perpendicular to the antenna direction\n        e_rr = tf.stack((1.,0.,0.), axis=0)\n        e_theta = tf.stack((0.,1.,0.), axis=0)\n        e_pp = tf.stack((0.,0.,1.), axis=0)\n\n        # Project the positions\n        p_pr = p*tf.cos(scene.tx_array.orientation[0]) + scene.tx_array.position\n        p_pr = tf.cast(p_pr, dtype=tf.complex64)\n        e_0 = tf.cast(scene.tx_array.antenna.pol_pattern_short(\"\", \"V\"), dtype=p_pr.dtype)\n        e_theta = tf.repeat(e_theta, p_pr.shape[0], 0)\n        e_pp = tf.repeat(e_pp, p_pr.shape[0], 0)\n        e_r = scene.tx_array.antenna.radiation_pattern(p_pr, e_theta, e_pp)\n        x = tf.math.pow(tf.math.abs(e_r), 2)\n\n        # Ensure numerical stability\n        x_max = tf.reduce_max(x, axis=-1, keepdims=True)\n        x = x + 1e-12\n        x_max = x_max + 1e-12\n        x_db = 10*np.log10(x)\n        x_db = tf.minimum(x_db, 50)\n        x = tf.math.reduce_max(x, axis=-1, keepdims=True)\n\n        # Ensure the energy has unit magnitude\n        x = tf.math.sqrt(x)\n\n        # Compute the CIR for the stochastic position\n        h = x*tf.exp(tf.complex(tf.constant(0, dtype=self._rdtype),\n            tf.random.normal((), 0., 0.01, dtype=self._rdtype)))\n        h = tf.expand_dims(h, axis=-1)\n\n        return h\n\n    def look_at_time(self, look_at, time, frequency):\n        # Set the receiver to the transmitters position\n        self.position = look_at.position\n        self.orientation = look_at.orientation\n        return self.look_at_space(look_at, frequency)\n\n    def look_at_space(self, look_at, frequency):\n        # Set the receiver to the transmitters position\n        self.position = look_at.position\n        self.orientation = look_at.orientation\n\n        # Compute the propagation paths\n\n        # Transmitter\n        tx = look_at\n\n        # We are not interested in the selected output port\n        ray_t, _ = tx.generate_return_rays()\n        r_t, _, _ = ray_t\n\n        # Selected device\n        device = tf.stack([self.position,\n                           r_t[...,0],\n                           r_t[...,0] - r_t[...,1],\n                           r_t[...,2]], axis=0)\n\n        # Receiver\n        r = tf.stack([0.0,\n                      0.0,\n                      0.0,\n                      0.0], axis=-1)\n        ray, _ = r\n\n        # Compute the intersection point\n\n        # Translate ray by receiver position\n        ray = ray + r\n\n        # The intersecting point between the ray and the receiver\n        # Antenna pattern is the output of the previous computation\n        ri, _ = intersect_ray_circle(ray, r, tf.constant(1.0, dtype=self._rdtype))\n        o = tf.stack([ri[...,0],\n                      ri[...,1],\n                      tf.constant(0.0, dtype=self._rdtype)], axis=-1)\n\n        # Compute the direct path gain\n        g_td = tf.math.reduce_sum(tf.math.pow(tf.math.abs(ri), 2),axis=-1)\n\n        # Compute the bounced path (specular reflection) gain\n\n        # Translate the ray by the specular reflection vector\n        s = tf.cast(2, dtype=self._rdtype)*tf.math.reduce_prod(ri, axis=-1, keepdims=True)\n        ray = ray + s\n\n        # The intersecting point between the ray and the receiver\n        # Antenna pattern is the output of the previous computation\n        ri, _ = intersect_ray_circle(ray, r, tf.constant(1.0, dtype=self._rdtype))\n        o = tf.stack([ri[...,0],\n                      ri[...,1],\n                      tf.constant(0.0, dtype=self._rdtype)], axis=-1)\n\n        # Ensure the module is differentiable\n        if look_at is None:\n            o += tf.random.normal(o.shape, 0.0, 0.01)\n\n        # Compute the indirect path gain\n        g_ti = tf.math.reduce_sum(tf.math.pow(tf.math.abs(ri), 2),axis=-1)\n\n        # And finally the total gain\n        g = g_td + g_ti\n\n        # The CIR is not directly used in the optimization as it is not\n        # diffentiable. It is computed here for visualization and analysis.\n        cir_time = tf.signal.irfft(g, tf.cast(tf.math.ceil(cir_length/time_res),\n                                              tf.int32),\n                                  fft_length=cir_length)\n\n        return cir_time\n\n    @property\n    def csi(self):\n        r\"\"\"\n        Compute the channel spatial impulse response corresponding\n        to the current receiver position and orientation.\n\n        The channel spatial impulse response is computed as follows.\n        Given a scene with a single transmitter as follows:\n\n        .. code-block:: Python\n\n            transmitter = Transmitter(name=\"tx\",\n                                       position=[8.5"
"The `ResourceGrid` in the Sionna Python package is used for synchronized OFDM transmissions with multiple layers. It's primary usage lies in system level simulations and it helps in modeling and simulating wireless channel transmissions. \n\nThe `ResourceGrid` is a part of the OFDM system module within the Sionna package and it allows you to define settings for the OFDM waveform and transmissions. You can define the number of OFDM symbols, FFT size, subcarrier spacing, number of transmitters, number of streams per transmitter, DC carrier, pilot pattern, pilot OFDM symbol indices, and indices for precoded streams. \n\nThe usage of the `ResourceGrid` mostly revolves around configuring these parameters specific to your simulation. Once configured, the `ResourceGrid` can also be visualized and it can be used to create synchronized transmissions of data and pilots. The `to_time_domain()` and `show()` methods allow you to see a rough sketch of the time-domain signal, based on the underlying OFDM waveform parameters.\n\nIn the context of multi-cell MIMO simulations, the `ResourceGrid` can be used to define time-frequency resources associated with each transmitter and receiver. This is key in helping to model and understand the propagation of channels in wireless communication systems.\n\nNote: The usage of `ResourceGrid` is detailed in the provided context through a series of code examples. However, these code examples directly reference components such as the `PilotPattern` and `LMMSEEqualizer` without providing a detailed explanation of what these components are. Understanding these components is essential to fully comprehend the usage of `ResourceGrid`."
"To generate a batch of frequency responses using the Sionna package, you can follow these steps:\n\n1. Import the necessary modules and configure your environment. This includes setting up GPU configuration and importing the Sionna package. You can also set the seed for reproducibility.\n\n2. Load the scene for which you want to compute the coverage map. This can be done using the `load_scene` function from the Sionna package.\n\n3. Configure the transmitter and receiver arrays. The number of rows and columns in the array should match the number of cross-polarized dipoles of the radio device.\n\n4. Add a transmitter and receiver to the scene using the `Transmitter` and `Receiver` classes. You can specify the name, position, and orientation of the transmitter and receiver.\n\n5. Compute the paths for the scene. This can be done using the `compute_paths` function. You can specify whether to include line-of-sight, reflection, and scattering paths. You can also specify the number of samples.\n\n6. Configure the radio materials. You can do this by setting the `radio_material` property of the transmitter and receiver arrays.\n\n7. Compute the channel impulse responses using the `cir` function from the `Paths` object. You can specify whether to include the direct path, diffuse and/or scattered paths, the number of samples, and the maximum depth.\n\n8. Compute the frequency response using the `ofdm_channel` function from the Sionna package. You can specify the noise variance and the number of OFDM symbols.\n\nHere is a simplified example of how you might do this:\n\n```python\n# Import Sionna and other necessary modules\nimport sionna\nfrom sionna.rt import load_scene, PlanarArray, Transmitter, Receiver\nfrom sionna.rt.utils import r_hat\nfrom sionna.rt import Paths\nfrom sionna.channel import OFDMChannel\n\n# Load the scene\nscene = load_scene(sionna.rt.scene.munich)\n\n# Configure the transmitter and receiver arrays\nscene.tx_array = PlanarArray(num_rows=1, num_cols=1, vertical_spacing=0.5, horizontal_spacing=0.5, pattern=\"tr38901\", polarization=\"V\")\nscene.rx_array = scene.tx_array\n\n# Add a transmitter and receiver\ndist = 5\nd = dist/np.sqrt(2)\n\tscene.add(Transmitter(name=\"tx\", position=[-d,0,d]))\n\tscene.add(Receiver(name=\"rx\", position=[d,0,d]))\n\n# Compute the paths\npaths = scene.compute_paths(max_depth=5, num_samples=int(1e6))\n\n# Configure the radio materials\nscene.tx_array.radio_material = \"my_radio_material\"\nscene.rx_array.radio_material = \"my_radio_material\"\n\n# Compute the channel impulse response\na, tau = paths.cir()\n\n# Compute the frequency response\nchannel = OFDMChannel(frequencies=freq_vec, fft_size=fft_size, impl=\"t\", normalize=True)\nh_f = channel(h, sigma_n)\n```\n\nPlease note that this is a simplified example and you may need to adjust it to fit your specific use case."
"To compute propagation paths in Sionna RT, you need to configure transmitters and receivers. Here's how to do it:\n\n1. Import the necessary modules and classes:\n\n```python\nfrom sionna.rt import Scene, Transmitter, Receiver\n```\n\n2. Load the scene that you want to analyze:\n\n```python\nscene = Scene(load, load_scene)\n```\n\n3. Add a transmitter to the scene:\n\n```python\nscene.add(Transmitter(\"tx\", [x_tx, y_tx, z_tx], [0.0, 0.0, 0.0]))\n```\n\nReplace `\"tx\"`, `[x_tx, y_tx, z_tx]`, and `[0.0, 0.0, 0.0]` with the name, position, and orientation of your transmitter.\n\n4. Add a receiver to the scene:\n\n```python\nscene.add(Receiver(\"rx\", [x_rx, y_rx, z_rx], [0.0, 0.0, 0.0]))\n```\n\nReplace `\"rx\"`, `[x_rx, y_rx, z_rx]`, and `[0.0, 0.0, 0.0]` with the name, position, and orientation of your receiver.\n\n5. Set the number of samples for the scene:\n\n```python\nscene.set_samples(sampling_frequency, num_time_samples)\n```\n\n6. Compute the propagation paths:\n\n```python\npaths = scene.compute_paths(max_depth=5, num_iter=50)\n```\n\nThis will compute the propagation paths for the defined transmitter and receiver. You can then use these paths for further analysis."
"The deprecated SymbolDemapperWithPrior class is a part of the `mapping` module of the Sionna MIMO Python package, which provides features around constellation shaping and mapping in the context of MIMO systems.\n\nThis SymbolDemapperWithPrior class was part of a previous version of this package and has been deprecated, indicating that it is no longer recommended to use this class for any new development as it may be removed in a future release. However, I can still provide some insights into its functionality based on the available context.\n\nThe SymbolDemapperWithPrior class seemed to be related to demapping operations in a MIMO communication system. It assumed prior knowledge of the bits or constellation points were available, which is a more general form of demapping function.\n\nIt is not clear from the context what specific function the deprecated SymbolDemapperWithPrior class performed, but it can be assumed that it was a demapping function that took prior information into account, which can be used to influence the way the demapping process is performed.\n\nPlease note that the demapping process in a MIMO system refers to the operation where the received symbol vectors are converted into bit/logical symbol estimates. This is a crucial part of the communication system as it reverses the mapping process that occurs at the transmitter.\n\nI would also like to add that in the former version of the package, the SymbolDemapperWithPrior class was used as a part of the SymbolDemapper class, which was used to implement a demapping function that includes prior information.\n\nHere is an illustrative example of how the SymbolDemapperWithPrior class might have been used:\n\n```python\nfrom sionna.mapping import SymbolDemapper, SymbolDemapperWithPrior\n\n# Initialize a demapper\ndemapper = SymbolDemapper(constellation_type=\"qam\", num_bits_per_symbol=4, hard_out=True)\n\n# Prior information provided as Log-likelihood Ratios (LLRs)\nllr_prior = ...  # Prior information provided by the user. Should be an appropriately shaped tensor.\n\n# Using deprecated SymbolDemapperWithPrior class\n# The deprecated class may still be functional but is not recommended\ndemapper_prior = SymbolDemapperWithPrior(demapper=demapper)\n\n# Call the demapper with prior information\nbits = demapper_prior([y, llr_prior, noise_var])\n\n# Please note that the actual usage may vary based on the specific implementation and constellation used.\n```\n\nThis is only an illustration, the actual usage may vary based on the specific implementation and constellation used. Please refer to the actual package documentation for the correct and recommended usage."
"To enable diffuse reflections and observe their impact on the coverage map in Sionna, you need to use the Sionna RT module and the Sionna channel module.\n\n1. Import the necessary modules and set up the environment:\n\n```python\nimport os\nimport tensorflow astf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sionna.rt import load_scene, PlanarArray, Transmitter, Receiver, RadioMaterial, Camera\nfrom sionna.rt.utils import r_hat\nfrom sionna.utils import expand_to_rank\n```\n\n2. Load the scene and add a transmitter and receiver:\n\n```python\nscene = load_scene(sionna.rt.scene.simple_wedge)\nscene.add_t"
"To compute the mean-squared error (MSE) between the transmitted and recovered symbols in the context of the Sionna package, you can follow these steps:\n\n1. First, ensure that you have the necessary imports. This includes the Sionna package and other necessary packages such as numpy and tensorflow. If the Sionna package is not already installed, you can install it using pip.\n\n```python\ntry:\n    import sionna as sn\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna as sn\n```\n\n2. Make a transmission using the `PAM` class from Sionna. The transmitted bits are input as a binary tensor, and a custom \"pam4\" constellation is used in this case.\n\n```python\nnum_bits_per_symbol = 2 # The number of bits per constellation point, i.e., 16-QAM\nconstellation_type = \"pam\" # Choose a PAM constellation\nconstellation = sn.mapping.Constellation.create_or_load(constellation_type, num_bits_per_symbol, \"pam4\")\npam_mod = sn.mapping.PAM(constellation)\nx = pam_mod(tx_bits)\n```\n\n3. Recover the symbols using a reference PAM receiver. The received symbols are computed from the transmit signal and noise variance, and a decision is made on the transmitted symbols.\n\n```python\nawgn_channel = sn.channel.AWGN()\nno = 0.04 # Noise variance\ny = awgn_channel([x, no])\nllr = sn.mapping.ReferencePAMReceiver([y, no])\nrx = llr(x)\n```\n\n4. Compute the MSE by taking the average of the squared difference between the transmitted and recovered symbols. This is done for both the real and imaginary parts of the symbols, and then averaged over the batch size and number of symbols per batch.\n\n```python\nmse_real = sn.utils.variance(torch.abs(torch.real(x) - torch.real(rx)), reduce=\"mean\")\nmse_imag = sn.utils.variance(torch.abs(torch.imag(x) - torch.imag(rx)), reduce=\"mean\")\nmse = (mse_real + mse_imag) / 2\n```\n\nThis approach assumes that you have pre-defined the variables `tx_bits` (the transmitted bits) and `x` (the transmitted signal) for the transmission, and that the received signal `y` and noise variance `no` have been computed after the transmission. If these variables are not defined, or if there are any other issues, the MSE computation may not give the desired result."
"The `create_or_check_constellation` method in the `Constellation` class is used to manage and return a constellation object. This object defines a complex-valued vector of constellation points, which is used in the mapping and demapping process of the simulation. \n\nThe method first checks if there is an existing constellation stored in the class. If not, it creates a new constellation with the desired size and constellation type, and initializes a seed for the generator. On the other hand, if a constellation has already been created, the method checks that the size and type of the existing constellation match the requested parameters. If they do, the method returns the existing constellation. If they don't match, an assertion is raised.\n\nThe constellation type can be either 'qam', 'pam', or 'custom'. For 'custom', an additional parameter `custom_labels` is required, which is a list of desired constellation point labels.\n\nHere is an abridged version of the `create_or_check_constellation` method:\n\n```python\n    def create_or_check_constellation(self,\n                                      num_bits_per_symbol,\n                                      constellation_type=None,\n                                      custom_labels=None):\n        # existing implementation of:\n        # * checking that an appropriate constellation has been created\n        # * creating a new constellation if there isn't one.\n\n        [...]\n        \n        # If constellation is not yet created, create a potentially random QAM\n        # constellation.\n        # Otherwise, ensure that the existing constellation is consistent with\n        # the requested parameters.\n        if self._num_bits_per_symbol is None:\n            assert (constellation_type is not None), \\\n                \"constellation_type must be provided\"\n            assert (1 == len(num_bits_per_symbol)),\"num_bits_per_symbol must be a list of int defining the number of bits per symbol for each of the 2 dimensions\"\n            self._num_bits_per_symbol = num_bits_per_symbol\n            assert (constellation_type in (None, \"qam\", \"pam\", \"custom\")), \\\n                \"Wrong value of constellation_type\"\n            if constellation_type is None:\n                constellation_type = \"qam\"\n            c = Constellation.create_constellation(  constellation_type,\n                                                    num_bits_per_symbol[0],\n                                                    Num=2,\n                                                    const_label=custom_labels,\n                                                    dtype=cfg.a_dtype,\n                                                    )\n            self._constellation = c.constellation\n            # The seed is initialized when calling the generate_analogy_channel\n            # function to ensure the same random samples are used by the\n            # create_custom_lut function to create the corresponding look-up\n            # table (LUT).\n            self._lut_seed = c.lut_seed\n        else:\n            # Existing num_bits_per_symbol must match requested one.\n            assert (self._num_bits_per_symbol == num_bits_per_symbol), \\\n                \"num_bits_per_symbol does not match the one of the existing \"\\\n                \"constellation.\"\n            # Return the existing constellation.\n            return self._constellation\n```\n\nIn the context of this method, the `Constellation` class is also shown to have a property `num_symbols`, which represents the number of constellation symbols, and a method `show` to plot the constellation. These are not directly related to the `create_or_check_constellation` method, but they are important for understanding and using the `Constellation` class effectively."
"The `scat_keep_prob` argument in the ray tracing process is a reference to a relative diffraction coefficient, which is used for scattered field tracing. This parameter defines the probability that a given diffraction coefficient will be selected when diffracted into a shadowed region. \n\nWhen an edge is hit by a ray at a diffraction point, the edge can give rise to a scattered field, which is based on the incident and diffracted modes. Each diffraction point can lead to multiple scattered points, depending on the number of edges it connects. \n\nThe `scat_keep_prob` is a number between 0 and 1. When it is set to 1, each such coefficient is retained with its original value. However, when it is lesser than 1, the diffracted field is scaled such that the total energy remains constant. This is useful to emulate fast diffractions that would otherwise vanish. \n\nHere is an example of how `scat_keep_prob` is used in the `DiffractionPattern` class:\n\n```python\ndef diffraction_pattern(self,\n                        scattered_x,\n                        scattered_y,\n                        scattered_z,\n                        reflection_x,\n                        reflection_y,\n                        reflection_z,\n                        k_final,\n                        dtype=tf.complex64):\n    # Various quantities defined hereafter are based on the paper\n    # \"Uniform Theory Array Group Analysis in Complex Environments\" by\n    # M. Di Renzo et al., IEEE Trans. on Antenna Propag., vol. 56, pp.\n    # 384-396, Feb. 2008.\n    alpha_1 = tfp.math.xlogy(1.0, self._diffraction_coefficient)\n    alpha_2 = tfp.math.xlogy(1.0, scat_keep_prob)\n    alpha_3 = tfp.math.xlogy(1.0, 1 - scat_keep_prob)\n    exp_j_2pi_k_d_sigma_hat_scattered =\n        tf.exp(tf.complex(0.0, tfp.math.xlogy(1.0, self.d)))\n    exp_j_2pi_k_d_sigma_hat_reflected =\n        tf.exp(tf.complex(0.0, tfp.math.xlogy(1.0, self.d)))\n    exp_j_2pi_k_bounding_box_min_corner =\n        tf.cast(tf.exp(tf.complex(0.0, tfp.math.xlogy(1.0, self.bounding_box_min_corner))), dtype=dtype)\n    exp_j_2pi_k_bounding_box_max_corner =\n        tf.cast(tf.exp(tf.complex(0.0, tfp.math.xlogy(1.0, self.bounding_box_max_corner))), dtype=dtype)\n    zero = tf.complex(0.0, 0.0)\n    one = tf.complex(1.0, 0.0)\n    two = tf.complex(2.0, 0.0)\n\n    # Determine the number of scattered rays at (scattered_x, scattered_y)\n    # The number of scattered rays depends on the diffraction coefficient.\n    # When the diffraction coefficient equals `scat_keep_prob`,\n    # only one scattered ray is considered (scat_keep_prob1 = 1).\n    # Otherwise, the diffraction pattern from a canonical diffraction\n    # coefficient is applied and the number of scattered rays is\n    # scat_keep_prob*diffraction_coefficient.\n    n_rays = tf.where(self._diffraction_coefficient == scat_keep_prob,\n                        1,\n                        scat_keep_prob)\n```"
"Here is the detailed information of sionna.rt.r_hat: [sionna.rt.r_hat(based_v, points, dtype=tf.float32)](source=https://3dais.github.io/sionna/_modules/sionna/rt/utils.html#r_hat)\n  \nComputes the unit norm vector of the plan containing the last dimension of points by casting the vector based_v to the same dimension as points.\n\nInput\n\n    based_v (Tensor) \u2013 Tensor of arbitrary shape containing at least rank one. Its last dimension is expected to be of length 3 and defines the direction of the plan.\n\n    points (Tensor) \u2013 Tensor of arbitrary shape containing at least rank two. The length of the last dimension is expected to be of length 3 and defines the position of the points.\n\n    dtype (tf.DType) \u2013 The dtype of the output. Defaults to tf.float32.\n\nOutput\n\n    r_hat (Tensor) \u2013 A tensor of the same shape and dtype as points containing the coordinates of the normed vector defining the plan.\n\n**Note: **  \nFor better numerical stability, the function applies abs to based_v and points before computing the sum along the last dimension of points. However, this does not change the result as both vectors of the basis of the tangent space are automatically transformed to unit norm vectors during the computation.\n\nsource code:\n```python\ndef r_hat(based_v, points, dtype=tf.float32):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    Computes the unit norm vector of the plan containing the last dimension\n    of ``points`` by casting the vector ``based_v`` to the same dimension as\n    ``points``.\n\n    Input\n    -----\n    based_v : Tensor\n        Tensor of arbitrary shape containing at least rank one. Its last\n        dimension is expected to be of length :code:`3` and defines the\n        direction of the plan.\n\n    points : Tensor\n        Tensor of arbitrary shape containing at least rank two. The length of\n        the last dimension is expected to be of length :code:`3` and defines\n        the position of the points.\n\n    dtype : tf.DType\n        The dtype of the output. Defaults to `tf.float32`.\n\n    Output\n    ------\n    r_hat : Tensor\n        A tensor of the same shape and dtype as ``points`` containing the\n        coordinates of the normed vector defining the plan.\n    \"\"\"\n\n    # For better numerical stability, we apply abs to based_v and points\n    # before computing the sum along the last dimension of points. However,\n    # this does not change the result as both vectors of the basis of the\n    # tangent space are automatically transformed to unit norm vectors during\n    # the computation.\n    return tf.reduce_sum(points*tf.math.abs(based_v), axis=-1, keepdims=True)*based_v/tf.norm(based_v, axis=-1, keepdims=True, keepdims=True)\n```"
"Here is the detailed information of OFDMDetectorWithPrior:   \n  \n[sionna.ofdm.OFDMDetectorWithPrior(detector, output, resource_grid, stream_management, constellation_type, num_bits_per_symbol, constellation, dtype=tf.complex64, **kwargs)](https://json-ld.org/drafts/layered-apis.html#class-definition)\n\nLayer that wraps a MIMO detector and a demapper for use with the OFDM waveform, given prior knowledge of the data or the resource grid.\n\nThe parameter detector is a callable (e.g., a function) that implements a MIMO detection algorithm for arbitrary batch dimensions.\n\nYou can use this layer to wrap one of the existing MIMO detectors, such as LinearMIMODetector, with an OFMDDetectorWithPrior so that it is applied taking into account prior knowledge of the transmitted data or the resource grid. This can be used to implement Expectation Propagation (EP) detection.\n\n**Parameters**\n\n- `detector` (Callable): The MIMO detection algorithm to be executed. This is a callable (e.g., a function) that implements a MIMO detection algorithm for arbitrary batch dimensions.\n- `output` (str): Type of output, either \"bit\" for LLRs on bits or \"symbol\" for logits on constellation symbols.\n- `resource_grid` (ResourceGrid): Instance of ResourceGrid.\n- `stream_management` (StreamManagement): Instance of StreamManagement.\n- `constellation_type` (str): Type of constellation, options are \"qam\", \"pam\", or \"custom\". For \"custom\", an instance of Constellation must be provided.\n- `num_bits_per_symbol` (int): Number of bits per constellation symbol, e.g., 4 for QAM16. Required only for \"qam\" or \"pam\" constellation types.\n- `constellation` (Constellation): Instance of Constellation or None. If None, both `constellation_type` and `num_bits_per_symbol` must be specified.\n- `dtype` (tf.DType): Data type of `y`. Options are tf.complex64 or tf.complex128. Defaults to tf.complex64. The output data type is the corresponding real type (tf.float32 or tf.float64).\n\n**Input**\n\n- `(y, h_hat, prior, err_var, no)` \u2013 Tuple:\n  - `y` ([batch_size, num_rx, num_rx_ant, num_ofdm_symbols, fft_size], tf.complex): Received OFDM resource grid after cyclic prefix removal and FFT.\n  - `h_hat` ([batch_size, num_rx, num_rx_ant, num_tx, num_streams_per_tx, num_ofdm_symbols, num_effective_subcarriers], tf.complex): Channel estimates for all streams from all transmitters.\n  - `prior` ([batch_size, num_tx, num_streams, num_data_symbols x num_bits_per_symbol] or [batch_size, num_tx, num_streams, num_data_points, num_interleaved_bits], tf.float): Prior of the transmitted signals. If `output` equals \"bit\", LLRs for the transmitted bits are expected; if \"symbol\", logits for the transmitted constellation points are expected.\n  - `err_var` ([Broadcastable to shape of h_hat], tf.float): Variance of the channel estimation error.\n  - `no` ([batch_size, num_rx, num_rx_ant] or only the first n dimensions, tf.float): Variance of the AWGN.\n\n**Output**\n\nDepending on the `output` parameter:\n- If `output` equals \"bit\":\n  - `[batch_size, num_tx, num_streams, num_data_symbols * num_bits_per_symbol]`, tf.float: LLRs for the transmitted bits.\n- If `output` equals \"symbol\":\n  - `[batch_size, num_tx, num_streams, num_data_symbols, num_points]`, tf.float or `[batch_size, num_tx, num_streams, num_data_points, num_interleaved_bits]`, tf.float: Logits for the Incoded constellation points. Because the constellation may be interleaved, the second option is applied when `output` is \"symbol\" and the constellation is 'interleaved'."
"Here is the detailed information of GaussianPriorSource:   \n  \n[sionna.ofdm.GaussianPriorSource(output, num_bits_per_symbol, resource_grid, stream_management, dtype=tf.complex64, **kwargs)](https://json-ld.org/distributed-carrier-network/)  \n  \nGenerate a sparse tensor of Gaussian prior source signals.\n\nThis layer generates a sparse tensor of Gaussian distributed prior source signals. The dimension of the sparse tensor for a batch of signals is obtained by folding resource_grid and stream_management, resulting in num_tx * num_streams_per_tx time-frequency resource elements. The underlying dense tensor has shape [batch_size, num_tx, num_streams_per_tx, num_ofdm_symbols,fft_size], and the sparse tensor dimensions depend on how the signals are distributed over the resource grid, on the number of active OFDM symbols num_ofdm_symbols, and the fft_size.\n\nThe Gaussian prior source signals are generated through tf.random.normal() and then oversampled using sionna.signal.OverSampler. The layer also assumes that the resource_grid\u2019s resource_grid.precoder is valid, such that the same prior sources can be reused for the precoder and detector.\n\n**Parameters**\n\n- `output` (str): One of \u201cLLR\u201d or \u201cBITS\u201d. The type of the output, either log-likelihood ratios or hard-decided bits.\n- `num_bits_per_symbol` (int): The number of bits per constellation symbol, e.g., 4 for QAM16.\n- `resource_grid` (ResourceGrid): An instance of ResourceGrid.\n- `stream_management` (StreamManagement): An instance of StreamManagement.\n- `dtype` (tf.DType): The dtype for the output. Defaults to tf.complex64.\n\n**Input**\n\n- `shape` (1D tensor array): The desired shape of the signals, starting at the batch_dim. The first dimension is the batch dimension.\n\n**Output**\n\n- `signals` (tf.float): The sparse tensor of Gaussian PRIOR source signals of shape [batch_size, num_tx, num_streams_per_tx, num_ofdm_symbols, fft_size, oversampling].\n\n-  `ind_freq` (tf.int32): The resource grid indicies (returned only if `output` is \u201cBITS\u201d) of shape [batch_size, num_tx, num_streams_per_tx, num_data_symbols].\n\n**Raises**\n\n- `AssertionError`: If `output` is neither \"LLR\" nor \"BITS\".\n- `AssertionError`: If `resource_grid.precoder` is not None.\n- `ValueError`: If `dtype` is not a complex datatype.\n\n**Source code**\n```python\nclass GaussianPriorSource(PriorSource):\n    # pylint: disable=line-too-long\n    r\"\"\"GaussianPriorSource(output, num_bits_per_symbol, resource_grid, stream_management, dtype=tf.complex64, **kwargs)\n\n    Generate a sparse tensor of Gaussian prior source signals.\n\n    This layer generates a sparse tensor of Gaussian distributed prior source\n    signals. The dimension of the sparse tensor for a batch of signals is\n    obtained by folding ``resource_grid`` and ``stream_management``, resulting\n    in ``num_tx * num_streams_per_tx`` time-frequency resource elements. The\n    underlying dense tensor has shape ``[batch_size, num_tx, num_streams_per_tx,\n    num_ofdm_symbols,fft_size]``, and the sparse tensor dimensions depend on\n    how the signals are distributed over the resource grid, on the number of\n    active OFDM symbols ``num_ofdm_symbols``, and the ``fft_size``.\n\n    The Gaussian prior source signals are generated through `tf.random.normal()`\n    and then oversampled using :class:`~sionna.signal.OverSampler`. The layer\n    also assumes that the ``resource_grid``'s ``resource_grid.precoder`` is\n    valid, suchjson-ld : https://json-ld.org/DistributedCarrierNetworks/proc/generated/sionna.ofdm.ResourceGrid.html#id1.22.0.1.1   that the same prior sources can be reused for the precoder and detector.\n\n    Parameters\n    ----------\n    output : One of [\"LLR\", \"BITS\"], str\n        The type of the output. Either :class:`~sionna.mimo.Equalizer`\n        or the \"soft\"-\"hard\" decision symbols are computed.\n\n    num_bits_per_symbol : int\n        The number of bits per constellation symbol, e.g., 4 for QAM16.\n\n    resource_grid : :class:`~sionna.ofdm.ResourceGrid`\n        An instance of :class:`~sionna.ofdm.ResourceGrid`.\n\n    stream_management : :class:`~sionna.mimo.StreamManagement`\n        An instance of :class:`~sionna.mimo.StreamManagement`.\n\n    dtype : One of [tf.complex64, tf.complex128] tf.DType\n        The dtype for the output. Defaults to `tf.complex64`.\n\n    Input\n    -----\n    shape : 1D tensor array, int\n        The desired shape of the signals, starting at the ``batch_dim``.\n        The first dimension is the batch dimension.\n\n    Output\n    ------\n    signals : : [batch_size, num_tx, num_streams_per_tx, num_ofdm_symbols, fft_size, oversampling], tf.complex\n        The sparse tensor of Gaussian PRIOR source signals.\n\n    ind_freq : : [batch_size, num_tx, num_streams_per_tx, num_data_symbols], tf.int32\n        The resource grid indicies (only for ``output`` equals `\"BITS\"`).\n\n    Raises\n    ------\n    AssertionError\n        If ``output`` is not `\"LLR\"` nor `\"BITS\"`.\n\n    AssertionError\n        If `resource_grid.precoder` is not None.\n\n    ValueError\n        If `dtype` is not a complex datatype.\n    \"\"\"\n    def __init__(self,\n                 output,\n                 num_bits_per_symbol,\n                 resource_grid,\n                 stream_management,\n                 dtype=tf.complex64,\n                 **kwargs):\n        super().__init__(output, resource_grid, stream_management,\n                         dtype=dtype, **kwargs)\n\n        assert output in (\"LLR\",\"BITS\"), \"Unknown output\"\n        assert resource_grid.precoder is None, \\\n            \"The resource_grid.precoder must be empty.\"\n\n        self._num_bits_per_symbol = num_bits_per_symbol\n        z = tf.random.normal([*self._batch_shape,\n                                 resource_grid.fft_size*resource_grid.num_ofdm_symbols],\n                        dtype=dtype)\n        z = tf.cast(z, output_dtype_complex)\n        oversampler = sionna.signal.OverSampler(resource_grid.fft_size,\n                                        resource_grid.num_ofdm_symbols,\n                                        one_sided=False)\n        z = tf.stack([oversampler(z[...,:i+1]) for i\n                      in range(self._num_bits_per_symbol)], axis=-1)\n        self._z = z\n\n\n    def call(self, shape):\n        \"\"\"GaussianPriorSource([], shape)\"\"\"\n        # shape is used by the base class `PriorSource`\n\n        signals = tf.tile(self._z, tf.concat([shape, [1,1,1]], axis=0))\n\n        if self._output == 'BITS':\n            # Remove nulled subcarriers and build indices\n            ind = tf.where(tf.not_equal(signals, 0.))\n            ind_sc = ind[...,:2]\n            ind_sy = tf.expand_dims(ind[...,2], -1)\n            self._ind_sy = ind_sy # Save it to avoid recompute\n\n            # Pad ind_sy to have the same shape as signals\n            pad = tf.zeros(tf.concat([tf.shape(ind_sy)[:-1], [signals.shape[-1]]], axis=0))\n            ind_sy = tf.concat([ind_sy, pad], axis=-1)\n            ind = tf.concat([ind_sc, ind_sy], axis=-1)\n\n            signals = tf.SparseTensor(ind, tf.squeeze(signals, -1), \\\n                                        tf.shape(signals, out_type=tf.int64))\n\n            bits = split_dim(tf.range(tf.reduce_prod(self._stream_management.data_shape)),\n                        self._num_bits_per_symbol, self._stream_management.n_rep)\n            bits = tf.cast(bits, self._dtype)\n\n            # We add ones to make sure that bits can be broadcastable to\n            # the shape of signals.\n            bits = tf.expand_dims(bits, axis=0)\n            bits = tf.tile(bits, tf.concat([[1,1], tf.shape(signals)[2:]],axis=0))\n            bits = flatten_last_dims(bits)\n\n            # Assign indices using gather\n            sy = tf.gather(bits, self._ind_sy, axis=-1)\n            sy = tf.expand_dims(sy, axis=1) # Insert strouma as first dimension\n            sy = tf.tile(sy, [1, signals.shape[1], 1, 1])\n            signals = tf.sparse.reorder(signals, tf.squeeze(sy, axis=-1))\n\n        return signals\n```"
"Here is the detailed information of [sionna.mapping.QAM2PAM](https://nvlabs.github.io/sionna/_modules/sionna/mapping.html#QAM2PAM):  \n  \nTransforms QAM symbol indices to PAM symbol indices.\n\nFor a given QAM symbol index $q$ the corresponding two PAM symbol indices $p_1$ and $p_2$ are computed. Note that this calculation is independent of the Gray labeling, i.e., PAM symbol indices are not Gray encoded.\n\nIf $q$ is smaller than $M/2$, the two closest PAM symbol indices are returned. If $q$ is larger or equal to $M/2$, the two furthest PAM symbol indices are returned.  \n  \n        qam2pam = QAM2PAM()\n        pam_indices = qam2pam(qam_indices)\n  \n**Input**\n\n- `qam_indices` (tensor, tf.int): A tensor of QAM symbol indices.\n\n**Output**\n\n- `pam_indices` (tensor, tf.int): A tensor of PAM symbol indices.  \n  \n  "
"To load the required Sionna components, you need to follow these steps:\n\n1. Import the `sionna` package. If it's not already installed, you can install it using pip.\n\n```python\ntry:\n    import sionna\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna\n```\n\n2. Load the required Sionna components. In this case, you need to load the `PlanarArray`, `Transmitter`, `DolphCohenPrecoding`, `SpatialDemapper`, `DolphCohenDetector`, `AntennaArray`, and specify the number of receive antennas `num_rx_ant`.\n\n```python\nfrom sionna.antenna import PlanarArray, Antenna\nfrom sjsonna.channel import OTAChannel\nfrom sionna.tx import Transmitter\nfrom sionna.utils import load_example, cir_to_ofdm_channel, subcarrier_frequencies\nfrom sionna.ofdm import ResourceGrid\nfrom sionna.mimo import StreamManagement\nfrom sionna.mapping import Mapper, Demapper, Constellation\nfrom sionna.fec.ldpc import LDPC5GEncoder, LDPC5GDecoder\n```\n\n3. Configure the GPU settings if necessary.\n\n```python\nimport os\ngpu_num = 0\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n```\n\n4. Import TensorFlow and other necessary packages.\n\n```python\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n```"
"To import the necessary modules for Sionna's API and set up the GPU configuration, you can follow these steps:\n\n1. Import the `sionna` module. If it's not already installed, you can install it using pip.\n\n```python\ntry:\n    import sionna\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna\n```\n\n2. Import the necessary modules from TensorFlow. You can also configure TensorFlow to use only a single GPU and allocate only as much memory as needed.\n\n```python\nimport tensorflow as tf\n# Set the TensorFlow log level to '3' to avoid warnings\ntf.get_logger().setLevel('ERROR')\n\n# Check for GPUs and allocate only as much memory as needed\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\n# Configure TensorFlow to use only a single GPU and allocate only as much memory as needed\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.set_visible_devices(gpus[0], 'GPU')\n        tf.config.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\n```\n\n3. Import the `sar` and `transmitter` modules from Sionna. Also, import the necessary layers from Keras.\n\n```python\nfrom sionna.rt import load_scene, PlanarArray, Transmitter, Receiver, Camera\nfrom sjson import Scene\nfrom sionna.rt.utils import r_hat\nfrom sionna.ofdm import ResourceGrid\nfrom sionna.ofdm import LSChannelEstimator, LMMSEInterpolator, KBestInterpolator, EPAPER\nfrom sionna.ofdm.utils import bilinear_interpolation, gen_rank_one\nfrom sionna.mimo import StreamManagement\nfrom sionna.mimo.ofdm import OLSFDetector, ZFDecoder, KBestDetector, EPDetector\nfrom sionna.mapping import Mapper, Demapper, Constellation\nfrom sionna.fec.ldpc.encoding import LDPC5GEncoder\nfrom sionna.fec.ldpc.decoding import LDPC5GDecoder\nfrom sionna.fec.scrambling import Scrambler, Descrambler\nfrom sionna.fec.crc import CRC16, CRC24, CRC11\nfrom sionna.utils import BinarySource, ebnodb2no, sim_ber\n```\n\n4. Set the TensorFlow log level to 'ERROR' to avoid warnings.\n\n```python\ntf.get_logger().setLevel('ERROR')\n```\n\nPlease note that the code snippets provided are directly taken from the context and should be used as is. If you encounter any issues, please verify the code with the original context."
"The 'CoverageMap' component of ray tracing in Sionna is used to represent the coverage map, which associates each point on a plane with the received power from a specific transmitter. This model assumes that all transmitters and receivers have the same orientation and that the transmit pre-orientation equals the receive orientation. \n\nThe 'CoverageMap' class in Sionna has methods for setup and for computing the coverage map. It utilizes multiple other Sionna components including 'Scene', 'Transmitter', 'TransmitterArray', 'Receiver', 'ReceiverArray', 'Camera' and 'CarrierFrequency'.\n\nA full example of how to use the coverage map is provided in the Sionna RT tutorial. In the provided context, the coverage map is computed for an area of interest using the 'coverage_map()' method and then loaded and visualized using the 'render()' method.\n\nHere is a simplified example of how to use the coverage map based on the context:\n\n```python\n# Import the necessary modules and classes\nfrom sionna.rt import load_scene, PlanarArray, Camera\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load the scene\nscene = load_scene(sionna.rt.scene.munich)\n\n# Configure the transmitter and receiver arrays\nscene.tx_array = PlanarArray(1,1,0.5,0.5,\"iso\",\"V\")\nscene.rx_array = PlanarArray(1,1,0.5,0.5,\"dipole\",\"auto\")\n\n# Add a transmitter and make it visible\nscene.add(Transmitter(\"tx\", (0,0,0.5), 3, \"GNSS_L1\"))\nscene.tx_array.show(\"r+\")\n\n# Set the carrier frequency\nscene.tx_array.tx_array[0].carrier = sionna.constants.CarrierFrequency(\"GNSS_L1\")\n\n# Add a receiver and compute the coverage map\nscene.add(Receiver(\"rx\", (-50, 0, 1.5), 1, 1, \"perfect\"))\n\ncm = scene.coverage_map(cm_cell_size=(1,1), num_samples=10e6)\n\n# Load a camera and visualize the scene\ncam = Camera(\"cam\", position=(-50,0,1.5), look_at=(10,0,0))\nscene.add(cam)\ncam.look_at = (0,0,0)\n\nscene.render(cam, resolution=(500, 300))\n\n# Visualize the coverage map\ncm.render(cam, tx=0)\n```\n\nThe output is a rendered image of the scene with the transmitter and receiver, and the coverage map overlayed."
"To decode a set of log-likelihood ratio (LLR) values using the TurboDecoder in Sionna, the input requirements and the output format for both hard and soft decisions should be observed.\n\n### Input Requirements for Decoding in TurboDecoder\n\n1. `llr_ch`: This input is a 3+D tensor containing the LLR values of the incoming signals from the channel, where `llr_ch` might denote the LLR signal after the demapping stage of the channel decoder.\n\n2. `ebnodb`: This input is a scalar or a tensor that can be broadcast to the shape of the first dimensions of `llr_ch`. It represents the energy per bit to noise power spectral density ratio (EBno) in dB.\n\n3. `num_iter`: This is an integer for configuring the number of iterations the Turbo decoding process will run.\n\n### Output Format for Decoding in TurboDecoder\n\n**Soft Output:**\n- `x_hat`: This output is a 3+D tensor that contains soft-decided estimates of all information bits.\n- `no_est`: This output is a tensor that contains the estimated noise variance for each bit soft-output `x_hat`.\n- Depending on the implementation, the output can also include intermediate variables like the extrinsic information `eta` of the constituent BCJR decoders.\n\n**Hard Output:**\n- For Turbo Codes with the InP interleaver: It returns a tensor of hard-decided estimates of all information bits. The output tensor's shape is the same as the input LLR tensor, except the last dimension's size changes according to the number of information bits per codeword.\n- For Turbo Codes with the S-PCA or List-based S-PCAs interleaver: It returns a tensor of hard-decided estimates of all information bits. The output tensor's shape is similar to the LLR tensor, except the last dimension's size is decreased by 1.\n\n### Using the Decoder for Hard Decisions\n\nTo perform decoding and obtain hard decisions using the TurboDecoder, you need to follow these steps:\n\n1. **Set up the Decoder**: You instantiate a `TurboDecoder` object from Sionna's API library, providing it with the necessary parameters including generator polynomials for the LDPC codes and the number of iterations.\n\n2. **Decode the LLR Values**: You pass the LLR values and other parameters to the decoder object. The LLR values `llr_ch` should be calculated from the channel output using a demapping method. The other parameters include the EBno and the number of iterations.\n\n3. **Obtain the Hard Decisions**: After decoding, you can obtain the hard decisions from the decoder's output. Depending on the scenario, you will get either the estimates or the state estimates of the information bits. Note that for Turbo Codes with the S-PCA or List-based S-PCAs interleaver, you need to use the `x_hat` and `no_est` tensors to get the hard decisions.\n\n4. **Understand the Output**: The output from decoding contains the estimates of the transmitted codeword bits and, optionally, other related information. The exact output will depend on the specific implementation and the parameters used.\n\nPlease note that the above steps are a general guide and the actual implementation may vary based on the specific requirements and the Turbo code flavor used. It's also crucial to ensure that the generator polynomials are correct and align with the Turbo code specification."
"Here is the detailed information of sionna.rt.antenna.iso_pattern:  [sionna.rt.antenna.iso_pattern(theta, phi, slant_angles=(0.0, 0.0), polarization='VH', antenna_pattern='isotropic', dtype=tf.complex64)](https://njson.github.io/sionna/_modules/sionna/rt/antenna.html#iso_pattern)\n\nIsotropic radiation pattern with linear polarization\n\nGiven arbitrary tensors $\\theta$ and $\\varphi$ corresponding to zenith and azimuth angles, this function computes the isotropic radiation pattern with linear polarization following [3GPP TR 38.901, \u201cStudy on channel model for frequencies from 0.5 to 100 GHz\u201d, Release 16.1] specification.\n\n**Input**\n\n- `theta` ([(...), 1], `tf.float`): Zenith angles.\n- `phi` ([(...), 1], `tf.float`): Azimuth angles.\n- `slant_angles` (Pair of `tf.float` or `None` and `tf.float`, default [0.0, 0.0]): Slant angle. The first element corresponds to the vertical polarization direction and the second element to the horizontal polarization direction. Set to 0 for purely vertical polarization. Set to $\\pi/2$ for purely horizontal polarization.\n- `polarization` (One of [\"V\", \"H\", \"VH\", \"HV\"], `str`): Polarization. For \u201cV\u201d (\u201cH\u201d), slant angle is 1 (0) respectively. For \u201cVH\u201d and \u201cHV\u201d, slant angle is such that the vertical direction has a cosinus value of `sqrt(3)/2`.\n- `antenna_pattern` (One of [\"isotropic\", \"dipole\", \"hw_dipole\"], `str`): Antenna pattern. Can be \"isotropic\", \"dipole\", or \"hw_dipole\".\n- `dtype` (`tf.complex64`, `tf.complex128`, `tf.DType`): Datatype used for all computations. Defaults to `tf.complex64`.\n\n**Output**\n\n- `[(...), 1], tf.complex`: Radiation pattern.\n\nsource code:\n```python\ndef iso_pattern(theta, phi, slant_angles=(0.0, 0.0),\n                polarization=\"VH\", antenna_pattern=\"isotropic\",\n                dtype=tf.complex64):\n    r\"\"\"\n    Isotropic radiation pattern with linear polarization\n\n    Given arbitrary tensors :math:`\\theta` and :math:`\\varphi` corresponding\n    to zenith and azimuth angles, this function computes the\n    isotropic radiation pattern with linear polarization following [TR38901]_.\n\n    Input\n    -----\n    theta : [...,1], tf.float\n        Zenith angles\n\n    phi : [...,1], tf.float\n        Azimuth angles\n\n    slant_angles : Pair of tf.float or None\n        Slant angle. The first element corresponds to the vertical\n        polarization direction and the second element to the horizontal\n        polarization direction. Set to 0 for purely vertical polarization.\n        Set to :math:`\\pi/2` for purely horizontal polarization.\n\n    polarization : One of [\"V\", \"H\", \"VH\", \"HV\"], str\n        Polarization. For \"V\" (\"H\"), slant angle is 1 (0) respectively.\n        For \"VH\" and \"HV\", slant angle is such that the vertical\n        direction has a cosinus value of :math:`\\sqrt{3}/2`.\n\n    antenna_pattern : One of [\"isotropic\", \"dipole\", \"hw_dipole\"], str\n        Antenna pattern. Can be \"isotropic\", \"dipole\", or \"hw_dipole\".\n\n    dtype : tf.complex64, tf.complex128, tf.DType\n        Datatype.\n        Defaults to `tf.complex64`.\n\n    Output\n    ------\n    : [...,1], tf.complex\n        Radiation pattern\n    \"\"\"\n\n    # Allow slant angles to be specified as a vector\n    if not isinstance(slant_angles, (list, tuple)):\n        if slant_angles.shape[-1] != 2:\n            raise ValueError(\"Slant angles must be specified with two angles\")\n        slant_angles = tf.cast(slant_angles, dtype)\n    else:\n        if len(slant_angles) > 2:\n            raise ValueError(\"slant_angles is not well defined\")\n        slant_angles = tf.cast(tf.stack(slant_angles), dtype=dtype)\n    theta_v = slant_angles[:, 0]\n    theta_h = slant_angles[:, 1]\n\n    # Add support of \"V\" and \"H\" not \"VH\" and \"HV\"\n    if len(polarization)==1 and polarization in (\"V\", \"H\"):\n        polarization = polarization+\"V\"\n    if not (len(polarization)==2 and polarization[1] in (\"V\", \"H\")):\n        raise ValueError(\"Invalid polarization\")\n    polarization_type = polarization[0]\n    if polarization_type==\"V\":\n        slant_angle = theta\n    elif polarization_type==\"H\":\n        slant_angle = tf.math.pi/tf.constant(2, dtype=dtype)\n    else:\n        slant_angle = tf.where(polarization==\"VH\",\n                            tf.asin(tf.sqrt(tf.constant(3.0)/tf.constant(2.0))),\n                            tf.constant(0.0, dtype=dtype))\n    slant_angle = tf.broadcast_to(slant_angle, theta.shape)\n\n    # Compute angles from (theta, phi) based on slant_angle\n    a = tf.complex(tf.cos(slant_angle), tf.constant(0.0, dtype))\n    b = tf.complex(tf.cos(theta), tf.constant(0.0, dtype))\n    c = tf.complex(tf.cos(phi), tf.sin(phi))\n    angles = tf.acos(tf.abs(tf.math.real(a*b + c)))\n\n    # Compute radiation pattern\n    if antenna_pattern==\"isotropic\":\n        result = tf.where(angles<tf.constant(0.5, dtype),\n                            tf.constant(1.0, dtype),\n                            tf.constant(0.0, dtype))\n    elif antenna_pattern==\"dipole\":\n        result = tf.where(angles<tf.constant(0.5, dtype),\n                            tf.complex(tf.sin(angles), tf.constant(0.0, dtype)),\n                            tf.constant(0.0, dtype))\n    elif antenna_pattern==\"hw_dipole\":\n        result = tf.where(angles<tf.constant(0.5, dtype),\n                            tf.complex(tf.sin(angles)*tf.sin(angles),\n                                        tf.constant(0.0, dtype)),\n                            tf.constant(0.0, dtype))\n\n    return result\n```"
"Since there is no code snippet provided to directly simulate the usage of the `ApplyTimeChannel` and `ApplyOFDMChannel` Keras layers, I will provide a general example of how you might use these layers based on the information provided in the context.\n\nFirst, you will need to import the necessary modules and set up the environment.\n\n```python\nimport os\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sionna.channel import OFDMChannel, TimeChannel, GenerateOFDMChannel, ApplyOFDMChannel, ApplyTimeChannel\nfrom sionna.channel.utils import adjoint, compute_ser\nfrom sionna.ofdm import ResourceGrid, LSChannelEstimator, LMMSEInterpolator, NoisyLMMSEInterpolator\nfrom sionna.channel import RayleighBlockFading\nfrom sionna.mimo import StreamManagement\nfrom sionna.mapping import Mapper, Demapper\nfrom sionna.modulation import QAM\nfrom sionna.fec.ldpc.encoding import LDPC5GEncoder\nfrom sionna.fec.ldpc.decoding import LDPC5GDecoder\n```\n\nThen, you need to set up the OFDM and channel layers. \n\n```python\n# Number of OFDM symbols\nnum_symbols = 14\n# FFT size\nfft_size = 256\n# Subcarrier spacing\nf = 15/64/2.**6\n# Sampling frequency\nfs = 1.92e6\n# Carrier frequency\nfc = 3.5e9\n# Antenna arrays\n...\n# Channel model\nchannel_model = RayleighBlockFading(num_rx = 1,\n                                   num_rx_ant = 128,\n                                   num_tx = 4,\n                                   num_tx_ant = 2)\n# Stream management\n...\n# Resource grid\nresource_grid = ResourceGrid(num_ofdm_symbols=num_symbols,\n                           fft_size=fft_size,\n                           subcarrier_spacing=f)\n```\n\nNext, you need to set up the channel sampler and the channel estimator.\n\n```python\n# Channel sampler\nchannel_sampler = GenerateOFDMChannel(channel_model = channel_model,\n                                      resource_grid = resource_grid,\n                                      stream_management = stream_management)\n# Channel estimator\nchannel_estimator = LSChannelEstimator(ls_type = \"fri\",\n                                      interpolation_type = \"nn\",\n                                      num_pilot_symbols = 8,\n                                      num_iterations = 2)\n```\n\nNow, you can generate a batch of channel responses and apply the channel.\n\n```python\n# Generate batch of channel responses\nh = channel_sampler(batch_size)\n# Estimate the channel\nh_hat, err_var = channel_estimator(h)\n# Apply the channel\nchannel = ApplyOFDMChannel()\nno = tf.complex(norm_dist([], tf.reduce_prod(h_hat.shape[:-1]), 1.), 0.)\ny = channel([h, x, no])\n```\n\nThis will generate an output `y` which consists of the received signals after going through the channel. \n\nPlease note that the actual usage may vary depending on the specific requirements of your application."
"To compute the log-likelihood ratios (LLRs) with the Demapper class in the Sionna package, you need to follow these steps:\n\n1. First, you need to import the Demapper class from the sionna.mapping module.\n\n```python\nfrom sionna.mapping import Demapper\n```\n\n2. You need to create an instance of the Demapper class. The Demapper class is used to compute the LLRs for the transmitted bits from the received samples.\n\n```python\ndemapper = Demapper(demapping_method, constellation_type=constellation_type, num_bits_per_symbol=num_bits_per_symbol, dtype=dtype)\n```\nIn this code, `demapping_method` is the demapping method used, `constellation_type` is the type of constellation applied, `num_bits_per_symbol` is the number of bits per constellation symbol, and `dtype` is the data type of the received samples. \n\n3. After creating an instance of the Demapper class, you can use it to compute the LLRs. The Demapper class takes the received samples and the noise variance estimate as inputs and returns the LLRs.\n\n```python\nllr = demapper(y, no)\n```\nIn this code, `y` is the received samples and `no` is the noise variance estimate. The method `demapper()` is used as a decorator for a custom demapping function.\n\nThe LLR is expressed as $L(y_n) = L_R(y_n|X=+1) - L_R(y_n|X=0) = L_R\\left(\\frac{y_n-1}{N_0}\\bigg|X=+1\\right) - L_R\\left(\\frac{y_n}{N_0}\\bigg|X=0\\right)$\n\nWhere $y_n = \\Re\\left(X_n\\right) + j\\Im\\left(X_n\\right) + n_n$ is the received symbol, $X_n \\sim \\mathcal{N}(0, s)$ is the transmitted symbol drawn from the constellation $s \\sim \\mathcal{C}$, and $n_n \\sim \\mathcal{N}\\left(0, N_0\\right)$ is the white Gaussian noise.\n\n4. Finally, you need to use the LLRs to decode the received message and recover the transmitted bits.\n\nPlease note that the steps 1 and 2 are based on the specific demapping method, constellation type, number of bits per symbol, and data type of your application. Make sure to replace the placeholders with the appropriate values."
"To obtain the Channel Impulse Responses (CIRs) in the Sionna MIMO model, you need to initialize a few components. First, ensure you have the necessary files. This includes the generator code to build the dataset, and the executor code to carry out the simulation. \n\nOnce you have the files, follow these steps to obtain the CIRs:\n\n1. Initialize the model: Load the MIT and NYU models using the `mit_model` and `nyu_model` functions. Set the model parameters and load the model.\n\n2. Set the scenario: Choose a scenario from either the MIT or NYU model. This can be done using the `set_scenario` method on the model object.\n\n3. Generate a dataset: Create a generator object from the model object using the `DatasetGenerator` class. The generator object will build the dataset. \n\n4. Generate the dataset: Use the `generate` method of the generator object to create the dataset. \n\n5. Initialize the CIR: Load the model and scenario files. Then, set the transmit array setting in the scenario file to `planar` and initialize the CIR object using the 'cir_builder' function.\n\n6. Generate CIR: Use the 'cir_generator' function to generate the CIRs. \n\nHere is a sample code snippet to help you understand these steps:\n\n```python\n# Initialize the model\nmodel = mit_model(panel_model=\"PANEL_MODEL\", # see section 4.2\n                  panel_array=\"panel_array\", # see section 4.2\n                  rx_array=\"rx_array\", # see section 4.2\n                  carrier_frequency = CARRIER_FREQUENCY,\n                  enable_pathloss = False,\n                  enable_shadow_fading = False,\n                  always_generate_lref = False)\nmodel.load_scenario(SCENARIO)\n\n# Generate dataset\ngenerator = DatasetGenerator(model,\n                             batch_size=BATCH_SIZE,\n                             num_time_steps=CIR_LENGTH // OFDM_SYMBOL_DURATION,\n                             sampling_frequency=SUBCARRIER_SPACING,\n                             number_of_subcarriers=FFT_SIZE,\n                             cir_pattern=\"a\",\n                             t_offset=0)\nX, A, tau = generator.generate(GENERATED_CIR)\n\n# Initialize the CIR\nmodel = load_model(CIR_MODEL)\nmodel.load_scenario(CIR_SCENARIO)\n\n# Generate CIR\ncir_generator = CIRGenerator(model,\n                           batch_size=1,\n                           num_time_steps=CIR_LENGTH // OFDM_SYMBOL_DURATION,\n                           sampling_frequency=SUBCARRIER_SPACING,\n                           number_of_subcarriers=FFT_SIZE)\nh_time, h_freq, tau = cir_generator()\n```\n\nPlease ensure that you have the necessary files and packages installed to run this code."
"Here is the detailed information of List2LLRSimple:   \n  \n[sic_list.List2LLRSimple](https://nvlabs.github.io/sionna/_modules/sic_list/sic.html#List2LLRSimple)  \n\nAbstract class defining a callable to compute LLRs from a list of candidate sequences.\n\nThe following classes are implemented:  \n  \n\n- [List2LLRSimple](https://nvlabs.github.io/sionna/_modules/sic_list/sic.html#List2LLRSimple)\n- [List2LLRApp](https://nvlabs.github.io/sionna/_modules/sic_list/sic.html#List2LLRApp)\n- [List2LLRDetectApp](https://nvlabs.github.io/sionna/_modules/sic_list/sic.html#List2LLRDetectApp)\n- [List2LLRDetectAppWithSyFrozen](https://nvlabs.github.io/sionna/_modules/sic_list/sic.html#List2LLRDetectAppWithSyFrozen)\n- [List2LLRSimpleWithPrior](https://nvlabs.github.io/sionna/_modules/sic_list/sic.html#List2LLRSimpleWithPrior)  \n  \n**Input**\n\n- `wanted_channel`: Tensor of shape `[num_dt_symbols, num_candi_symbols]`, containing the desired channel reliability values.\n- `undesired_channel`: Tensor of shape `[num_dt_symbols, num_candi_symbols]`, containing the undesired channel reliability values.\n- `prior`: Tensor of shape `[num_dt_symbols, 2]` containing the a priori LLR values for each data symbol.  \n  \n**Output**\n\n- `llr`: Tensor of shape `[num_data_symbols]` containing the LLR values for all of the data symbols.  \n  \n  "
"Here is the definition of MaximumLikelihoodDetector: sion.mimo.MaximumLikelihoodDetector(output, num_streams, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, dtype=tf.complex64, **kwargs)\n  \n[source code](https://nvlabs.github.io/sion/_modules/sion/mimo/detection.html#MaximumLikelihoodDetector)  \n\nsource code:\n```python\nclass MaximumLikelihoodDetector(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"MaximumLikelihoodDetector(output, num_streams, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, dtype=tf.complex64, **kwargs)\n\n    MIMO maximum-likelihood (ML) detector.\n\n    This layer implements maximum-likelihood (ML) MIMO detection. It can\n    generate ML detections of symbols or bits with either soft- or\n    hard-decisions. Both detection of symbols or bits are supported.\n    The OFDM and/or stream configuration will be automatically inferred\n    from the input ``x`` if ``x`` is a :class:`~sion.ofdm.ResourceGrid`, otherwise\n    the corresponding parameters must be provided as constructor arguments.\n\n    **\\ Examples**\n\n    Generate ML-detected bits with hard-decisions:\n\n    >>> channel = RayleighBlockFading(channel_model=\"rayleigh\", num_rx=1, num_rx_ant=32, num_tx=4, num_streams_per_tx=4)\n    >>> channel.generate()\n    >>> s = BinarySource()\n    >>> x = s([batch_size, 4*8])  # Generate random binary data\n    >>> x_c = channel([x, no])  # Simulate channel output\n    >>> rg = ResourceGrid()  # Instantiate resource grid\n    >>> x_rg = rg.time_oftdm(x_c)  # Generate OFDM resource grid\n    >>> fft_size = rg.num_ofdm_symbols*rg.fft_size\n    >>> bits = x_rg._symbol_indices  # Get symbol indices as ML detector input\n    >>> mapper = Mapper(\"qam\", num_bits_per_symbol=4)  # Instantiate mapper\n    >>> x_b = mapper(bits)  # Map symbol indices to bit strings\n    >>> mld = MaximumLikelihoodDetector(output=\"bit\", num_streams=4, hard_out=True)  # Instantiate ML detector\n    >>> bits_hat = mld([x_b, rg])  # Generate ML-decisions\n    >>> print(bits_hat.shape)\n\n    Generate ML-detected symbols with soft-decisions:\n\n    >>> channel = RayleighBlockFading(channel_model=\"rayleigh\", num_rx=1, num_rx_ant=1, num_tx=1, num_streams_per_tx=1)\n    >>> channel.generate()\n    >>> s = QAMSource(num_bits_per_symbol=4)  # Instantiate QAM source\n    >>> x = s([batch_size, 4])  # Generate random QAM symbols\n    >>> x_c = channel([x, no])  # Simulate channel output\n    >>> rg = ResourceGrid()  # Instantiate resource grid\n    >>> x_rg = rg.ofdm_resource_grid(x_c)  # Generate OFDM resource grid\n    >>> x_rg.show()  # Visualize resource grid\n    >>> mld = MaximumLikelihoodDetector(output=\"symbol\", num_streams=1)  # Instantiate MIMO detector\n    >>> s_hat = mld([x_rg, rg])  # Generate ML-detected symbols\n    >>> err = rg.calc_num_symbol_errors(s_hat)  # Count number of detected symbol errors\n    >>> print(\"Number of symbol errors:\", err)\n\n    **Parameters**\n\n    output : One of ``(\"bit\", \"symbol\")`` str\n        The type of output, either bits or symbols. Whether soft- or\n        hard-dejsons are returned can be configured with the\n        ``hard_out`` flag.\n\n    num_streams : tf.int\n        Number of transmitted streams\n\n    constellation_type : One of ``(\"qam\", \"pam\", \"custom\")`` str\n        For \"custom\", an instance of :class:`~sion.mapping.Constellation`\n        must be provided.\n\n    num_bits_per_symbol : int\n        The number of bits per constellation symbol, e.g., 4 for QAM16.\n        Only required for ``constellation_type`` in `(\"qam\", \"pam\")`.\n\n    constellation : Constellation\n        An instance of :class:`~sion.mapping.Constellation` or `None`.\n        In the latter case, ``constellation_type``\n        and ``num_bits_per_symbol`` must be provided.\n\n    hard_out : bool\n        If `True`, the detector computes hard-decided bit values or\n        constellation point indices instead of soft-values.\n        Defaults to `False`.\n\n    dtype : One of [``tf.complex64``, ``tf.complex128``] tf.DType\n        The dtype of `y`. Defaults to ``tf.complex64``.\n        The output dtype is the corresponding real dtype (``tf.float32``\n        or ``tf.float64``).\n\n    **Input**\n\n    ``(x, h, no)`` or ``x``:\n        The input to the MIMO detector.\n\n    ``x`` ([..., num_streams] or [..., num_streams, num_symbols_per_tx], tf.complex):\n        Flattened vector of all transmitted streams.\n        If an :class:`~sion.ofdm.ResourceGrid` is provided as input, ``x``\n        is equivalent to the ``data`` property of the resource grid.\n\n    ``h`` ([..., num_streams], tf.complex or\n        [..., num_streams, num_streams], tf.complex):\n        Channel matrices of all streams.\n\n    ``no`` ([..., num_streams] or only ``b`` [], tf.float):\n        Variance of the Gaussian noise.\n\n    **Output**\n\n    One of:\n\n    ``[batch_size, num_streams * num_bits_per_symbol]``, if ``output`` equals `\"bit\"`:\n        ML-decisions for all bits of every stream.\n\n    ``[batch_size, num_streams, num_points]`` or  ``[batch_size, num_streams]```, if ``output`` equals `\"symbol\"`:\n        ML-decisions for all symbols of every stream.\n\n    Note\n    ----\n    If you want to use this layer in Graph mode with XLA, i.e., within\n    a function that is decorated with ``@tf.function(jit_compile=True)``,\n    you must set ``sion.Config.xla_compat=true``.\n    See :py:attr:`~sion.Config.xla_compat`.\n    \"\"\"\n\n    def __init__(self,\n                 output,\n                 num_streams,\n                 constellation_type=None,\n                 num_bits_per_symbol=None,\n                 constellation=None,\n                 hard_out=False,\n                 dtype=tf.complex64,\n                 **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        _is_xla = sion.Config().xla_compat\n        if output==\"bit\":\n            if hard_out is False:\n                err_msg = \"Soft-decisions for bits are not supported.\"\n            else:\n                err_msg = \"MaximumLikelihoodDetector is not required for piecewise linear detection of bits (use BERAnalyser or BLERAnalyser instead).\"\n        else:\n            if hard_out is True:\n                err_msg = \"Hard-decisions for the symbol are not supported.\"\n            else:\n                err_msg = \"MaximumLikelihoodDetector is not required for extracting metrics (use Demodulator or Decoder instead).\"\n        if dtype in [tf.complex64, tf.complex128] and not _is_xla:\n            warnings.warn(\"You are using the MaximumLikelihoodDetector in Graph mode with XLA, i.e., within a function that is decorated with `@tf.function(jit_compile=True)`.\".n\\\n                \"This is only possible if `sion.Config.xla_compat` is `true`. See `sion.Config.xla_compat`.\", RuntimeWarning)\n        if output==\"bit\":\n            if constellation_type is not None:\n                if constellation_type!=\"qam\":\n                    raise ValueError(\"Only \\\"qam\\\" is supported for num_bits_per_symbol=4.\")\n                if num_bits_per_symbol!=4:\n                    raise ValueError(\"num_bits_per_symbol must be 4 for QAM.\")\n            self._symbol_demapper = SymbolDemapper(constellation_type=constellation_type,\n                                                  num_bits_per_symbol=num_bits_per_symbol,\n                                                  constellation=constellation,\n                                                  hard_out=hard_out,\n                                                  with_prior=False,\n                                                  dtype=dtype)\n            self._rx2stream = ResourceGridRxStreamTransformer(0, 1, interpolation_order=None, interchange_streams=False)\n        else:\n            self._constellation = Constellation(\n                                constellation_type, num_bits_per_symbol, constellation)\n            self._symbol_demapper = SymbolDemapper(\"qam\",\n                                                   constellation_type=None,\n                                                  num_bits_per_symbol=None,\n                                                  constellation=constellation,\n                                                  hard_out=hard_out,\n                                                  with_prior=False,\n                                                  dtype=dtype)\n            self._remove_nulledc = RemoveNulledSubcarriers(self._constellation.anon_subcarrier)\n            self._subcarrier_sampler = SubcarrierSampler(self._constellation.num_bits_per_symbol,\n                                                        sample_ind=0,\n                                                        dtype=dtype)\n        self._num_streams = num_streams\n        self._hard_out = hard_out\n        self._symbol_or_bits = \"symbol\" if output==\"symbol\" else \"bits\"\n\n    def complain_unuseable_shape(self, x):\n        if len(x.shape)<2:\n            raise ValueError(\"The input must have at least two axes.\")\n        batch_dims = list(range(len(x.shape)-2))\n        if len(batch_dims)>1:\n            raise ValueError(\"Only single batch axis is supported.\")\n        if batch_dims[0]>0:\n            raise ValueError(\"The first axis is interpreted as the batch axis.\"\n                            \" Please add dummy dimensions if you want to decode\"\n                            \" all streams separately.\")\n        if self._symbol_or_bits==\"bits\":\n            if x.shape[-1]%self._num_streams!=0:\n                raise ValueError(\"The last dimension must be a multiple of \"\n                                \" `num_streams`.\")\n        else:\n            if x.shape[-1]%self._num_streams!=0:\n                raise ValueError(\"The last dimension must be a multiple of \"\n                                \" `num_streams`.\")\n\n    def compute_noise_covariance(self, h, no):\n        \"\"\"Computes the noise covariance matrix for MIMO detection.\n\n        This method assumes the noise to be complex Gaussian with the usual\n        correlation matrix model:\n        .. math::\n            \\mathbf{C} = \\mathbf{S} \\mathbf{S}^H\n        where :math:`\\mathbf{S}` is the diagonal matrix of the singular values\n        of the channel covariance matrix.\n\n        This leads to the noisy observation:\n        .. math::\n            \\mathbf{y} = \\mathbf{H}\\mathbf{x} + \\mathbf{n}\n        with covariance\n        .. math::\n            \\text{cov}(\\mathbf{n}) = \\mathbf{C}.\n\n        The symmetric loglikelihood ratio (LLR) for the :math:`i\\text{th}` stream and\n        th :math:`k\\text{th}` antenna is given as\n\n        .. math::\n            \\begin{align}\n                LLR(k,i) &= \\ln\\left(\n                    \\frac{\\Pr\\left(b_{k,i}=1\\lvert \\mathbf{y},\\mathbf{H}\\right)}{\\Pr\\left(b_{k,i}=0\\lvert \\mathbf{y},\\mathbf{H}\\right)}\n                    \\right)\\\\\n                    &=\\ln\\left(\n                    \\frac{\n                    \\Pr\\left(\\mathbf{n} \\lvert \\mathbf{y},\\mathbf{H}, \\mathbf{b}_{k,i}=1 \\right)\n                    }{\n                    \\Pr\\left(\\mathbf{n} \\lvert \\mathbf{y},\\mathbf{H}, \\mathbf{b}_{k,i}=0 \\right)\n                    }\\right)\\\\\n                    &=\\ln\\left( \\frac{\n                    \\Pr\\left( \\mathbf{n} \\lvert \\mathbf{y},\\mathbf{H} \\right)\n                    }{\n                    \\Pr\\left( \\mathbf{n} \\lvert \\mathbf{b}_{k,i}=0 \\mathbf{and} \\mathbf{H} \\right)\n                    } \\right)\n                    +\\ln\\left( \\frac{\n                    \\Pr\\left( \\mathbf{b}_{k,i}=1 \\lvert \\mathbf{H} \\right)\n                    }{\n                    \\Pr\\left( \\mathbf{b}_{k,i}=1 \\mathbf{and} \\mathbf{y} \\right)\n                    } \\right),\n            \\end{align}\n\n        where the last term is zero as it is assumed that the bits are uniformely\n        distributed given the channel and are therefore independent of the obersved vector.\n        The definition of the LLR has been chossen such that it is equivalent to that of logit channel.\n        This definition is different from many textbooks in communications, but is more\n        convenient for optimization.\n        \"\"\"\n        # Compute first term of LLR\n        s, r = tf.linalg.eigh(h*self._s*self._s.conj().transpose())\n        s = tf.cast(s, tf.complex64)\n            #s = tf.complex(s, tf.cast(0.0, tf.float32))\n        expn = tf.reduce_sum(tf.math.log1p(tf.divide(no, tf.abs(s)*tf.abs(s)+no)))\n        expn = tf.expand_dims(expn, -1)\n        expn = tf.broadcast_to(expn, tf.shape(s))\n        pr = tf.exp(-expn)\n\n        #pr = tf.cast(pr, tf.complex64)\n        pr = tf.complex(pr, tf.cast(0.0, tf.float32))\n        prb = tf.where( pr<1e-30, 1e-30*tf.ones_like(pr), pr)\n        llr1 = tf.math.log(prb)\n\n        #llr2 = tf.cast(0.0, tf.float32) # Second term of LLR\n        llr2 = tf.cast(0.0, tf.complex64) # Second term of LLR\n        for i in range(h.shape[1]):\n            llr2 = llr2 + log10_prb(h, s, r, no, i, prb)\n\n        llr2 = tf.squeeze(llr2, 1)\n        llr2 = tf.expand_dims(llr2, -1)\n        llr2 = tf.broadcast_to(llr2, (tf.shape(h)[0], tf.shape(h)[2]))\n\n        llr = tf.where(s<1e-30, llr1, llr2)\n        llr = tf.cast(llr, self.output_dtype)\n        return llr\n\n    def call(self, inputs):\n\n        if self._symbol_or_bits==\"bits\":\n            x, h, no = inputs\n            self.complain_unuseable_shape(x)\n            z = self._rx2stream([x,h])\n        else:\n            z, h, no = inputs\n\n        # Transform params to right shape\n        h = tf.transpose(h, perm=[0, 2, 1])\n        no = expand_to_rank(no, tf.rank(z), axis=0)\n\n        # Remove nulled subcarriers (i.e., DC and guard carriers)\n        z = self._remove_nulledc(z)\n\n        # Compute S matrix\n        s = tf.linalg.matmul(h, h, transpose_a=True)\n        eye = tf.eye(s.shape[0], dtype=s.dtype)\n        s = tf.linalg.solve(s, eye)\n\n        # Compute LLRs\n        llr = self.compute_noise_covariance(h, no)\n        llr = tf.where(tf.is_inf(llr), tf.cast(0.0, llr.dtype), llr)\n        if self._symbol_or_bits == \"bits\":\n            llr = self._symbol_demapper([z, tf.abs(llr)])\n        else:\n            llr = self._symbol_demapper([z, tf.abs(llr), tf.ones(z.shape[:2], tf.float32)*-1.0])\n\n        return llr\n```"
"Here is the detailed information:   \n  \n[sionna.channel.generate_5g_ranking(rxs, resource_grid_stream, precoding, subcarrier_spacing, num_subcarriers, l, n_tot, num_layers, dtype=tf.complex64)](https://jsonsax.lablis.fr//blob:master/sionna/channel/rayleigh.py#L161 \"sionna.channel.generate_5g_ranking\")  \n\nApply the state-of-the-art [Ranking method from 3GPP](https://www.3gpp.org/ftp/Specs/archive/38_series/38.214/38214-h60.zip) to compute for each transmitter a list of $l$ candidate receivers for a given stream $s\\in[0,..,n_\\text{tot}-1]$.\n\n**Note:**The 3D position computation in [3GPP TR 38.214, \u201cBFR for IMT. Base station and mobile vehicle models\u201d, Release 16.1](https://www.3gpp.org/ftp/Specs/archive/38_series/38.214/38214-h60.zip) is needed to obtain the correct [3GPP channel impulse responses](https://www.3gpp.org/ftp/Specs/archive/38_series/38.214/38214-h60.zip) for this function.\n\n[3GPP38901-fig10]: https://raw.githubusercontent.com/nr5g/sionna/main/examples/figures/3GPP38901-f10.png\n\n[Ranking process according to 3GPP TS 38.90]: https://github.com/nr5g/sionna/blob/main/examples/3GPP_TS_38.901.ipynjson/#\n\n**Input**\n\n- `rxs` ([..., num_rx, 4], tf.float): 3D positions of the receivers.\n- `resource_grid_stream` ([..., num_tx, num_layers, num_symbols_per_slot, subcarrier_spacing], tf.complex): Resource grid for the stream. The resource grid is a time-frequency representation of the channel signals. It indicates which part of the spectrum is occupied by each signal.\n- `precoding` ([..., num_tx, num_layers, num_symbols_per_slot, num_subcarriers], tf.complex): Precoded resource grids.\n- `subcarrier_spacing` (int): Subcarrier spacing, i.e., the spacing between the carriers in the FFT step.\n- `num_subcarriers` (int): Number of subcarriers.\n- `l` (int): Number of ranked receivers to compute.\n- `n_tot` (int): Total number of streams across all transmitters.\n- `num_layers` (int): Number of layers.\n- `dtype` (tf.DType): Datatype to use. Every dtype from t_complex is supported. Defaults to t.complex64.\n\n**Output**\n\n- `positions` ([..., num_tx, l, 3], tf.float): 3D positions of the ranked receivers.\n- `g` ([..., num_tx, l, num_rx, num_layers, num_subcarriers], tf.complex): Channel frequency responses between thejson/mt and json/mr. For more details, see [3GPP TS 38.141-070, \u201cBase Station (BS) conduction model;\u201d Release 16.1.0](https://www.3gpp.org/ftp/Specs/archive/38_series/38.141/38141-h10.zip).\n- `scores` ([..., num_tx, l], tf.float): LoS scores of the ranked receivers.\n\n**Error**\n\n- `ValueError`: If `dtype` is not supported.\n\n**Note:** \nThe optional array of RSCP values from [3GPP TR 37.355]: https://www.3gpp.org/ftp/Specs/archive/37_series/37.355/37355-h00.zip is not implemented as it is not needed for [3GPP TS 38.90]."
"Running the channel modeling in Sionna involves a few steps as follows:\n\n1. Import the necessary modules and configure the GPU settings.\n2. Load the required Sionna components, including the SSF winner models, 3GPP NR PUSCH and PDSCH, and the antenna array, transmitters, and receivers.\n3. Configure the scenario, carrier, array pattern, and motion vectors. The scenario can be chosen from a list of predefined scenarios, or it can be custom defined. The carrier object is configured with the desired carrier settings, and the array pattern is set using the URA pattern. The motion vectors determine the movement of the user terminals (UTs) and base stations (BSs).\n4. Create a simulation topology consisting of single or multiple UTs and BSs. Each UT or BS is equipped with one or multiple transmitters or receivers, depending on the number of antennas the UT or BS is equipped with.\n5. Run the channel modeling process. This is done by calling the `generate_topology()` method of the scenario object with the desired topology as an argument. The `generate_initial_state()` method is then called to generate the initial channel state. The `generate_lsf()` method is used to generate the channel frequency response, and the `upscale_channel()` method is called to upscale the channel to the correct number of subcarriers.\n6. If using a custom-defined scenario, the `Scenario` object is instantiated with the desired settings. The `build_topology()` method is then called to build the topology, and the `channel_model` property of the `Scenario` object is used to run the channel modeling process.\n\nThe exact steps may vary depending on the specific requirements of the simulation."
"Here is the detailed information of load_alist: [sionna.fec.utils.load_alist(fp, version=1)](https://nvlabs.github.io/sionna/_modules/sionna/fec/utils.html#load_alist)  \n  \nLoad an alist (i.e., a code in alist format) [D. J. C. Mackay and M. S. Post. \u201cNote on error-correcting codes for networks and crosstalk\u201d. Electronics Letters 29, no. 12 (1993): 881\u20132.] from a file.\n\nA list representation of an alist is returned.\n\n**Input**\n\n- `fp` (File): A file object containing the alist representation of a code.\n- `version` (int, default: 1): Version of the alist format. Currently, only `version 1` is supported. This is used to interpret the file format.\n\n**Output**\n\n- `code` (list): A nested list representation of the loaded alist.\n\n**Note: **  \nThis function is part of the sionna.fec.utils module which is not part of the as is available in the as is available in the sionna package. Please make sure that the sionna package and its dependencies are correctly installed before using this function.  \n  \nsource code:  \n```python\ndef load_alist(fp, version=1):\n    # pylint: disable=line-too-long\n    r\"\"\"Load an `alist` (i.e., a code in `alist` format) [MackayAlist]_ from a file.\n\n    A list representation of an `alist` is returned.\n\n    Input\n    -----\n    fp : File\n        A file object containing the alist representation of a code.\n\n    version : int, default: 1\n        Version of the alist format. Currently, only `version 1` is\n        supported.\n\n    Output\n    ------\n    code : list\n        A nested list representation of the loaded alist.\n\n    Note\n    ----\n    .. note:: `load_alist` is part of the :mod:`~sionna.fec.utils` module.\n        Make sure the module is correctly imported.\n\n    .. note:: In `version 1`, the first elements of the list (row index) are\n        omitted.\n\n    \"\"\"\n    if version!=1:\n        raise ValueError(\"Invalid version. Only 1 is supported.\")\n    # only v1 is supported\n    context = (load_dlm.__enter__(), fp)\n    code = load_dlm(context, version=1)\n    # drop first element of each row\n    code = [code[i][1:] for i in range(len(code))]\n    return code\n```  \n  "
"Here are some useful links of it:\n\n[Run in Google Colab](https://colab.research.google.com/github/nvlabs/sionna/blob/main/examples/5G_NR_PUSCH_Tutorial.ipynb)\n\n[View on GitHub](https://github.com/nvlabs/sionna/blob/main/examples/5G_NR_PUSCH_Tutorial.ipynb)\n\n[Download notebook](https://nvlabs.github.io/sionna/examples/5G_NR_PUSCH_Tutorial.ipynb)"
"The Scene class in Sionna provides the `cameras` property which contains a list of cameras that can be used to visualize a scene and render it to an image. \n\nEach camera in the list is defined by its own pose (`position` and `orientation`) and is represented by a `Camera` object. A rendered scene will include the objects of the Scene that are within the field of view of the camera.\n\nThe `cameras` property allows for the manipulation and customization of scenes for rendering and visualization purposes. You can create multiple cameras, each with its own viewpoint, and then use the appropriate one for each rendering or visualization task.\n\nThe cameras are rendered the final image with the scene from the viewpoint that they are defined. The rendered image can either be previewed within a Jupyter notebook or saved to a file on the system. \n\nThe `cameras` property and its related functionality like setting/getting cameras, targeting a specific object, etc. make Sionna a powerful tool for rendering and visualizing scenes."
"Here is the detailed information of RaisedCosineFilter:   \n  \n[sionna.signal.RaisedCosineFilter(span_in_symbols, samples_per_symbol, beta, window=None, normalize=True, trainable=False, dtype=tf.float32, **kwargs)](https://jsonnld.link/doc/sionna/signal.raised_cosine_filter.html#sionna.signal.RaisedCosineFilter)  \n\nLayer for applying a raised-cosine filter of length K to an input x of length N.\n\nThe filter length K is equal to the filter span in symbols (span_in_symbols) multiplied by the oversampling factor (samples_per_symbol). If this product is even, a value of one will be added.\n\nThe filter is applied through discrete convolution.\n\nAn optional windowing function window can be applied to the filter.\n\nThe dtype of the output is tf.float only if both x and the filter coefficients have dtype tf.float. Otherwise, the dtype of the output is tf.complex.\n\nThree padding modes are available for applying the filter:\n\n- \u201cfull\u201d (default): Returns the convolution at each point of overlap between x and the filter. The length of the output is N + K - 1. Zero-padding of the input x is performed to compute the convolution at the borders.\n- \u201csame\u201d: Returns an output of the same length as the input x. The convolution is computed such that the coefficients of the input x are centered on the coefficient of the filter with index (K-1)/2. Zero-padding of the input signal is performed to compute the convolution at the borders.\n- \u201cvalid\u201d: Returns the convolution only at points where x and the filter completely overlap. The length of the output is N - K + 1.\n\n### Parameters\n\n- `span_in_symbols` (int): Filter span as measured by the number of symbols.\n- `samples_per_symbol` (int): Number of samples per symbol, i.e., the oversampling factor.\n- `beta` (float): Roll-off factor. Must be in the range $[0,1]$.\n- `window` (Window or string [\"hann\", \"hamming\", \"blackman\"]): Instance of Window applied to the filter coefficients or a string indicating the window name. Custom windows must be provided as an instance.\n- `normalize` (bool): If True, the filter is normalized to have unit power. Defaults to True.\n- `trainable` (bool): If True, the filter coefficients are trainable variables. Defaults to False.\n- `dtype` (tf.DType): The dtype of the filter coefficients. Defaults to tf.float32.\n\n### Input\n\n- `x` ([..., N], tf.complex or tf.float): The input to which the filter is applied. The filter is applied along the last dimension.\n- `padding` (string [\"full\", \"valid\", \"same\"]): Padding mode for convolving x and the filter. Must be one of \"full\", \"valid\", or \"same\". Case insensitive. Defaults to \"full\".\n- `conjugate` (bool): If True, the complex conjugate of the filter is applied. Defaults to False.\n\n### Output\n\n- `y` ([..., M], tf.complex or tf.float): Filtered input. It is tf.float only if both x and the filter are tf.float. It is tf.complex otherwise. The length M depends on the padding.\n\n### Properties\n\n**Property: `aclr`**\n- ACLR of the filter. This corresponds to using the filter as a pulse shaping filter on an i.i.d. sequence of symbols, with the in-band assumed to range from [-0.5, 0.5] in normalized frequency.\n\n**Property: `beta`**\n- Roll-off factor.\n\n**Property: `coefficients`**\n- The filter coefficients (after normalization).\n\n**Property: `length`**\n- The filter length in samples.\n\n**Property: `normalize`**\n- True if the filter is normalized to have unit power.\n\n**Property: `sampling_times`**\n- Sampling times in multiples of the symbol duration.\n\n**Property: `trainable`**\n- True if the filter coefficients are trainable.\n\n**Property: `window`**\n- The window function that is applied to the filter coefficients. None if no window is applied.\n\n### Method: `show(response='magnitude', scale='lin')`\nPlots the impulse or magnitude response of the filter.\n- **Input**:\n  - `response` (str, one of [\"impulse\", \"magnitude\"]): Desired response type. Defaults to \u201cmagnitude\u201d.\n  - `scale` (str, one of [\"lin\", \"db\"]): Y-scale of the magnitude response. Can be \"lin\" (linear) or \"db\" (decibel). Defaults to \"lin\"."
"Here is the detailed information of sionna.signal.Upsampling:   \n  \n[sionna.signal.Upsampling(samples_per_symbol, upsampling_factor=1, num_bits=1, axis=-1, **kwargs)](https://json.blotai.com/doc/source/sionna/signal/misc.html#sionna.signal.Upsampling)  \n\nLayer for applying arbitrary upsampling with interpolation followed by a reshape.\n\nThis class inherits from the Keras layer class and can be used as layer in a Keras model.\n\nThis layer performs upsampling by inserting zeros between samples followed by low-pass filtering.\nIt is particularly useful in the context of digital communications, where the message or signal is upsampled after baseband mapping.\n\nThe class inherits from the Keras layer class and can be used as is, although some parameters may not be practical in all configurations.  \n  \nParameters\n\n- `samples_per_symbol` (int): Number of samples per symbol, i.e., the oversampling factor.\n- `upsampling_factor` (int): Upsampling factor, i.e., the number of new samples to insert between each pair of adjacent samples.\n- `num_bits` (int): Number of bits per sample for quantizing the amplitude. Defaults to 1.\n- `axis` (int): The dimension along which to insert samples. Defaults to -1.\n\nInput\n\n- `x` ([..., n], tf.float or tf.complex): The input to which upsampling is going to be applied. The upsampling is performed on the axis indicated by the axis parameter.\n\nOutput\n\n- `[..., samples_per_symbol*(upsampling_factor-1)+n], tf.float or tf.complex`: The upsampled output. The type of the output depends on the type of the input x.\n\nNote: To prevent a change in the shape of the data, the length of the input signal must be larger than  (samples_per_symbol) * (upsampling_factor). The signal is upsampled by repeating the sequence of symbols.  \n  \nsource code:  \n```python\nclass Upsampling(ExpandDims):\n    # pylint: disable=line-too-long\n    r\"\"\"Upsampling(samples_per_symbol, upsampling_factor=1, num_bits=1, axis=-1, **kwargs)\n\n    Layer for applying arbitrary upsampling with interpolation followed by a\n    reshape.\n\n    This class inherits from the Keras layer class and can be used as layer in a\n    Keras model.\n\n    This class inherits from the :class:`~sionna.signal.expand_dims` class.\n    It performs upsampling by inserting additional `upsampling_factor`\n    samples between each pair of adjacent samples and then reshaping the\n    tensor to the desired shape. For example, if `samples_per_symbol` is two and\n    `upsampling_factor` is four, the vector ``[a, b]`` will be upsampled to\n    ``[a, 0, 0, b]``.\n\n    It is particularly useful in the context of digital communications, where the\n    message or signal is upsampled after baseband mapping.\n\n    The class inherits from the Keras layer class and can be used as is, although\n    some parameters may not be practical in all configurations.\n\n    Parameters\n    ----------\n    samples_per_symbol : int\n        Number of samples per symbol, i.e., the oversampling factor.\n\n    upsampling_factor : int\n        Upsampling factor, i.e., the number of new samples to insert between each\n        pair of adjacent samples.\n\n    num_bits : int\n        Number of bits per sample for quantizing the amplitude.\n        Defaults to 1.\n\n    axis : int\n        The dimension along which to insert samples.\n        Defaults to `-1`.\n\n    Input\n    -----\n    x : [...,n], tf.float or tf.complex\n        The input to which upsampling is going to be applied.\n        The upsampling is performed on the axis indicated by the `axis`\n        parameter.\n\n    Output\n    ------\n    : [...,samples_per_symbol*(upsampling_factor-1)+n], tf.float or tf.complex\n        The upsampled output.\n\n    Note\n    ----\n    To prevent a change in the shape of the data, the length of the input\n    signal must be larger than ` (samples_per_symbol) * (upsampling_factor)`.\n    The signal is upsampled by repeating the sequence of symbols.\n    \"\"\"\n\n    def __init__(self, samples_per_symbol, upsampling_factor=1, num_bits=1, axis=-1, **kwargs):\n\n        super().__init__(axis=axis, tf_function=False, expand_last_dim=False)\n\n        self._check_parameters(samples_per_symbol, upsampling_factor, num_bits)\n\n        self._samples_per_symbol = samples_per_symbol\n        self._upsampling_factor = upsampling_factor\n        self._num_bits = num_bits\n        self._axis = axis\n\n    def _check_parameters(self, sps, usf, nb):\n        # pylint: disable=line-too-long\n        assert isinstance(sps, int), \"samples_per_symbol must be int.\"\n        assert sps>0, \"samples_per_symbol must be >0.\"\n        assert isinstance(usf, int), \"upsampling_factor must be int.\"\n        assert usf>0, \"upsampling_factor must be >0.\"\n        assert (sps-1)%(usf)==0, \"`samples_per_symbol`-`upsampling_factor` \" \\\n            \"must be a multiple of `upsampling_factor`.\"\n        assert isinstance(nb, int), \"`num_bits` must be int.\"\n        assert nb>0, \"`num_bits` must be >0.\"\n```  \n  "
"The `polarization_model_1` and `polarization_model_2` functions in the `sionna.channel.tr38901` module are used to implement the polarizarion model from the 3GPP TR 38.901 specification, which accounts for the directional properties of the radio channel.\n\nThe `polarization_model_1` function is more user-specific and allows the specification of a different pattern in each direction. It takes the following inputs:\n- `theta`: The zenith angles of the incoming directions of the rays.\n- `phi`: The azimuth angles of the incoming directions of the rays.\n- `theta_s`: The zenith angle of the stationary antenna.\n- `phi_s`: The azimuth angle of the stationary antenna.\n- `l`: The polarization decorrelation length.\n- `a_x` and `a_y` : The polarization coefficients related to the antenna patterns.\n- `pattern_x` and `pattern_y` : The antenna patterns.\n\nThe output of `polarization_model_1` is the function that computes the average of the right- and left-hand circularly polarized scattering gains.\n\nThe `polarization_model_2` function is based on model 2 in the 3GPP specification and assumes a sinc pattern for the antenna elements. It takes almost the same inputs as `polarization_model_1`, except it doesn't take the antenna patterns as inputs. Also, additional parameters like `h_s`, `h_i`, and `h_o` are required which correspond to the reflection coefficients of the ground, the incoming and the outgoing surfaces, respectively.\n\nThe output of `polarization_model_2` is the function that computes the average of the right- and left-hand circularly polarized scattering gains, based on the parameters of the model.\n\nAs for usage, these functions can be used as part of a radio channel model to simulate wireless propagation in a scene, taking into account the polarization effects. However, their precise usage and integration depends on the larger framework they are used in, which includes other functionalities like ray tracing, optimization, and propagation modeling."
"Here is the detailed information of Mapper:   \n  \n[sionna.ofdm.Mapper(mapping=\"greedy\", bit_depth=1, use_twigs=False, normalize=False, **kwargs)](https://json-ml.github.io/sionna/_modules/sionna/ofdm/mapping.html#Mapper)  \n\nMaps a 2+D tensor of bits to QAM constellation points, and symbol indices, or a 3+D tensor of bits to constellation points.\n\n**Parameters**\n\n- `mapping` (str): The mapping method. Options are \"greedy\", \"coordination\", or \"non-differential\".\n- `bit_depth` (int): The number of bits per constellation symbol, e.g., 4 for QAM16. Only required for \"greedy\" and \"coordination\".\n- `use_twigs` (bool): If enabled, uses \"twigs\" to realize the constellation. This is mainly for higher-order QAM constellations. Only required for \"greedy\" and \"coordination\".\n- `normalize` (bool): If enabled, the constellation is normalized to have unit power. Overwrites normalize_num_bits. Options are True or False.\n- `dtype` (tf.DType): The output dtype. Options are tf.complex64 or tf.complex128. Defaults to tf.complex64.\n\n**Input**\n\n- `[, n]`, or `[, n, num_bits_per_symbol]`, tf.float: A 2+D tensor of bits to be mapped to constellation symbols. The last dimension is an optional dimension for different bit per symbol.\n\n**Output**\n\n- `[,n//nu]`, tf.complex: Mapped constellation symbols.\n- `[,n//nu, num_bits_per_symbol]`, tf.float: The symbol indices corresponding to the constellation symbols.\n\n**Note:**  \nOne way to understand the mapping process is as follows: For \"greedy\" mapping, the algorithm first assigns the bit index (0,1,2, or 3) of the first two bits (00,01,10, or 11) to a 2D index position \\[(i_1,i_2)\\]. This index position determines the corresponding QAM symbol. The same process is then repeated for the remaining pairs of bits. With \"coordination\", the two zero-pairs of the bit sequence \\[0,0,1,1\\] are first assigned to the 4-tuple \\[z_1=0,z_2=0,z_3=1,z_4=1\\]. This 4-tuple also determines the constellation symbol. More details about the constellation design can be found in [A. P. Elzanowski and H. A. Surma, \u201cEfficient methods for generation of full-diversity nontwisted-array codes,\u201d IEEE Trans. Commun., vol. 54, no. 1, pp. 84-88, Jan. 2006.].\n\nWith \"non-differential\", the exact bit labels of the constellation are used. This type of mapping is relevant when the receiver has no knowledge of the sign of the channel frequency response [(W. Fu and J. S. Cui, \u201cChannel estimation for OFDM transmission using Kalman filter on a symbol layer,\u201d IEEE Trans. Wireless Commun., vol. 4, no. 5, pp. 2501-2514, Sep. 2005.)](https://json-ml.github.io/sionna/_modules/sionna/ofdm/channel_estimation.html#LS_ESTIMATION).\n\nDtype and shape of the output [(), tf.complex64/128]:  \nThe output dtype and shape is as given by the corresponding input dimension."
"The `LDPC5GEncoder` in the Sionna package is used for encoding data using 5G NR compliant LDPC codes. It has the following parameters and types for its constructor:\n\n1. `k` (type: `int`): This is the number of information bits per codeword. It must be a valid value according to the 3GPP standard, meaning it should be one of the allowed code lengths for the particular 5G code under consideration.\n\n2. `n` (type: `int`): This is the desired codeword length. Similar to `k`, it must comply with the standards and be a suitable length for 5G encoding.\n\n3. `num_bits_per_symbol` (type: `int`): This is the number of bits per QAM symbol. It must be 2, 4, or 6 and represents the modulation order.\n\n4. `out_int` (type: `int`): This parameter dictates the output interleaver type for the data interleaver and the scrambled bits interleaver. It can be an integer from 1 to 4.\n\n5. `out_int_seed` (type: `int`): This is the seed for the data interleaver and scrambled bits interleaver generation and should be an integer.\n\n6. `z` (type: `int`): This is the lifting factor for the check node update rules and should be an integer. It defaults to `None`, which is equivalent to a lifting factor of 1.\n\n7. `return_perm_seq` (type: `bool`): This dictates if the permutation sequence should be returned, and defaults to `False`.\n\n8. `pcm` (type: `bool`): This boolean parameter, when set to `True`, will perform puncturing for low-rate codes. It defaults to `False`.\n\n9. `c_r` (type: `str`): This is the code rate as a string and must be a valid 3GPP rate. Example values are '1/4', '2/3', and '3/4'.\n\n10. `n_r` (type: `int`): This is the n_r parameter for the code construction as an integer and defaults to 0.\n\n11. `num_ldpc_symbols` (type: `int`): This is the number of LDPC encoded symbols.\n\n12. `hard_out` (type: `bool`): This boolean parameter, when set to `True`, will make the encoder output hard-decided bits. Defaults to `True`.\n\n13. `cn_type` (type: `str`): This is the check node update type and must be either \"offset\" or \"minsum\". Defaults to \"minsum\".\n\nAn example of how these parameters might be used is given in the context of setting up the encoder:\n\n```python\nfrom sionna.fec.ldpc.encoding import LDPC5GEncoder\n\n# Define the required parameters\nk = 100  # Number of information bits per codeword\nn = 200  # Desired codeword length\n\n# Create an instance of the encoder with the parameters\nencoder = LDPC5GEncoder(k=k,\n                        n=n,\n                        num_bits_per_symbol=2, # Could be other values like 4 or 6\n                        out_int= 2,            # Choose type of interleaver\n                        out_int_seed= 3,        # Set seed for interleaver\n                        hard_out=True)        # Get hard-decided bits as output\n```\n\nIn this example, the encoder is instantiated with the given parameters, which must adhere to the restrictions and formats specified for each parameter. The provided context does not indicate any constraints on the values of these parameters."
"This is the detailed explanation of each function:\n\n- `complex2real_vector(z)`: This function returns a real-valued vector composed of the real and imaginary parts of a complex-valued vector. The last dimension is interpreted as the one corresponding to the imaginary part.\n\n- `real2complex_vector(z)`: Opposite to `complex2real_vector`, this function returns a complex-valued vector from a given real-valued vector. It assumes that the last dimension is associated with the imaginary part.\n\n- `complex2real_matrix(z)`: This function returns a real-valued matrix composed of the real and imaginary parts of a complex-valued matrix. The last two dimensions are interpreted as those corresponding to the real and imaginary parts, respectively.\n\n- `real2complex_matrix(z)`: Opposite to `complex2real_matrix`, this function returns a complex-valued matrix from a given real-valued matrix. It assumes that the last dimension is associated with the imaginary part.\n\n- `complex2real_covariance(r)`: This function returns a real-valued covariance matrix from a complex-valued covariance matrix. It assumes that the last two dimensions represent the real and imaginary parts, respectively.\n\n- `real2complex_covariance(q)`: Opposite to `complex2real_covariance`, this function returns a complex-valued covariance matrix from a given real-valued covariance matrix. It assumes that the last dimension is associated with the imaginary part.\n\n- `complex2real_channel(y, h, s)`: This function implements the complex-valued equivalent of a real-valued AWGN channel. It assumes that the channel outputs, channel estimates, and error variances are all given as complex-valued quantities.\n\n- `real2complex_channel(y, h, s)`: Opposite to `complex2real_channel`, this function implements a real-valued equivalent of a complex-valued AWGN channel. It assumes that the channel outputs, channel estimates, and error variances are given as real-valued quantities.\n\n\nEach of these functions comes with a counterpart that does the inverse operation. These functions are used to translate between complex-valued representations and corresponding real-valued representations. This can be important in certain types of data analysis and processing."
"Here is the detailed information of FlatFadingChannel:    \n  \n[sion.wireless.FlatFadingChannel(usc_vec, nonreg_fading=None, dtype=tf.complex64)](https://nvlabs.github.io/sion/_modules/sion/wireless/channel.html#FlatFadingChannel)  \n\nApplies random channel realizations to a vector according to some distribution and adds AWGN.\n\nThis class combines :class:`~sion.wireless.GenerateFlatFadingChannel` and :class:`~sion.wireless.ApplyFlatFadingChannel` to compute the output of a flat-fading channel, followed by AWGN.\n\n**Parameters**\n\n- `usc_vec` ([batch size, num_rx, num_rx_ant, num_tx, num_tx_ant], tf.complex): 2D tensor containing right SVD matrices of the channel. The dimension `num_tx_ant` indicates the number of antennas at the transmitter, `num_rx_ant` indicates the number of antennas at the receiver, and `num_tx_ant` + `num_rx_ant` - 1 indicates the number of retained singular values.\n- `nonreg_fading` (None | \"reyes\", \"cai\", or \"cerf\"): Method for generating the channel realizations. Only required if ``h`` is complex. Defaults to None, which indicates that :class:`~sion.wireless.GenerateFlatFadingChannel` will be used. `nonreg_fading` is ignored if ``h`` is real-valued. Possible values are \"reyes\", \"cai\", or \"cerf\".\n- `dtype` (tf.complex64 or tf.complex128): Datatype for internal calculations and the output. Defaults to tf.complex64.\n\n**Input**\n\n- `(x, no)` \u2013 Tuple:\n  - `x` ([batch size, num_tx_ant], tf.complex or tf.float): Tensor containing the input signals to the channel.\n  - `no` (Scalar or Tensor, tf.float): The noise variance of the AWGN.\n\n**Output**\n\n- `y` ([batch size, num_rx, num_rx_ant, num_tx, num_tx_ant], tf.complex): Output of the channel, corresponding to ``x``.\n\n**Property**\n\n- `generate_channel`: Instance of :class:`~sion.wireless.GenerateFlatFadingChannel`, which is used to generate channel realizations.\n- `apply_channel`: Instance of :class:`~sion.wireless.ApplyFlatFadingChannel`, which is used to apply the channel to the channel input."
"Here is the definition of PilotPattern: sionna.ofdm.PilotPattern(mask, pilots, trainable=False, normalize=False, dtype=tf.complex64)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/ofdm/pilot_pattern.html#PilotPattern)  \n\nsource code:\n```python\nclass PilotPattern(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"Defines a pilot pattern for an OFDM ResourceGrid.\n\n    This class defines a :class:`~sionna.ofdm.ResourceGrid` that sparsely\n    populated with non-zero elements. It provides a method for\n    configuring the pilot pattern based on a ``mask`` and the number of\n    overlapping PilotSymbols. The average energy of the pilots can be\n    controlled through the parameter ``pilots``. An option allows making the\n    pilots trainable.\n\n    Parameters\n    ----------\n    mask : Property\n        A callable that takes a :attr:`~sionna.ofdm.ResourceGrid` instance and\n        returns a boolean tensor of shape `[num_tx, num_streams_per_tx, num_ofdm_symbols, fft_size]`\n        indicating which resource elements are masked (ijsonable).\n\n    pilots : Property\n        A callable that takes a :class:`~sionna.ofdm.ResourceGrid` instance and\n        returns a tensor of pilots of shape `[num_tx, num_streams_per_tx, num_pilots_per_tx, num_ofdm_symbols, num_effective_subcarriers]`\n        containing the pilot symbols. The pilots can be possibly normalize\n        to ensure that they have unit average energy across the last\n        dimension, depending on the value of the ``normalize`` flag.\n\n    trainable : bool\n        Indicates if the pilots are trainable or not. Defaults to `False`.\n\n    normalize : bool\n        Indicates if the pilots are normalized to have unit average energy\n        across the last dimension. Defaults to `False`.\n\n    dtype : tf.Dtype\n        Defines the datatype for internal calculations and the output\n        dtype. Defaults to `tf.complex64`.\n    \"\"\"\n    def __init__(self,\n                 mask,\n                 pilots,\n                 trainable=False,\n                 normalize=False,\n                 dtype=tf.complex64,\n                 **kwargs):\n\n        super().__init__(dtype=dtype, **kwargs)\n\n        assert normalize in [True, False], \"normalize must be True or False.\"\n        self._normalize = normalize\n\n        assert trainable in [True, False], \"trainable must be True or False.\"\n        self._trainable = trainable\n\n        # Note: From TF 2.0, one can use functions for setting non-trainable\n        # weights. As of Tensorflow 2.3, one can use `tf.function` with the\n        # `tf.set_weights` method, but this approach seems to be\n        # problematic with respect to the random initialisation of weights.\n        # self._pilots = tf.Variable(pilots, trainable=self._trainable)\n\n        # The following implementation using get_weights() and set_weights()\n        # seems to be the most portable one.\n\n        # For non-trainable weights, we use a Zero of the shape of the pilots\n        # and do not apply the mask.\n        if self._trainable:\n            self._pilots = tf.Variable(self.get_weights(),\n                                        dtype=dtype,\n                                        trainable=self._trainable)\n        else:\n            self._pilots = tf.keras.layers.Zero().pilots\n\n        self.mask = mask\n\n        # The number of ofdm_symbols where a pilot is applied\n        self._num_pilot_ofdm_symbols = tf.shape(self._pilots)[-2]\n\n        # Number of pilots that are computed per effective subcarrier\n        num_pilots = tf.shape(self._pilots)[-1] # The last dimension\n\n        # The actual number of pilot symbols\n        # Note: Due to the masking of unused subcarriers and/or ofdm_symbols,\n        # the actual number of pilot symbols can be smaller than `num_pilots`.\n        # We select only the non-masked pilot symbols.\n        self._num_pilot_symbols = tf.reduce_sum(\n            tf.cast(tf.abs(self._pilots), tf.int32), axis=-1)\n        self._num_pilot_symbols = tf.math.reduce_sum(\n            self._num_pilot_symbols, axis=-1)\n\n        # Compute the pilot pattern indicies\n        t = tf.range(0, tf.shape(self.mask)[-2])\n        s = tf.range(0, tf.shape(self.mask)[-1])\n        tt, ss = tf.meshgrid(t, indexing=\"ij\")\n        self._pilot_ind = tf.transpose(tf.stack([tt, ss], -1), [0, 2, 1])\n        self._pilot_ind = tf.cast(self._pilot_ind, tf.int32)\n\n    def get_weights(self):\n        \"\"\"Returns the trainable weights of the pilot symbols.\n\n        Returns\n        -------\n        : tf.complex\n            The trainable pilot weights.\n        \"\"\"\n        return [self._pilots, self._pilots_mask]\n\n    def set_weights(self, weights):\n        \"\"\"Sets the trainable weights of the pilot symbols.\n\n        Parameters\n        ----------\n        weights : List\n            A list of two elements representing the trainable weights and\n            the mask, respectively.\n        \"\"\"\n        pilots, mask = weights\n        self._pilots = pilots\n        self._pilots_mask = mask\n\n    @property\n    def mask(self):\n        \"\"\"The pilot mask.\"\"\"\n        return self._mask\n\n    @mask.setter\n    def mask(self, value):\n        if value is not None:\n            self._mask = value\n            self._pilots_mask = tf.cast(self._mask, self.dtype)\n        else:\n            self._mask = None\n            self._pilots_mask = None\n\n    @property\n    def pilots(self):\n        \"\"\"The (possibly) normalized trainable pilot symbols.\"\"\"\n        if self._normalize:\n            h = tf.reduce_sum(\n                tf.square(tf.abs(self._pilots)), axis=-1, keepdims=True)\n            h = tf.math.sqrt(h)\n            pilots = self._pilots/h\n        else:\n            pilots = self._pilots\n\n        return pilots\n\n    @property\n    def trainable(self):\n        \"\"\"Returns `True` if the pilots are trainable.\"\"\"\n        return self._trainable\n\n    @property\n    def normalize(self):\n        \"\"\"Returns `True` if the pilots are normalized.\"\"\"\n        return self._normalize\n\n    def call(self, resource_grid):\n        \"\"\"Returns an OFDM resource grid with sparse pilot symbols.\n\n        Parameters\n        ----------\n        resource_grid : :class:`~sionna.ofdm.ResourceGrid`\n            An instance of a :class:`~sionna.ofdm.ResourceGrid`.\n\n        Returns\n        -------\n        : :class:`~sionna.ofdm.ResourceGrid`\n            A modified version of the input :class:`~sionna.ofdm.ResourceGrid`\n            where the pilot symbols are set according to the configured\n            pilot pattern.\n        \"\"\"\n        # Create a copy of the input tensor and mask its pilot entries\n        x = tf.tensor_scatter_nd_update(resource_grid, self._pilot_ind,\n                                         self.pilots)\n\n        # Set the mask for the pilots\n        zero = tf.constant(0, tf.float32)\n        ind = tf.where(self._pilots_mask, zero, self._pilots_mask)\n        mask_ = tf.tensor_scatter_nd_update(self._pilot_ind, ind, ind)\n        mask_ = tf.expand_dims(mask_, axis=-1)\n        mask_ = tf.expand_dims(mask_, axis=-2)\n        resource_grid = tf.where(mask_, x, resource_grid)\n\n        return resource_grid\n```"
"To import the necessary packages for using Sionna, including TensorFlow and NumPy, you can follow these steps:\n\n1. Import the necessary packages using the regular import statements:\n```python\nimport os\nimport numpy as np\nimport tensorflow as tf\n```\n\n2. Configure the environment to use only a single GPU and allocate only as much memory as needed:\n```python\ngpu_num = 0 # Use \"\" to use the CPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n```\n\n3. Set the random seed for reproducibility:\n```python\ntf.random.set_seed(1)\nnp.random.seed(1)\n```\n\n4. Import the Sionna package. If it's not already installed, the code will automatically install it using pip:\n```python\ntry:\n    import sionna as sn\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    os.system(\"pip install sionna\")\n    import sionna as sn\n```\n\n5. Configure the notebook to use only a single GPU and allocate only as much memory as needed when using TensorFlow:\n```python\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\n```\n\nPlease ensure that you have the necessary permissions to install packages on your system, and that you have the necessary version of pip installed (a package manager for Python). If you encounter any issues, please verify your installation environment."
"Here is the detailed information of LinearInterpolator:   \n  \nsionna.ofdm.LinearInterpolator(freq_domain_time_smearing, l_min, resource_grid, dtype=tf.complex64, **kwargs)\nDeprecated \nInterpolates time-domain signals to the OFDM resource grid. \n\nThe class inherits from the Keras layer class and can be used as layer in a Keras model. \n  \nIt is deprecated as the functionality has been integrated into the ChannelModel, and will be removed in a future release.\n\nParameters\n\n- `freq_domain_time_smearing` (bool): \n    Enable frequency-domain time smearing.\n\n- `l_min` (int): \n    First OFDM symbol used for interpolation.\n\n- `resource_grid` (ResourceGrid): \n    Resource grid.\n\n- `dtype` (tf.Dtype): \n    Datatype for internal calculations and the output dtype. Defaults to tf.complex64.\n\nInput\n\n- `sampling_times` ([batch_size, num_ofdm_symbols, fft_size, n_channel_samples], tf.float): \n    Normalized time samples.\n\nOutput\n\n- `signal_out` ([batch_size, num_tx, num_ant, num_ofdm_symbols, fft_size], tf.complex): \n    Time-domain signals.\n\n**Note: ** \nFor the deprecated LinearInterpolator, the \u201cfrequency-domain time smearing\u201d is always applied, i.e., the method assumes perfect channel knowledge in the frequency domain, but random channel samples in the time domain. This is equivalent to the OFDMChannelModel with the \u201cperfect_cir\u201d interpolation method.  \n  \nsource code: [LinearInterpolator](https://nvlabs.github.io/sionna/_modules/sionna/ofdm/channel_model.html#LinearInterpolator).  \n  "
"Here is the detailed information of [sionna.utils.SymbolLogits2Moments](https://nvlabs.github.io/sionna/_modules/sionna/utils/tensors.html#SymbolLogits2Moments):  \n  \nComputes the mean and variance of a Gaussian distributed variable from logits (unbounded scale parameters).\n\nThis function assumes that the Gaussian distributed variable is considered as the sum of a large number of independent Gaussian distributed variables, each defined by a logit p_i = 0.5 * log( \u03c3_i 2 / 1 - \u03c3_i 2 ), with 0 < \u03c3_i < \u221e.  \n  \nThe mean and variance of the Gaussian distributed variable can be assumed to be computed from the logits according to the following expressions:\n\n\u03bc = \u2211 i p_i \u03bc i  \n\u03c3 2 = \u2211 i p_i \u03c3 i + \u2211 i p_i \u03bc i 2  \n\nThis class can be used as a layer in a Keras model.  \n  \n### Parameters\n\n- **shape**: 1D array, representing the shape of the Gaussian distributed variable.  \n- **dtype**: tf.DType (tf.float32), the dtype of the Gaussian distributed variable.  \n- **reduction_dims**: None or list of integers, default is None. List of dimensions to reduce when computing the moments. If None, no dimensions are reduced.  \n- **epsilon**: float, default is 1e-10. Used to avoid numerical instability in the computation of the moments.\n\n### Input\n\n- **logits**: Tensor with arbitrary shape, containing logits.\n\n### Output\n\n- **means**: Tensor with the same shape as shape, containing the computed mean(s).\n- **variances**: Tensor with the same shape as shape, containing the computed variance(s).\n\nsource code:\n```python\nclass SymbolLogits2Moments(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"SymbolLogits2Moments(shape, dtype=tf.float32, reduction_dims=None, epsilon=1e-10)\n\n    Computes the mean and variance of a Gaussian distributed variable from logits (unbounded scale parameters).\n\n    This function assumes that the Gaussian distributed variable is considered as the sum of a\n    large number of independent Gaussian distributed variables, each defined by\n\n    .. math::\n\n        p_i = 0.5 \\log( \\sigma_i^2 / 1 - \\sigma_i^2 )\n\n    with :math:`0 < \\sigma_i < \\infty`.\n\n    The mean and variance of the Gaussian distributed variable can be assumed to be computed from\n    the logits according to the following expressions:\n\n    .. math::\n\n        \\mu = \\sum_i p_i \\mu_i\n\n    .. math::\n\n        \\sigma^2 = \\sum_i p_i \\sigma_i + \\sum_i p_i \\mu_i^2\n\n    This class can be used as a layer in a Keras model.\n\n    Parameters\n    ----------\n    shape : 1D array\n        Representing the shape of the Gaussian distributed variable.\n\n    dtype : tf.DType, tf.float32\n        The dtype of the Gaussian distributed variable.\n\n    reduction_dims : None or list of integers, default is None.\n        List of dimensions to reduce when computing the moments.\n        If `None`, no dimensions are reduced.\n\n    epsilon : float\n        Default is 1e-10.\n        Used to avoid numerical instability in the computation of the moments.\n\n    Input\n    -----\n    logits : Tensor\n        :[...,n] Logits.\n\n    Output\n    ------\n    means : Tensor\n        :[...n,1], Mean values.\n\n    variances : Tensor\n        :[...n,1], Variance of the Gaussian distributed variable.\n    \"\"\"\n    def __init__(self,\n                 shape,\n                 dtype=tf.float32,\n                 reduction_dims=None,\n                 epsilon=1e-10):\n\n        super().__init__(dtype=dtype)\n\n        # store required attributes\n        self._reduction_dims = reduction_dims\n        self._epsilon = epsilon\n\n        # compute number of moments\n        n = int(np.max(shape))\n        self._n = n\n\n        # compute index mask for reducing the last `n` dimensions\n        mask = np.ones(shape=[n, n])\n        self._index_mask = np.triu(mask).ravel()\n\n    def compute_output_shape(self, input_shape):\n        # the shape of the output is the same as the shape of the logits, but with\n        # the last dimension removed\n        return input_shape[0:-1]\n\n    def call(self, logits):\n\n        # compute probabalistic logits\n        # the implementation of logit here is slightly different as in the original paper\n        # to avoid nans for zero s.\n        p = tf.math.exp(tf.concat([logits, tf.zeros(self._n, tf.float32)], axis=-1))\n            / tf.constant(2.0, dtype=logits.dtype))\n        p = tf.math.divide_no_nan((p - tf.math.divide_no_nan(1.0, p+1e-12)) , p+1e-12)\n        p = tf.where(p>0, p, tf.zeros_like(p)) # remove nans that could result from 0 logs.\n\n        # compute weights for reduction\n        if self._reduction_dims is None:\n            w = tf.ones(p.shape)\n        else:\n            w = tf.reduce_prod(1.0, tf.gather(p, self._reduction_dims, axis=-1))\n\n        # compute mean and variance\n        mu = tf.reduce_sum(p, axis=-1, keepdims=True)\n        sigma = tf.reduce_sum(p*2, axis=-1, keepdims=1) # multiply by 2 as sig^2 = 2*mu\n\n        # exclude ignored dimensions\n        mu = tf.gather(mu, self._index_mask, axis=-1)\n        sigma = tf.gather(sigma, self._index_mask, axis=-1)\n\n        # reduce dimensions\n        mu = tf.reduce_sum(mu, axis=-1, keepdims=True)\n        sigma = tf.reduce_sum(sigma, axis=-1, keepdims=1)\n\n        # exclude ignored dimensions\n        mu = tf.squeeze(mu, axis=self._reduction_dims)\n        sigma = tf.squeeze(sigma, axis=self._reduction_dims)\n\n        # add small epsilon to avoid nans\n        sigma = tf.math.maximum(sigma, self._epsilon)\n        mu = tf.squeeze(mu, axis=-1)\n\n        return mu, sigma\n```  \n  "
"['channel_freq', 'ofdm_channel'] = cir_to_ofdm_channel(self,                                            cir,                                            subcarrier_spacing,                                            fft_size,                                            normalize_channel=False)\n  \n  \nThe 'cir_to_ofdm_channel' function in the sionna.mimo module is used to compute the frequency response of the channel at frequencies, and given as 'channel_freq', using the input circularly symmetric complex Gaussian (CSCG) channel matrices.\n\nIt also generates the channel frequency responses, behaviour, and the channel estimates. The function outputs a tuple: 'channel_freq', 'ofdm_channel', 'h_time', 'h_freq', 'err_inter', 'prec', 'ind_panicked', 'ind_not_nul'], where:\n    \n- 'channel_freq' is the channel frequency response at the frequencies\n- 'ofdm_channel' represents the channel frequency response for an OFDM waveform after cyclic prefix removal\n- 'h_time' is the channel transfer function in the time domain\n- 'h_freq' is the channel transfer function in the frequency domain\n- 'err_inter' is the interpolation error variance for each link and each estimated channel time vector\n- 'prec' is the channel precision for each link and each estimated channel time vector\n- 'ind_panicked' is a tensor of sampling indicies for which the channel is most impacted by the interpolation error\n- 'ind_not_nul' is a tensor of sampling indicies for which the channel is not impacted by the interpolation error\n      \n  Note that, a link is defined by a tuple of transmitters and receivers."
"The `EPDetector` class in simulations is a part of the 'Sionna' signal and channel propagation simulation package for Python. This class is utilized for implementing an equal-power combining receiver to detect transmitted signals. It supports both matched filter (MF) and linear minimum mean-square error (LMMSE) methods for the detection. \n\nThe `EPDetector` class has the following parameters:\n- `output`: The type of output, either bits or symbols, controlled by the `soft_estimates` parameter.\n- `num_bits_per_symbol`: The number of bits per constellation symbol, for example, 4 for QAM16. This parameter is required only for QAM or PAM constellations.\n- `soft_estimates`: A boolean value to indicate whether soft-symbol estimates are returned or not. \n- `lmmse_equalizer`: A boolean value to indicate whether LMMSE equalization or MF equalization is used. \n- `detection_method`: The detection method used, either LMMSE or MF.\n- `feedback_estimator`: The instance of `LMMSEFeedbackEstimator` to use, or None if no feedback is used.\n- `dtype`: The data type of `y`. It can be either `tf.complex64` or `tf.complex128`, and the default is `tf.complex64`. The output data type is the corresponding real type (`tf.float32` or `tf.float64`).\n- `**kwargs`: Additional keyword arguments. \n\nThe `EPDetector` class is essential in simulations as it plays a crucial role in experimenting with equal-power combining of transmitted signals and different detection methods. It is particularly useful in scenarios where you need to work with mismatched filters or when you want to compare different detection algorithms under the same conditions."
"The first step into utilizing the Erbium-Doped Fiber Amplifier (`EDFA`) class within the Sionna package is to import the necessary class. Once you have the relevant class imported, you can instantiate it by setting the specific parameters as per your requirements. Afterward, you can run the instance by providing it the corresponding input.\n\nFor the `EDFA` class, typical parameters include but are not limited to:\n\n- `g`: Gain per unit length (in SI unit, default is 4.0).\n- `f`: Noise figure (in SI unit, default is 7.0).\n- `f_c`: Carrier frequency (in Hz, default is 193.55e12).\n- `dt`: Time step (in s, default is 1e-12).\n- `with_dual_polarization`: Indicates if the amplifier is dual polarized (default is False).\n- `alpha`: Link error rate (default is 1.0e-2).\n- `beta_2`: Group velocity dispersion coefficient (in s^2/m, default is -12.0).\n- `gamma`: Nonlinearity coefficient (in 1/m/W, default is 1.27).\n\nSetting up EDFA instance:\n\n```python\nfrom sionna.channel import EDFA\n\nedfa = EDFA(\n    g=4.0,\n    f=7.0,\n    f_c=193.55e12,\n    dt=1.0e-12,\n    with_dual_polarization=False,\n    alpha=1.0e-2,\n    beta_2=-12.0,\n    gamma=1.27\n)\n```\n\nRunning the EDFA instance:\nThe EDFA instance is run by passing the input signal to it. If the `no` parameter is provided, the noise is added to the output.\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\n# Number of samples in the signal\nnum_samples = 1000\n\n# Generate a random Gaussian signal as input\n# The signal is complex-valued\n# Equivalent to: np.random.randn(num_samples) + 1j*np.random.randn(num_samples)\nreal_part = np.random.randn(num_samples)\nimag_part = np.random.randn(num_samples)\nx = tf.complex(real_part, imag_part)\n\n# Define the noise variance explicitly\n# Scalar value, in this case simply the noise power\n# Hereafter, we assume that the noise has complex Gaussian statistics\nno = 1e-4\n\n# For the explicit noise variance, the noise has per-sample statistics\n# It can be specified as a tensor along the signal: `x + no*eta`,\n# where `eta` is the noise tensor with the same shape as `x`.\n# In the following, we simply add complex Gaussian noise to `x`.\n# The noise variance `no` is scaled by the number of samples to\n# reflect the noise power per sample.\nnoise = tf.complex(\n    tf.random.normal(x.shape, 0.0, tf.sqrt(no/2.0)),\n    tf.random.normal(x.shape, 0.0, tf.sqrt(no/2.0))\n)\n\n# Apply the EDFA\ny = edfa(x + noise)\n\n# Print the output signal shape\nprint(\"Output signal shape:\", y.shape)\n```\n\nIt's important to note that the input signal 'x' is a complex-valued tensor, and the output signal 'y' will also be a complex-valued tensor that retains the structure of the input signal. The output signal will be affected by the parameters specific to the EDFA instance that you provided during its setup."
"The `render_to_file()` function in the `Scene` class of the Sionna package is used to render the scene to a file. This function helps in visualizing the scene that has been created using the `load_scene()` or `synthetic_scene()` functions. The scene can be rendered as either a camera view or a top view.\n\nThe function takes the following parameters:\n\n- `filename`: This is the name of the file to save the rendering to. It can be a PNG, JPG, or any other supported file format.\n- `camera`: This parameter is optional and is used to specify the camera to use for the rendering. If not provided, the function uses the first camera found in the scene.\n- `resolution`: This parameter is optional and is used to specify the resolution of the rendering. It must be provided as a tuple of the width and height of the rendering in pixels. The default value is `(800, 600)`.\n- `show`: This parameter is optional and is used to specify if the rendering should be opened after rendering. The default value is `True`.\n- `background`: This parameter is optional and is used to specify the background color of the rendering. It must be provided as a string such as \"white\", \"black\", or \"#FFFFFF\". The default value is \"black\".\n- `overlay`: This parameter is optional and is used to specify if the overlay should be included in the rendering. The default value is `True`.\n- `fov`: This is the field of view for the rendering in degrees. The default value is 60.\n- `l_min`: This is the minimum brightness (in dB) for the rendering. The default value is -160.\n- `l_max`: This is the maximum brightness (in dB) for the rendering. The default value is -40.\n- `num_samples`: This is the number of samples to use for the rendering. The default value is 128.\n\nAn example of how to use this function is:\n\n```python\nscene = load_scene(sionna.rt.scene.munich)\nscene.render_to_file(\"rendering.png\")\n```\n\nThis will render the scene to a file named \"rendering.png\" using the specified parameters."
"Unfortunately, the provided context does not include any specific information about a \"wrap_angle_0_360\" function. As such, I am unable to describe this function in detail based on the provided context. \n\nI advise checking the original source code or documentation from where the function is supposed to be coming for detailed information about what it does and how to use it in a programming scenario."
"Here is the definition of ChannelModel: sionna.channel.ChannelModel\n  \n[source code:](https://nvlabs.github.io/sionna/_modules/sionna/channel/channel_model.html#ChannelModel)  \n```python\nclass ChannelModel(ABC):\n    # pylint: disable=line-too-long\n    r\"\"\"ChannelModel(inputs, **kwargs)\n    Abstract class for channel models.\n\n    Any channel model which generates channel impulse responses must implement this\n    interface.\n\n    Parameters\n    ----------\n    batch_size : int\n        Batch size\n\n    Property: real_additive\n        `bool`  Abstract property indicating if the channel model is\n        real-valued or complex-valued.\n        Defaults to `False`.\n    Property: single_taper\n        `bool`  Abstract property indicating if the channel model has only\n        a single path with a given delay and Doppler shift.\n        Defaults to `False`.\n    Property: spatial_correlation\n        `bool`  Abstract property indicating if the channel model has\n        spatial correlation.\n        Defaults to `False`.\n    Property: temporal_correlation\n        `bool`  Abstract property indicating if the channel model has\n        temporal correlation.\n        Defaults to `False`.\n\n    Property: using_past\n        `bool`  Abstract property indicating if the channel model makes use\n        of the past symbol or frame values.\n        Defaults to `False`.\n\n    Property: num_tx\n        `int`  Abstract property for the number of transmitters.\n\n    Property: num_rx\n        `int`  Abstract property for the number of receivers.\n\n    Property: num_tx_ant\n        `int`  Abstract property for the number of antennas per\n        transmitter.\n\n    Property: num_rx_ant\n        `int`  Abstract property for the number of antennas per receiver.\n\n    Property: num_paths\n        `int`  Abstract property for the number of paths.\n\n    Property: table_table467\n        `dict`  Abstract property containing the R[nt] and R[nr] regularization\n        parameters for the 3GPP 38.901 channel model.\n        If not `None`, must be a callable for :math:`\\text{r_bar}` and a\n        callable for :math:`\\text{c_bar}` that return the regularization\n        parameters taking as inputs the batch index and the instance index\n        which are learned during training.\n        Defaults to `None`.\n\n    Property: l_min\n        `int`  Abstract property representing the smallest time-lag for\n        the discrete complex baseband channel. Defaults to 0.\n\n    Property: l_max\n        `int`  Abstract property representing the largest time-lag for\n        the discrete complex baseband channel. Defaults to 0.\n\n    Property: tau_bar\n        `float`  Abstract property for the average delay of the\n        line-of-sight path [sec].\n\n    Property: num_time_samples\n        `int`  Abstract property for the number of time samples forming\n        the channel impulse response.\n\n    Property: num_ofdm_symbols\n        `int`  Abstract property for the number of OFDM symbols.\n\n    Property: fft_size\n        `int`  Abstract property for the FFT size.\n\n    Property: band\n        `str`  Abstract property for the wireless channel model. Must be\n        from the set [`\"UHF\"`, `\"SMT\"`].\n\n    Property: third_dim\n        `int`  Abstract property for the third dimensions of the complex\n        -valued tensor that the channel model will generate.\n        For a complex-valued single-taper channel model, this is equal to 1.\n        For all other models, it is equal to three.\n\n    Property: dtype\n        `tf.complex`  ChannelModel generate complex-valued channel responses.\n\n    Property: input_dtype\n        `tf.complex`  ChannelModel generate complex-valued channel responses.\n\n    :Input Defaults:\n\n    Inputs\n    ------\n    los :\n        `bool` or `None`  Line of sight.\n\n    delay_spread :\n        `tf.float` or `None`  RMS delay spread [s].\n\n    dtype :\n        `tf.complex` or `None`  Datatype for internal calculations and the\n        output dtype. Defaults to `tf.complex64`.\n\n    :Output:\n\n    a :\n        [`batch size`, `num_rx`, `num_rx_ant`, `num_tx`, `num_tx_ant`,\n        `num_paths`, `num_time_samples`] or\n        [`batch size`, `num_rx`, ` num_rx_ant`, `num_tx`, `num_tx_ant`,\n        `num_paths`, `num_ofdm_symbols`, `fft_size`], `tf.complex`\n        Channel responses\n        (either in the time domain or the frequency domain, depending on\n        the :class:`~sionna.channel.ChannelModel` implementation)\n    b :\n        [`batch size`, `num_rx`, `num_rx_ant`, `num_tx`, `num_tx_ant`,\n        `num_paths`, `num_time_samples`, `num_interpolation`],\n        tf.complex\n        Channel outputs and linear combining vectors\n        for each receiver.\n        (In the time domain, channel outputs are computed for all\n        paths and transmitters, and linear combining vectors are\n        applied after the time-domain-to-frequency-domain conversion.\n        In the frequency domain, channel outputs are computed for all\n        paths and transmitters on the corresponding resource elements,\n        and linear combining vectors are applied on the resource elements.).\n        Only output  `b` is given if ``return_indices`` is `True`.\n    ind :\n        [`batch size`, `num_rx`, `num_rx_ant`, `num_tx`, `num_tx_ant`,\n        `num_paths`, `num_time_samples`, `num_ofdm_symbols`,\n        `fft_size`, `num_paths_per_tap`], `tf.int32`\n        Indices describing which channel tap (i.e., which cluster)\n        and which multipath component belongs to which path.\n        Only output  `ind` is given if ``return_indices`` is `True`.\n\n    :Note:\n    An instance of the :class:`~sionna.channel.ChannelModel` class is callable,\n    meaning that once implemented, a channel model can be used like a function:\n\n    .. code-block:: Python\n\n        # Assuming 'model' is an instance of ChannelModel\n        a, b = model(los, delay_spread)\n\n    Implementing a ChannelModel\n    ---------------------------\n    Note that implementing an :class:`~sionna.channel.ChannelModel` model\n    requires the following input values according to the specific model:\n\n    *Input Defaults:*\n\n    Inputs\n    ------\n    los :\n        `bool` or `None` : Line of sight\n\n    delay_spread :\n        `tf.float` or `None` : RMS delay spread [s].\n\n    dtype :\n        `tf.DType` : Datatype for internal calculations and the output.\n            Defaults to `tf.complex64`.\n\n    Customizable Channel Models\n    ---------------------------\n\n    Several customizable channel models are provided. They can be instantiated\n    either directly or using the :class:`~sionna.channel.ESMModel`. See\n    :ref:`customizable-channel-models` for more details.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def spatial_correlation(self):\n        \"\"\"Does the channel model exhibit spatial correlation.\n\n        This property must be implemented by concrete classes. It\n        indicates if the channel model exhibits spatial correlation.\n        `sionna.channel.ChannelModel.spa"
"Here is the detailed information of sionna.rt.Scene.render_to_file: \n\nRenders the scene to a file.\n\nThe function first renders the scene to a framebuffer. It then renders the corresponding spatial frequency\nto a file in the optical channel model, i.e., a PNG file for the fourier transform of the received signals.\n  \nName | Type | Description\n--- | --- | ---------\nfb_size | 2-tuple | Rendered resolution. \nfilename | str | Output filename. \n\nPlease note that the sionna.rt.Scene instance should be precised before calling this function, and the `sionna.rt.optical_channel_model` function should be used to return an optical channel model."
"To configure DeepMIMO for use with Sionna, follow these steps:\n\n1. Import the necessary libraries and modules. This includes Sionna, DeepMIMO, and TensorFlow.\n\n```python\nimport os\nimport sionna\nfrom sionna.nr import PUSCHConfig, PUSCHTransmitter, PUSCHReceiver\nfrom sionna.channel import cir_to_ofdm_channel, subcarrier_frequencies, OFDMChannel, ApplyOFDMChannel, CIRDataset\nfrom sionna.channel.tr38901 import AntennaArray, CDL, UMi, UMa, RMa\nfrom sionna.utils import compute_ber, ebnodb2no, sim_ber\nfrom sionna.ofdm import KBestDetector, LinearDetector\nfrom sionna.mimo import StreamManagement\nfrom sionna.utils import PlotBER\n```\n\n2. Configure the GPU settings.\n\n```python\ngpu_num = 0\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n```\n\n3. Import the DeepMIMO library. If it's not already installed, the code will install it.\n\n```python\ntry:\n    import DeepMIMO\nexcept ImportError as e:\n    import os\n    os.system(\"pip install DeepMIMO\")\n    import DeepMIMO\n```\n\n4. Configure the DeepMIMO dataset. This includes the scenario, number of paths, active basestations, user rows, antenna shapes, and the dataset size.\n\n```python\nfrom DeepMIMO import GenerateDataset, LoadDataset\nscenario = 'UMi'\nnum_paths = 10\nactive_bs = 4\nuser_row = 10\nbs_antenna = ['itu2633_panel', 'tr38901_array']\ndataset_size = 10000\n```\n\n5. Generate the DeepMIMO dataset.\n\n```python\ntry:\n    os.makedirs('data')\nexcept:\n    pass\nif not os.path.isfile('data/dataset_%s_%i_%i_row_per_ut_antenna_%i.p'%(scenario, num_paths, active_bs, user_row)):\n    GenerateDataset(output_dir='data',\n                 scenario=scenario,\n                 num_paths=num_paths,\n                 tx='high',\n                 rx='row',\n                 polarization='dual',\n                 antenna='uma',\n                 antenna_shape='38.4,16,0.4',\n                 cov_mat='deep_learning',\n                 gen_los=True,\n                 gen_nlos=True,\n                 gen_sf=False,\n                 subcarrier_frequency=freq,\n                 carrier_frequency=frq,\n                 o2i_model=None,\n                 verbose=True)\nelse:\n    print('  Dataset already generated.')\n```\n\n6. Load the DeepMIMO dataset.\n\n```python\ndeep_mimo = LoadDataset('data/dataset_%s_%i_%i_row_per_ut_antenna_%i.p'%(scenario, num_paths, active_bs, user_row))\n```\n\n7. Configure the DeepMIMO CIR Sampler.\n\n```python\ndeep_mimo_cir_sampler = deep_mimo.get_multi_cell_cir_sampler(num_rx,\n                                                      num_rx_ant,\n                                                      deep_mimo_scenario,\n                                                      los,\n                                                      min_angle_of_arrival,\n                                                      max_angle_of_arrival)\n```\n\n8. Configure the DeepMIMO CIR Dataset.\n\n```python\ndepp_mimo_cir_generator = DeepMIMOCIRGenerator(deep_mimo_cir_sampler,\n                                             deep_mimo,\n                                             batch_size)\n```\n\n9. Get the DeepMIMO CIR.\n\n```python\na, tau = next(deep_mimo_cir_generator)\n```\n\n10. Convert the DeepMIMO CIR to Sionna OFDM channel.\n\n```python\ndef deepmimo_cir_to_channel(tau, a, normalize=False):\n    o = tf.shape(tau)[0]\n    tau = tf.squeeze(tau, -1)\n    a = tf.complex(a, tf.zeros_like(a))\n    a = tf.transpose(a, [0, 2, 1])\n    tau = tf.pad(tau, [[0, 6], [0, 0]])\n    tau = tf.cast(tau, tf.float32)\n    tau = tf.expand_dims(tau, -1)\n    h_freq = tf.reduce_sum(a, -1) # a is now a sum of complex paths -> H is complex\n    h_time = tf.constant(0.0, tf.float32)*tf.eye(64) # Delta (assuming 200MHz BW)\n    for i in range(64):\n        h_time = tf.tensor_scatter_nd_update(h_time, tf.reshape(i+6, [1]), tf.reshape(tau, [-1, 1])))\n    h_time = tf.squeeze(h_time, -1)\n    if normalize:\n        n = tf.complex(tf.random.normal([o, 1], 0.0, 1.0), tf.random.normal([o, 1], 0.0, 1.0))\n        n = tf.squeeze(tf.math.reduce_sum(tf.math.reduce_sum(n**2, -1, keepdims=True, reduce_prod=True), -1, keepdims=True), -1)\n        h_time, n = sionna.channel.utils.normalize_channel(h_time, axis=[-2, -1], name=None, return_scale=True)\n    return h_time, tau\n```\n\n11. Configure the Sionna OFDM Channel.\n\n```python\nchannel = sionna.channel.tr38901. OFDMChannel(channel_config, name='channel')\n```\n\n12. Configure the Sionna PUSCH Receiver.\n\n```python\npusch_receiver = sionna.nr. PUSCHReceiver(pusch_transmitter, name='pusch_receiver')\n```\n\n13. Configure the Sionna PUSCH Transmitter.\n\n```python\npusch_transmitter = sionna.nr. PUSCHTransmitter(pusch_config, name='pusch_transmitter')\n```\n\nRemember to replace the variables `freq`, `frq`, `deep_mimo_scenario`, `los`, `min_angle_of_arrival`, `max_angle_of_arrival`, `num_rx`, `num_rx_ant`, `batch_size`, and `pusch_config` with their appropriate values."
"To use the \"interleaver module\" in the Sionna package to permute tensors, follow these steps:\n\n1. First, you need to import the necessary modules. You will need TensorFlow and the interleaver module from the Sionna package.\n\n```python\nimport tensorflow as tf\nfrom sionna.fec.interleaving import RandomInterleaver, Deinterleaver, RowColumnInterleaver, Turbo3GPPInterleaver, InverseTurbo3GPPInterleaver\n```\n\n2. Define your tensor. The tensor should have at least rank 2, for example:\n\n```python\nx = tf.constant([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n```\n\n3. Create an instance of the interleaver you want to use. You can choose from `RandomInterleaver`, `Deinterleaver`, `RowColumnInterleaver`, `Turbo3GPPInterleaver`, or `InverseTurbo3GPPInterleaver`. \n\nIn this example, we create a `RandomInterleaver`:\n\n```python\ninterleaver = RandomInterleaver(seed=1234, keep_batch_constant=False, axis=-1)\ndeinterleaver = Deinterleaver(interleaver=interleaver) \n# choose same instance to interleave and deinterleave\n```\n\n4. Call the `interleaver` instance with your tensor as the argument. This will return the interleaved tensor.\n\n```python\nx_int = interleaver(x)\n```\n\n5. To reverse the permutation, call the `deinterleaver` instance with the interleaved tensor as the argument. This will return the original order of the tensor.\n\n```python\nx_deint = deinterleaver(x_int)\n```\n\nPlease note that the `axis` parameter is used to choose which axis of the tensor will be permuted. The first example interleaves the last dimension, but you can choose any other dimension by changing the `axis` parameter."
"The `MMSEPICDetector` function is part of the 'Sionna' signal detection module, and is used for Multiple-Input Multiple-Output (MIMO) signal detection with Iterative Detection and Deoding (IDD) applied to a batch of MIMO links. The function implements Minimum Mean Square Error (MMSE) estimation for Iterative Detection and Decoding.\n\nINPUTS:\n- `output`: A callable that takes an MMSEPICDetectorOutput object as input and doesn't return anything. This is used to output the detection results.\n- `num_iter`: Number of iterations for the IDD. Must be at least 1.\n- `conv_k`: The number of convolutional layers within the LDPC decoder. Each layer processes one or two channels. The total number of channels after interleaving is num_lr * (conv_k + 1).\n- `num_iter_ldpc`: Number of iterations for the LDPC decoder. Must be at least 1.\n- `sim_id`: Simulation ID, used to keep track of multiple detectors.\n- others: Additional parameters that are required by the detector, such as the LDPC decoder, the constellation type, the number of bits per symbol, the encoder, the FEC code, the interleaver, or a pattern selector. These parameters must be provided as keyword arguments.\n\nOUTPUTS: \nThe function outputs `y_hat` and `iter_stats`. \n- `y_hat` is a 2+D tensor containing the estimated symbols.\n- `iter_stats` is an MMSEPICDetectorStats object that encapsulates diagnostic information for each iteration, such as LLR histograms.\n\nIn terms of usage, the function is typically used to detect symbols transmitted over a MIMO channel, where it might not be possible to directly calculate the desired output. It uses an iterative algorithm to alternate between detection and decoding to approximate the ideal MIMO detector.\n\nThe number of iterations can be configured based on the desired trade-off between performance and computational complexity. The function is particularly useful in scenarios where the channel state information (CSI) is not known at the receiver and must be estimated from the received signals. This is crucial in mobile communication systems to enable efficient transmission and reception over fast-changing channels."
"Here is the definition of RaisedCosineFilter: sionna.signal.RaisedCosineFilter(span_in_symbols, samples_per_symbol, beta, window=None, normalize=True, trainable=False, dtype=tf.float32, **kwargs)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/signal/filter.html#RaisedCosineFilter)\n\nsource code:\n```python\nclass RaisedCosineFilter(Filter):\n    # pylint: disable=line-too-long\n    r\"\"\"RaisedCosineFilter(span_in_symbols, samples_per_symbol, beta, window=None, normalize=True, trainable=False, dtype=tf.float32, **kwargs)\n\n    Layer for applying a raised-cosine filter of ``length`` K\n    to an input ``x`` of length N.\n\n    The filter is applied through discrete convolution.\n\n    The output length is equal to the input length N.\n\n    The raised-cosine filter is defined by\n\n    .. math::\n        h(t) =\n        \\begin{cases}\n        \\frac{1}{T} \\left(1 + \\beta\\left(\\frac{4}{\\pi}-1\\right) \\right), & \\text { if }t = 0\\\\\n        \\frac{\\beta}{T\\sqrt{2}} \\left[ \\left(1+\\frac{2}{\\pi}\\right)\\sin\\left(\\frac{\\pi}{4\\beta}\\right) + \\left(1-\\frac{2}{\\pi}\\right)\\cos\\left(\\frac{\\pi}{4\\beta}\\right) \\right], & \\text { if }t = \\pm\\frac{T}{4\\beta} \\\\\n        \\frac{1}{T} \\left( \\frac{\\sin\\left(\\pi\\frac{t}{T}(1-\\beta)\\right) + 4\\beta\\frac{t}{T}\\cos\\left(\\pi\\frac{t}{T}(1+\\beta)\\right)}{\\pi\\frac{t}{T}\\left(1-\\left(4\\beta\\frac{t}{T}\\right)^2\\right)} \\right), & \\text { otherwise}\n        \\end{cases}\n\n    where :math:`\\beta` is the roll-off factor and :math:`T` the symbol duration.\n\n    Parameters\n    ----------\n    span_in_symbols: int\n        Filter span as measured by the number of symbols.\n\n    samples_per_symbol: int\n        Number of samples per symbol, i.e., the oversampling factor.\n\n    beta : float\n        Roll-off factor.\n        Must be in the range :math:`[0,1]`.\n\n    window: Window or string ([\"hann\", \"hamming\", \"blackman\"])\n        Instance of :class:`~sionna.signal.Window` that is applied to the filter coefficients. Alternatively, a string indicating the window name can be provided. In this case, the chosen window will be instantiated with the default parameters. Custom windows must be provided as instance.\n\n    normalize: bool\n        If `True`, the filter is normalized to have unit power.\n        Defaults to `True`.\n\n    trainable: bool\n        If `True`, the filter coefficients are trainable variables.\n        Defaults to `False`.\n\n    dtype: tf.DType\n        The `tf.DType` of the filter coefficients.\n        Defaults to `tf.float32`.\n\n    Input\n    -----\n    x : [..., N], tf.complex or tf.float\n        The input to which the filter is applied.\n        The filter is applied along the last dimension.\n\n    Output\n    ------\n    y : [...,N], tf.complex or tf.float\n        Filtered signal.\n        Same shape as the input ``x``.\n    \"\"\"\n\n    def __init__(self,\n                 span_in_symbols,\n                 samples_per_symbol,\n                 beta,\n                 window=None,\n                 normalize=True,\n                 trainable=False,\n                 dtype=tf.float32,\n                 **kwargs):\n\n        assert 0.0 <= beta <= 1.0, \"beta must be from the intervall [0,1]\"\n        self._beta = beta\n\n        super().__init__(span_in_symbols,\n                         samples_per_symbol,\n                         window=window,\n                         normalize=normalize,\n                         trainable=trainable,\n                         dtype=dtype,\n                         **kwargs)\n\n    @property\n    def beta(self):\n        \"\"\"Roll-off factor\"\"\"\n        return self._beta\n\n    @property\n    def _coefficients_source(self):\n        h = self._coefficients(self._length, self._sampling_time)\n        h = tf.constant(h, self.dtype)\n        return h\n\n    def _coefficients(self, n, ts):\n        # pylint: disable=line-too-long\n        t = ts*(tf.range(n, dtype=self.dtype) - (n-1.)/2.)\n        h = np.zeros([n])\n        beta = self.beta\n        if beta==0.0:\n            h = np.where(t==0.0, 1.0, 0.0)\n        else:\n            h = np.sin(np.pi*t*(1.0-beta)) + \\\n                4.0*beta*t*np.cos(np.pi*t*(1.0+beta))\n            h = np.where(t==0.0, h, 0.0)\n        h = h/np.sqrt(self.sampling_time)\n        return h\n```"
"The PAM2QAM class in sionna.mapping is a Layer that converts one or more PAM-corrected values to a QAM symbol. It helps in the execution of code by providing a mapper from digital baseband symbols of a PAM constellation to those of a corresponding QAM constellation.\n\nIn the process of code execution, it helps in the transformation of the PAM constellation to the QAM constellation. For instance, two PAM constellations are received as inputs, and these are subsequently mapped to a single QAM constellation. \n\nPAM2QAM also supports the addition of AWGN and fading. This functionality allows for simulations that consider channel impairments. \n\nNote: The usage of this class in code operations is determined by the specific requirements of the task or simulation at hand."
"To configure StreamManagement for a simple uplink scenario using Python, you need to define the following variables:\n\n- `num_tx`: This is the number of transmitters you have. For a simple MIMO scenario, you can set this to 1.\n\n- `num_rx`: This is the number of receivers. For a simple MIMO scenario, you can set this to 1.\n\n- `num_streams_per_tx`: This is the number of streams that each transmitter is sending. For a simple MIMO scenario, you can set this to 1.\n\n- `rx_tx_association`: This is an array that determines which receiver gets the streams from which transmitter. Since we have a single transmitter and receiver in this scenario, the `rx_tx_association` is simply [[0,0]]. This means that the first transmitter is associated with the first receiver.\n\nHere is the Python code that defines these variables:\n\n```python\nnum_tx = 1\nnum_rx = 1\nnum_streams_per_tx = 1\nrx_tx_association = [[0,0]]\n```\n\nThese variables are used to configure the StreamManagement class in the Sionna library. The StreamManagement class is responsible for managing the streams in your MIMO system. It determines which transmitter is sending which stream to which receiver. \n\nIn the provided context, you can see how these variables are used to create a StreamManagement object:\n\n```python\nstream_manager = StreamManagement(\n    num_tx=num_tx,\n    num_rx=num_rx,\n    num_streams_per_tx=num_streams_per_tx,\n    rx_tx_association=rx_tx_association\n)\n```\n\nThe `stream_manager` object is now configured to handle the streams in your MIMO system. It determines which transmitter is sending which stream to which receiver based on the `rx_tx_association` you provided."
"Constellations and bit-channels are fundamental concepts in higher order modulation systems.\n\nConstellations refer to the pattern of signal points in the in-phase (I) and quadrature (Q) components of a digital modulation scheme. In higher order modulation, multiple bits are represented by each modulation symbol. The constellation is a mapping between integer values and complex numbers, where each integer value (or binary representation thereof) is associated with a complex number representing the modulation symbol.\n\nA bit-channel, on the other hand, is the channel through which bits are transmitted in digital communications. This term is often used in the context of the Binary Phase Shift Keying (BPSK) and Quadrature Phase Shift Keying (QPSK) modulation schemes. In such systems, each bit of the input sequence is independently and identically distributed (i.i.d.) in the bit-channel, and the channel output is based on the received symbols from the bit-channel.\n\nIn the context of higher order modulation, the performance of the system can be evaluated in terms of the bit error rate (BER), which is the probability of an error in the demodulation process. The BER is strongly related to the quality of the bit-channel, the size of the constellation, and the signal-to-noise ratio (SNR).\n\nTo simulate the system and evaluate the BER, one can use the Sionna package, which provides a suite of tools for simulating the PHY-layer link-level performance of 5G-compliant components. This includes the simulation of constellations in both uplink and downlink settings, the generation of bit channels from code words to constellation points, and the application of AWGN noise."
"The `pam` function in the Sionna mapping module is used for the generation of a vector of a given length with uniformly distributed PAM (Pulse Amplitude Modulation) values. This function serves as a helper to make it easy to work with quadtrature amplitude modulation (QAM) constellations.\n\nThe function takes two main input parameters. The first one, `num_bits_per_symbol`, specifies the number of bits per constellation point. The second input parameter, `double_precision`, is a boolean defaulting to False. If it is set to True, the dtype of the output y is set to tf.float64.\n\nThe function produces an output, `y`, which is a 1+D tensor (or dense tensor) of shape [num_points] (or [num_points, 2] if `num_bits_per_symbol` > 2). The type of `y` is tf.complex or tf.complex64 (or tf.complex128 if `double_precision` is set to True), corresponding to the correct data type as per the input parameters.\n\nThe output, `y`, contains symbols from the QAM constellation with an arbitrary energy per symbol. If a custom energy per symbol is desired, the function `es_normalize` from `sionna.utils` should be used.\n\nNote: The [`pam` function](https://nvlabs.github.io/sionna/api/mapping.html#sionna.mapping.pam) and its output are a part of the Sionna mapping module, used in physical layer link simulations. The function serves to implement a mapping of binary values to constellation points for modulation schemes like QAM."
"Assumptions to fulfill when using \"List2LLR\" class:\n\n1. Call the `List2LLR` class from the `sionna.mimo` module. \n2. The `num_bits_per_symbol` parameter data type should be an `int`. It assumes that each element in the list input has shape `[num_bits_per_symbol, M, 1]`.\n3. The `num_bits_per_symbol` parameter represents the number of bits per channel use, i.e., the resolution of the LLRs.\n4. The `num_bits_per_symbol` parameter stands for the number of bits taken for each symbol within an OFDM resource grid, assuming one is following the base-rate coding assumption.\n5. The `hard_out` parameter is assumed to be a boolean data type, which indicates whether the implementation is expected to return hard decisions on the symbols or not.\n6. The `dtype` parameter is assumed to be a valid tensorflow data type. It decides the output data type of the layer.\n7. It is assumed that the input `llrs` tensor, defining the log-likelihood ratios, is at least of shape `[num_bits_per_symbol, M, n_max]`, where `n_max` is the number of candidate waveforms.\n8. It is assumed that an extra dimension is automatically added to the input `llrs` if it's not already there. The extra dimension interprets each element in the tensor as `[num_bits_per_symbol, M, n]`.\n9. It is assumed that for every (scaled) energy per each dim 0 element in `llrs` there is an additional dimension that interprets it. This means that the shape of the input tensor can effectively be `[num_bits_per_symbol, M, n, s]`, where `s` is the number of (scaled) energy per each dim 0 elements.\n10. An input tensor `llrs` and the `num_bits_per_symbol` parameter must be provided.\n11. The input tensor and parameters must not have NaN values.\n12. The `spurious_emissions` parameter cannot take an internal implementation. It is an external method.\n13. It is assumed that this method is part of the `List2LLR` class, which in turn is part of the `mimo` module of the Sionna package. \n\nPlease note that these assumptions are based on the context provided and may need to be verified with the developers or official documentation of the Sionna package."
"The `MMSEPICDetector` class is from the `sionna.ofdm` module and is used for Equalization of streams in multi-cell MIMO networks. It is an Equalization-PI Detector for use in Iterative Detection and Decoding (IDD) algorithms.\n\nThis class is a type of MIMO equalizer, but it is different from the K-Best detector or the Linear Detector in the sense that it neither needs nor assumes channel state information at the receiver (i.e., it doesn't need the channel matrices `H` and `H_inv`).\n\nThe parameters for the `MMSEPICDetector` are:\n- `output_bit`: A string that determines the type of output, either LLRs on bits or logits on constellation symbols.\n- `resource_grid`: An instance of the `ResourceGrid` class.\n- `stream_management`: An instance of `StreamManagement` class.\n- `numberOfIterations`: An integer that represents the number of MMSE PIC iterations. The default value is 1.\n- `dtype`: tf.Dtype attribute for internal calculations and the output data type. The default value is tf.complex64.\n\nThe conditions under which `MMSEPICDetector` is used are:\n- It can be used for the detection of streams in multi-cell MIMO networks. This is particularly useful when dealing with large networks where the channel state information (CSI) might not be immediately available at all receivers.\n- The number of iterations can be repeated to improve the performance of the detector, but this might lead to an increase in computational complexity."
"1. Defining the QAM2PAM Class:\n\nIn the given context, the QAM2PAM class is defined as a subclass of the `Layer` class from `keras`. This class implements a transformation from a Quadrature Amplitude Modulation (QAM) constellation to a Pulse Amplitude Modulation (PAM) constellation. QAM and PAM are modulation techniques used in signal processing and communication systems.\n\nThe parameters for the QAM2PAM class are:\n- `num_bits_per_symbol` (integer type): It represents the number of bits per QAM constellation point. Possible values are 1, 2, 4, or 6.\n- `normalize` (boolean type): If set to `True`, the PAM constellation will be normalized to have unit power. It defaults to `True`.\n- `dtype` (tf.DType): It indicates the data type of the inputs and outputs. It defaults to tf.complex64.\n\nThe QAM2PAM class requires the number of bits per QAM symbol as input. For forward pass through this layer, a 2-tensor of shape `[..., n]` and dtype `tf.complex64` is expected. This tensor contains the QAM symbol indices. The method transforms these indices into PAM symbol indices following the normalization, if applicable.\n\n```python\nclass QAM2PAM(Layer):\n    def __init__(self, num_bits_per_symbol, normalize=True, dtype=tf.complex64):\n        super().__init__(dtype=dtype)\n        assert num_bits_per_symbol in [1, 2, 4, 6], \\\n            \"num_bits_per_symbol must be 1,2,4,or 6.\"\n        self._num_bits_per_symbol = num_bits_per_symbol\n        self._pam_scaling = 1 / np.sqrt(2 * self._num_bits_per_symbol)\n\n        if normalize:\n            self._normalizing_factor = np.sqrt(1 / 42 * self._num_bits_per_symbol)\n        else:\n            self._normalizing_factor = np.float32(1.)\n\n        self._hardmax = tf.keras.layers.Activation(\"hardmax\")\n\n    def call(self, inputs):\n        x = self._hardmax(inputs)\n        x = self._pam_scaling * tf.cast(x, tf.float32) \\\n            - self._pam_scaling * 3. * tf.cast(x == 0, tf.float32)\n        x = tf.math.reduce_sum(x, axis=-1)\n        x = tf.expand_dims(x, axis=-1)\n        return x\n```\n\n2. Explaining the Transformation from QAM to PAM:\n\nThe QAM2PAM class transforms QAM symbol indices to PAM symbol indices. This process involves several steps:\n\n- The input, which represents the QAM symbol indices, is first converted to PAM symbol indices with soft demapping. This is done by transforming the QAM symbol indices into a PAM modulation with normalization. The number of bits per symbol, `num_bits_per_symbol`, determines the conversion process.\n\n- The converted PAM symbols are then decimated by two, starting with the most recent one. This step is crucial in ensuring that the transformation is reversible and does not lead to ambiguity between different sequences of QAM symbols.\n\n- The decimated PAM symbols are pre-scaled and post-scaled to ensure that the transformation is exact and reversible. The `normalize` parameter indicates whether the PAM constellation should be normalized to have unit power.\n\n- Finally, a hard decision is made on the PAM symbols to get the indices of the PAM symbols. This hard decision is equivalent to rounding the values to the nearest integer.\n\nRemember that the reversal of this process is also possible. PAM symbols can be up-scaled and d"
"To verify that the generator matrix and parity-check matrix are orthogonal in GF(2) with the \"verify_gm_pcm\" function of the Python package Sionna, you need to do the following steps:\n\n1. Import the necessary functions from the Sionna package. Since the \"verify_gm_pcm\" function is part of the `sionna.fec.utils` module, you should import it from this module.\n\n2. Load the generator and parity-check matrices. In the context, the generator matrix `G` and the parity-check matrix `H` are loaded from two files using the `load_parity_check_examples` function. \n\n3. Call the `verify_gm_pcm` function with the generator matrix and the parity-check matrix as arguments. The function will return `True` if the matrices are orthogonal, and it will return `False` if they are not orthogonal.\n\nHere is the code that implements these steps:\n\n```python\nfrom sionna.fec.utils import verify_gm_pcm, load_parity_check_examples\n\n# load the generator and parity-check matrices\npcm, G, L, M = load_parity_check_examples(pcm_id=0) # load example matrices\n\n# verify the matrices\nresult = verify_gm_pcm(gm=G, pcm=pcm)\nprint(\"Result: \", result)\n```\n\nIn this code, `verify_gm_pcm(gm=G, pcm=pcm)` tests if the generator matrix `G` is orthogonal to the parity-check matrix `pcm`. The function returns `True` if the matrices are orthogonal, and `False` if they are not. \n\nPlease note that the generator matrix and parity-check matrix must be of full rank and binary for the function to return `True`. If the function returns `False`, this means that the matrices are not orthogonal."
"LinearEncoder is a class from the sionna.fec.linear module of the Sionna library. The class forms part of the framework for working with arbitrary linear binary and non-binary (sub)component encoders. It serves as a wrapper for already existing generator matrices or other 2D arrys describing a linear transformation.\n\nHere is how you can import the LinearEncoder class from the sionna.fec.linear module:\n\n```python\nfrom sionna.fec.linear import LinearEncoder\n```\n\nThis statement allows you to access the LinearEncoder class, which can then be used to define an encoder instance. The encoder will associate with a binary or non-binary generator matrix `gm` or a parity-check matrix `pcm`, given that they are valid and comply with expected properties (e.g., full rank, correctness).\n\nPlease ensure that you have installed the Sionna library and its dependencies before running the code. If the library is not installed, you may need to install it using pip:\n\n```python\npip install sionna\n```\n\nFor the purpose of the present context, no explicit encoding is done with `LinearEncoder` as the provided description focuses on importing the class. However, here is a snippet showing how you might set up the encoding process:\n\n```python\n# Assuming you have a valid generator matrix `gm` of shape [k, n] (where k < n)\nencoder = LinearEncoder(gm, is_pcm=False) # If `gm` is a valid generator matrix\n# If a parity-check matrix `pcm` is provided instead\n# encoder = LinearEncoder(pcm, is_pcm=True) # If `pcm` is a valid parity-check matrix\n```\n\nWhile this code provides a scaffold for encoding data, it is crucial to ensure the `gm` or `pcm` you use is valid and meets the required properties. The property `is_pcm=False` indicates that the provided matrix is a generator matrix, not a parity-check matrix. If the `LinearEncoder` is instantiated with `is_pcm=True`, then `pcm` is treated as a parity-check matrix."
"The `OFDMDetectorWithPrior` class is part of Sionna's signal processing module and is used in Orthogonal Frequency-Division Multiplexing (OFDM) systems. This class serves as a layer for detecting symbols in an OFDM waveform, with the assumption that prior knowledge of the data or channel state information (CSI) is available.\n\nThis class pre-processes an OFDM resource grid, channel estimation, and optional prior information, and computes for each receiver antenna, the post-equalization symbol estimates and error variances. These can be either represented as soft-symbol estimates or (effective) signal-to-noise ratios. This makes the `OFDMDetectorWithPrior` class extremely versatile.\n\nIt is built on top of the resource grid, timing offset, and channel estimator. The LS channel estimates are computed, and optionally combined with additional prior information. Then, channel interpolation, equalization, and optionally a demapper are applied. If the `return_indices` parameter is set, the permutation of the receive antennas is undone, allowing the user to conveniently inspect the receiver output for all transmit streams or users.\n\nThe `OFDMDetectorWithPrior` class is unique because it combines the functionalities of an OFDMDetector and a PriorDetector. This means that it can detect symbols using a detection algorithm, for either soft-symbols or SNR values, that operates on the observation, the channel, the receive filter, and pre-coding or combining vector. This is particularly useful when prior information is provided, such as for a maximum ratio detector.\n\nNote that the layer is not differentiable, and does not allow to compute gradients with respect to itself. However, it can be used in Graph or XLA mode as it does not rely on the compatibility of compiled sub-functions."
"\"bin2int_tf\" function is used to convert a binary tensor to an integer tensor. The function operates by taking the binary representation of the last dimension of the input tensor and converting it to an integer. This function can be particularly useful in scenarios where a binary representation needs to be converted to an integer, for example, in decoding operations in neural networks or communication systems.\n\nHere is a sample of how you can use this function:\n\n```python\nfrom sionna.utils import bin2int_tf\n\n# Assume `binary_tensor` is a binary tensor of shape [2, 2] containing values 0 and 1\n# binary_tensor = [[0, 1], [1, 1]]\n\nint_tensor = bin2int_tf(binary_tensor)\n\n# int_tensor is now an integer tensor of shape [2, 2] containing integer equivalents of binary values\n# int_tensor = [[2, 3], [1, 3]]\n```\n\nIn the example above, we first import the \"bin2int_tf\" function from the sionna.utils module. We then assume that there is a binary tensor called `binary_tensor` that contains binary values. We call the \"bin2int_tf\" function with `binary_tensor` as the sole argument. The function processes the last dimension of `binary_tensor` and returns an integer tensor.\n\nDo note that the input to the function must be a tensor of 0s and 1s only for it to work properly. errorCode would be raised if the input tensor contains any other values."
"In the given context, we have a usage of the TB5GScrambler class from the Sionna library to enable 5G New Radio (NR) compliant scrambling. This class is used to implement bitstream scrambling as per the 3GPP TS 38.211 standard.\n\nHere's how you can use the TB5GScrambler class:\n\n1. You need to import the class:\n\n```python\nfrom sionna.nr import TB5GScrambler\n```\n\n2. Initialize the scrambler:\n\nYou can pass an initializer with a seed and an explicit frame ID (if not provided, it defaults to 1) to the scrambler for initialization.\n\n```python\nscrambler = TB5GScrambler(seed_init=(0, 1), init_sb=1)\n```\n\n3. Define your input parameters:\n\n`tb_size (int)` - The transport block size, which defines the number of bits per codeword.\n\n`n_rnti (int or list of ints)` - RNTI identifier provided by higher layer. Defaults to 1 and must be in the range [0, 65335]. If explicit for more than one RNTI is provided, the output list form is assumed.\n\n4. Generate the scrambled bits:\n\nYou can generate a scrambled bit (or bits) by calling the scrambler with your bit sequence(s) and, optionally, the `n_rnti` parameter. If `n_rnti` is provided as a list, the output will be a list of scrambled bits for each corresponding RNTI.\n\n```python\nscrambled_bits = scrambler((bits, n_rnti))\n```\n\nIn this usage, `bits` is your original bit sequence and `n_rnti` is the RNTI(s) you want to scramble the bits with. If no `n_rnti` is provided, it defaults to 1. \n\nPlease be aware that the parameters' specific values (e.g., `tb_size`, `seed_init`, `init_sb`) are as per the 3GPP specifications and must be chosen accordingly. Also, ensure that the `explicit` parameter is set to `True` if you are providing an explicit RNTI. \n\nThe bit scrambling process in 5G NR is defined in TS 38.211 and TS 38.213, and the TB5GScrambler class implements these specifications. Further, this class includes parameters related to the 5G NR-specific scrambling mechanisms. For a different scrambling mechanism, a different class (such as `PUSCHScrambler` or `PDSCHScrambler`) should be used. \n\nPlease make sure that the Sionna library is correctly installed and imported before running this code. The 5G NR utilities may require a specific version of Sionna, which could be verified in the context provided.\n\nIf there are any additional requirements specific to your use case, you might need to adjust the parameters or use other Sionna classes/methods."
"Here is the detailed source code of int_mod_2: [sionna.fec.utils.int_mod_2(int_num)](https://nvlabs.github.io/sionna/_modules/sionna/fec/utils.html#int_mod_2)  \n  \nComputes the integer modulo 2 operation. This corresponds to a parity-check for the validity of a binary string.\n\nIf the result is zero, the operation is valid.\n\nint_mod_2(pos_rlncount)  \n  \nInput\n\n    int_num (int) \u2013 An integer to be tested by the modulo operation.\n\nOutput\n\n    bool \u2013 True if the result of the operation is zero. Otherwise, False.\n\nsource code:  \n```python\ndef int_mod_2(pos_rlncount):\n    \"\"\"Computes the integer modulo 2 operation. This operation corresponds to\n    a parity-check for the validity of a binary string.\n\n    If the result is zero, the operation is valid.\n\n    Input\n    -----\n        int_num : int\n            An integer to be tested by the modulo operation.\n\n    Output\n    -----\n        : bool\n            True if the result of the operation is zero. Otherwise, False.\n    \"\"\"\n    return pos_rlncount%2==0\n```  \n  "
"Here is the detailed information of sionna.utils.SymbolSource: \n\nsionna.utils.SymbolSource\n  \nSource that generates a constant symbol with given out_dim.\n\nThis class generates a constant symbol of length out_dim. The symbol is either complex or real valued, depending on the value of the parameter complex_out. Whether the generated symbol is real or complex is automatically detected by the Transform precoding method. \n\n### Parameters\n\n- `out_dim` (int): This is the length of the symbol to be generated. It can be any positive integer. If complex_out is False, only the first element of `out_dim` is used.\n  \n- `complex_out` (bool): This specifies if the symbol should be complex-valued or not. Defaults to True. If you desire to have real-valued symbols, set this to False.\n\n### Input\n\n- `batch_size` (int): This is the batch size. It determines the number of symbols to generate.\n\n### Output\n\n- `tf.complex or tf.float`: This is the output constant symbol. It is tf.complex if `complex_out` is set to True, and tf.float if `complex_out` is set to False.\n\n**Note: **  It is important to note that this class is part of the Sionna package, which is used for the simulation of wireless communication systems. For the actual source code of this class, you can view it on [GitHub](https://github.com/nok/sionna/blob/main/sionna/utils.py)."
"Here is the detailed information of ChannelModel:   \n  \n[sionna.channel.ChannelModel](https://nvlabs.github.io/sionna/_modules/sionna/channel/channel_model.html#ChannelModel)  \n  \nAbstract class that defines an interface for channel models.\n\nAny channel model which generates channel impulse responses must implement this interface. All the channel models available in Sionna, such as RayleighBlockFading or TDL, implement this interface.\n\nRemark: Some channel models only require a subset of the input parameters.  \n  \nInput\n    batch_size  (int) \u2013 Batch size.\n    num_time_steps (int) \u2013 Number of time steps.\n    sampling_frequency (float) \u2013 Sampling frequency in Hertz (Hz).\n\nOutput\n    a, tau\n        a [Tensor of shape [batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps] or shape [batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths]] and tau [Tensor of shape [batch size, num_rx, num_tx, num_paths] or shape [batch size, num_rx, num_tx, num_paths]], of cstype float32 \u2013 Channel impulse responses a and corresponding delays tau (in second, s).  \n  \nsource code:  \n```python\nclass ChannelModel(ABC):\n    # pylint: disable=line-too-long\n    r\"\"\"ChannelModel is an abstract class that defines an interface for channel models.\n\n    Any channel model which generates channel impulse responses must implement this\n    interface. All the channel models available in Sionna, such as\n    :class:`RayleighBlockFading` or :class:`TDL`, implement this interface.\n\n    Remark: Some channel models only require a subset of the input parameters.\n\n    Input\n    ------\n    batch_size : int\n        Batch size\n\n    num_time_steps : int\n        Number of time steps\n\n    sampling_frequency : float\n        Sampling frequency [Hz]\n\n    Output\n    -------\n    a : [batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps] or [batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths] tf.complex\n        Channel impulse responses\n\n    tau : [batch_size, num_rx, num_tx, num_paths] or [batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths] tf.float\n        Channel path delays [s]\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, batch_size, num_time_steps, sampling_frequency):\n        # pylint: disable=line-too-long\n        r\"\"\"Generates random channel impulse responses.\n\n        Input\n        batch_size : int\n            Batch size\n\n        num_time_steps : int\n            Number of time steps\n\n        sampling_frequency : float\n            Sampling frequency [Hz]\n\n        Output\n        a : [batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps] or [batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths] tf.complex\n            Channel impulse responses\n\n        tau : [batch_size, num_rx, num_tx, num_paths] or [batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths] tf.float\n            Channel path delays [s]\n        \"\"\"\n        pass\n```  \n  "
"Building a Custom RadioMaterial and Attaching it to a SceneObject\n\nIn Sionna, a custom RadioMaterial can be implemented by creating a class with a call method that computes the complex-valued relative permittivity. This material can then be attached to a SceneObject. The following steps show how to build a custom RadioMaterial and attach it to a SceneObject:\n\n1. Import the necessary modules and classes.\n\n```python\nfrom sionna.rt import RadioMaterial, SceneObject\nfrom sionna.constants import EPS0, MU\n```\n\n2. Implement the custom RadioMaterial class. It should compute the complex-valued relative permittivity based on the frequency and material parameters.\n\n```python\nclass PerfectElectricalConductor(RadioMaterial):\n    # Relative permeability\n    mu_r = 1.0\n\n    def __init__(self, frequency=None):\n        super().__init__()\n        self.frequency = frequency\n\n    def call(self, scene, frequency):\n        # The relative permittivity is set to -infty,\n        # meaning that the conductor is perfectly reflective\n        # for all frequencies.\n        return {\"complex_relative_permittivity\" : complex(1.0*EPS0),\n                \"relative_permeability\" : complex(1.0*MU),\n                \"complex_conductivity\" : complex(1.0/scene.ray_tracer.frequency_dimension[0])}\n```\n\n3. Create an instance of the RadioMaterial class and specify the material parameters.\n\n```python\nradiating_designed_mat = PerfectElectricalConductor()\n```\n\n4. Create an instance of the SceneObject class and attach the RadioMaterial to it.\n\n```python\nso = SceneObject(name=\"radiating_transmitter\",\n                 geometry=\"file\",\n                 material=radiating_designed_mat)\n```\n\nThe SceneObject \"radiating_transmitter\" is now defined as a part of the scene and will be used in further simulations. Please note that the file name of the geometry should be provided in the `geometry` parameter of the `SceneObject`."
"To perform Maximum A Posteriori Probability (MAP) decoding using an instance of `BCJRDecoder` from the Sionna package, you can follow the steps below. The `BCJRDecoder` is a type of decoder for Low-Density Parity-Check (LDPC) and Turbo codes. It computes the log-likelihood ratios (LLRs) or hard decisions for each bit of the received vector, which is based on the LLRs of the channel and certain assumptions about the code.\n\nFirst, you'll need to import the necessary modules and initialize the `BCJRDecoder`. Make sure to set up your environment with the necessary packages. For MAP decoding, you provide the output of the channel LLRs (Log-Likelihood Ratios) or the equivalent received as a `soft_in` parameter. If you have LLRs from the demodulator and also LLRs from the `LDPCLinearEncoder` (which represents knowledge of the code), you can provide both to the `BCJRDecoder`.\n\nThe `hard_out` parameter in the `BCJRDecoder` is critical for the type of output received from the decoder. When `hard_out=True`, the decoder will provide hard decisions (i.e, decoding output will be bits). On the other hand, when `hard_out=False`, the decoder will provide soft-values, which are typically LLRs. Please note that when `hard_out=False`, the output tensor will be of type `tf.float` as it would represent the LLRs.\n\nHere's a general step outline:\n\n1. Import the decoder and necessary modules.\n\n```python\nfrom sionna.fec.ldpc.decoding import BCJRDecoder\nimport tensorflow asjson\n```\n\n2. Initialize the `BCJRDecoder` with the necessary parameters including the LDPC parity-check matrix and the order of the `soft_out` provided as a `list` type parameter.\n\n```python\ndecoder = BCJRDecoder(pcm, soft_out=[<order>], hard_out=<hard_out_flag>)\n```\n\n3. Prepare your input data according to the `soft_in` parameter requirement. If `soft_in` is `True`, it expects channel LLRs or `logits`. If `soft_in` is `False`, it expects binary format of `x` instead.\n\n4. Decode your input with the `BCJRDecoder`. If you've provided pre-computed LLRs from the demodulator and have other knowledge about the code, you might need to call the decoder with both LLRs from the channel and also the LLRs from the `LDPCLinearEncoder`.\n\n5. Analyse the output tensor, which will depend on the `hard_out` parameter. If `hard_out=True`, it will be a tensor of shape `[k]` containing the hard-decided codeword bits. If `hard_out=False`, it will be a tensor of shape `[k]` containing the LLRs (or the bits) of the codeword.\n\nPlease note that you need to have the appropriate `LDPC` parity-check matrix to use in the `BCJRDecoder`. The Goemans-Parlagi-Sudan (GPS) irregular parity-check matrix is an example used in the context for illustration. Also, the actual use may require additional steps such as handling stopping set patterns for higher order LDPC decoding processes.\n\nNote: The shape of the output tensor will be determined by the shape of the input tensor, which is typically `[n]`, and the type of output, which will be determined by the `hard_out` parameter (bit array or LLR values)."
"To utilize the `ebnodb2no` function in the Sionna package for wireless simulation, proceed as follows:\n\n1. Make sure the required modules are imported. In this case, since we don't have access to the actual code, we can only assume that it would at least import the Sionna package.\n\n```python\nimport Sionna\n```\n\n2. Identify the init parameters for the function. Again, this is an assumption since actual parameters are not provided due to the context's limitations.\n\n```\nno : Scalar Tensor, typically float\n    The noise floor in linear scale (not in dB).\n\nebno : Scalar Tensor, typically float\n    The desired Eb/No in dB.\n```\n\n3. Ensure that the function is called in the correct manner. The first argument should be `no` and the second should be `ebno`.\n\n```python\n# no and ebno are two scalars typically floats\n# An example value for no\nno = 1e-4\n# An example value for ebno\nebno = 10\n\n# Call the function with the two scalar parameters\nSionna.utils.ebnodb2no(no, ebno)\n```\n\nThe function `ebnodb2no` converts the energy per bit to noise power spectral density ratio (Eb/No) in dB to the noise power spectral density `no` in linear scale. The return value is `no`, the noise power spectral density. \n\nPlease note that actual parameters for `no` and `ebno` should be provided based on the specific use case. This is important to ensure that the function is called with the correct arguments and that it will return the desired results."
"To evaluate the baseline and the two autoencoder-based communication systems, follow these steps:\n\n1. **Set up the systems**: Import the necessary libraries and set up the systems. This includes defining the number of transmit and receive antennas, the number of bits per symbol, the batch size, and the signal-to-noise ratio (SNR) in dB for the evaluation. Also, set up the communication systems, including the baseline, the system with the convolutional encoder and decoder, and the system with the LMMSE equalizer and the neural receiver.\n\n2. **Evaluate the Baseline**: Create an instance of the `BaselineModel` class and evaluate it. The baseline is a non-optimized end-to-end communication system.\n\n3. **Evaluate the Autoencoder-based Communication Systems**: Create instances of the `AEConventionalModel` and `AEResidualModel` classes and evaluate them. These systems implement communication systems using autoencoders, where the transmitter and receiver are implemented as layers. The `AEConventionalModel` uses conventional training of the neural network, while the `AEResidualModel` uses residual learning to train the neural network.\n\nEnsure that the necessary parameters are correctly set, such as the batch size and the SNR. Also, set the `trainable` parameter to `False` when evaluating the systems, as the models do not need to be trained during the evaluation. The `verbose` parameter can be set to control the printing of performance metrics during the evaluation.\n\n```python\nbaseline = BaselineModel(num_tx_ant, num_rx_ant, num_bits_per_symbol, batch_size, ebno_dB=ebno_db)\nprint('Baseline Model:')\nbl_performance = baseline()\nprint('\\n')\naeconv = AEConventionalModel(num_tx_ant, num_rx_ant, num_bits_per_symbol, batch_size, ebno_dB=ebno_db, trainable=False, detector=detector, demapper=demapper)\nprint('AEConventional Model:')\naeconv_performance = aeconv()\nprint('\\n')\naeres = AEResidualModel(num_tx_ant, num_rx_ant, num_bits_per_symbol, batch_size, ebno_db=ebno_db, trainable=False, detector=detector, demapper=demapper)\nprint('AEResidual Model:')\naeres_performance = aeres()\nprint('\\n')\n```\n\nThe output of each evaluation will be the bit error rate (BER) and block error rate (BLER) of the system."
"To configure the notebook to use only a single GPU and allocate only as much memory as needed, follow these steps:\n\n1. Import the necessary modules:\n```python\nimport os\nimport tensorflow as tf\n```\n\n2. Specify the GPU you want to use. If you want to use the CPU, set `gpu_num` to an empty string (\"\"). In this example, we're using the first GPU, so we set `gpu_num` to 0:\n```python\ngpu_num = 0\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n```\n\n3. Set the TensorFlow log level to 3 to see warnings:\n```python\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n```\n\n4. Get the list of all available GPUs:\n```python\ngpus = tf.config.list_physical_devices('GPU')\n```\n\n5. If GPUs are found, configure TensorFlow to use only the first one and allocate only as much memory as needed:\n```python\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\n```\nIn the above code, `tf.config.experimental.set_memory_growth` is used to enable memory growth for the selected GPU. When memory growth is enabled, TensorFlow allocates only as much GPU memory as needed for the runtime allocations: it starts out allocating very little memory, and as the program gets used more memory, TensorFlow extends its GPU memory region to fulfill the runtime allocations."
"To calculate the J-function using the \"j_fun\" function in the Sionna package, do the following steps:\n\n1. Import the \"j_fun\" function from the Sionna package.\n\n```python\nfrom sionna.stats import j_fun\n```\n\n2. Compute the mean square error (MSE) for the scalar case. This can be done using the \"j_fun\" function. It takes as input the signal-to-noise ratio (SNR) in dB and the number of information bits per codeword. Also, specify the explicit output which is the codeword length if you want to obtain it.\n\n```python\nsnr_db = 2.0\nbatch_size = 1000\nn = 4 # Codeword length\nres = j_fun(snr_db, batch_size, n)\n```\n\nIn the above code, \"snr_db\" is the SNR specified in dB, \"batch_size\" is the number of SNR samples, \"n\" is the codeword length. The output \"res\" will be the value of the J-function. If you want to calculate the asymptotic value of the J-function, set \"asymptotic=True\".\n\n3. Compute the complex-valued generalization of the J-function for the joint detection of multiple coded streams. This can be done using the \"j_fun\" function. It takes as input the number of transmitters, the number of streams per transmitter, the number of bits per constellation point, and the SNR in dB.\n\n```python\nnum_tx = 2\nnum_streams_per_tx = 2\nnum_bits_per_symbol = 2 # QPSK\nsnr_db = 2.0\nres = j_fun(num_tx, num_streams_per_tx, num_bits_per_symbol, snr_db)\n```\n\nIn the above code, \"num_tx\" is the number of transmitters, \"num_streams_per_tx\" is the number of streams per transmitter, \"num_bits_per_symbol\" is the number of bits per constellation point, and \"snr_db\" is the SNR specified in dB. The output \"res\" will be the value of the J-function."
"Here is the definition of PUSCHTransmitter: sionna.nr.PUSCHTransmitter(pusch_configs, return_bits=True, output_domain='freq', dtype=tf.complex64, verbose=False, **kwargs)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/nr/pusch_transmitter.html#PUSCHTransmitter)  \n\n```python\n#\n# SPDX-FileCopyrightText: Copyright (c) 2021-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n\"\"\"PUSCH Transmitter for the nr (5G) sub-package of the Sionna library.\n\"\"\"\nimport numpy as np\nimport tensorflow as tf\nfrom precoding import Precoder\nfrom ofdm import OFDMResourceGrid, LSChannelEstimator\nfrom .pusch_config import PUSCHConfig\n\nclass PUSCHTransmitter:\n    # pylint: disable=line-too-long\n    r\"\"\"Creates a PUSCHTransmitter responsible for generating batches of 5G NR PUSCH slots in the frequency domain.\n\n    The PUSCHTransmitter takes as input a batch of codewords and associated\n    parameters and generates a batch of frequency domain baseband signals\n    corresponding to the resource grid structure defined in 3GPP TS 38.211\n    and 38.213 [3GPP38211]_.\n\n    Parameters\n    ----------\n    pusch_configs : instance of :class:`~sionna.nr.PUSCHConfig`\n        PUSCH configuration parameters\n\n    return_bits : bool\n        If `True`, the ``PUSCHTransmitter`` outputs modulated symbols.\n        Defaults to `True`.\n\n    output_domain : One of [\"freq\", \"time\"]\n        The output domain of the :class:`~sionna.nr.ofdm.OFDMModulator`.\n        Defaults to \"freq\".\n\n    dtype : One of [tf.complex64, tf.complex128], tf.DType\n        The dtype of inputs `x`. Defaults to tf.complex64.\n        The output dtype is the corresponding real dtype (tf.float32 or tf.float64).\n\n    verbose : bool\n        If `True`, print layer's parameter at initialization.\n        Defaults to `False`.\n\n    Input\n    -----\n    : [batch_size, num_tx, num_layers, num_codewords, num_dmrs_ports, num_ofdm_symbols, fft_size], tf.complex\n        Batch of frequency domain signals to be precoded.\n\n    : [batch_size, num_tx, num_layers, num_codewords, num_dmrs_ports, num_ofdm_symbols, num_subcarriers], tf.complex\n        Batch of time-domain signals to be precoded.\n        Only required if ``output_domain`` equals \"time\".\n\n    Output\n    ------\n    : [batch_size, num_tx, num_ant, num_ofdm_symbols, fft_size], tf.complex or [batch_size, num_tx, num_ant, num_time_samples], tf.complex\n        Frequency- or time-domain PUSCH signal\n\n    Note\n    ----\n    The :class:`~sionna.nr.PUSCHConfig` pusch_configs.icw ``ICW`` method is\n    implemented by setting the ``num_codewords`` attribute of the underlying\n    :class:`~sionna.nr.Precoder`` instance.\n    \"\"\"\n    def __init__(self,\n                 pusch_configs,\n                 return_bits=True,\n                 output_domain='freq',\n                 dtype=tf.complex64,\n                 verbose=False,\n                 **kwargs):\n        super().__init__(**kwargs)\n        assert output_domain in [\"freq\", \"time\"], \\\n            \"output_domain must be either 'freq' or 'time'\"\n        assert pusch_configs.rnti is not None, \\\n            \"rnti cannot be None\"\n        self.pusch_configs = pusch_configs\n        self._precoder = Precoder(precoder=\"nr\", pusch_configs=pusch_configs)\n        self.resource_grid = OFDMResourceGrid(pusch_configs.resource_grid,\n                                              pusch_configs.carrier)\n        self.channel_estimator = LSChannelEstimator(pusch_configs.carrier,\n                                                     interpolation_type=\"nn\",\n                                                     channel_type=\"pusch\")\n        self.return_bits = return_bits\n        self.output_domain = output_domain\n        self.dtype = dtype\n        self.verbose = verbose\n\n    @property\n    def precoder(self):\n        \"\"\"The precoder used by the transmitter\"\"\"\n        return self._precoder.precoder\n\n    def call(self, inputs):\n        if self.output_domain==\"freq\":\n            return self._call_freq_domain(inputs)\n        else:\n            return self._call_time_domain(inputs)\n\n    def _call_time_domain(self, inputs):\n        x, no = inputs\n        h_hat = self.resource_grid.precoding_matrices\n        occ = self.resource_grid.occupied_carriers\n        num_tx = x.shape[1]\n        num_layers = x.shape[2]\n        num_ofdm_symbols = x.shape[5]\n        precoding_flag = tf.ones([x.shape[0], num_tx, num_layers])\n        output = self._precoder(x,\n                               h_hat,\n                               precoding_flag,\n                               occ)\n        if self.return_bits:\n            output = self.precoder(output)\n        return output\n\n    def _call_freq_domain(self, inputs):\n        x, no = inputs\n        h_hat = self.resource_grid.precoding_matrices_freq\n        pcm_hat = tf.transpose(h_hat, (0, 1, 2, 5, 3, 4))\n        num_tx = x.shape[1]\n        num_layers = x.shape[2]\n        num_ofdm_symbols = x.shape[5]\n        precoding_flag = tf.zeros([x.shape[0], num_tx, num_layers])\n        output = self._precoder(x,\n                               pcm_hat,\n                               precoding_flag,\n                               None)\n        if self.return_bits:\n            output = self.precoder(output)\n        return output\n```"
"Here is the detailed information of verify_gm_pcm: [sionna.fec.utils.verify_gm_pcm(g, verify_results=True)](https://nvlabs.github.io/sionna/_modules/sionna/fec/utils.html#verify_gm_pcm)  \n  \nVerify that a generated parity-check matrix is full rank.\n\nThis function checks that for every subset of k columns of the identity matrix (or, equivalently, for every row of the parity-check matrix), the subset contains no more than k columns of non-zeros. If this condition is fulfilled for all subsets, the function returns True. Otherwise, it returns False.\n\nNote that a full rank parity-check matrix implies that the kernel of the parity-check matrix, and thus, the all-ze"
"Permittivity is a measure of the electric polarizability of a medium. In physics, it is a measure of the ability of a substance to store electric dipoles, which are created by separating positive and negative charges. The permittivity of a medium is typically denoted by the Greek letter \u03b5 (epsilon) and is a pure number, representing the ability of a material to permit the formation of an electric field in response to an applied electric field.\n\nPermeability is a measure of the ability of a medium to support the formation of a magnetic field. In physics, it is a measure of the resistance of a material to the formation of a magnetic field, which is typically denoted by the Greek letter \u03bc (mu) and is a pure number.\n\nPermittivity and permeability are calculated for specific materials using complex equations that take into account the internal structure of the material, such as the arrangement of atoms and molecules. For example, the permittivity of a dielectric material can be calculated using the formula:\n\n\u03b5 = \u03b50\u03b5r\n\nWhere \u03b50 is the vacuum permittivity, \u03b5r is the relative permittivity (also known as the dielectric constant), and \u03b5 is the permittivity of the material. The relative permittivity is a measure of how much a dielectric material can store electric potential energy in the form of charge separation.\n\nPermeability can be similarly calculated, taking into account the structure of the material and the applied magnetic field. For paramagnetic and diamagnetic materials, the permeability is typically approximated as:\n\n\u03bc \u2248 \u03bc0\u03bcr\n\nWhere \u03bc0 is the vacuum permeability, \u03bcr is the relative permeability, and \u03bc is the permeability of the material. Paramagnetic and diamagnetic materials are those that are affected by magnetic fields, but not by electric fields.\n\nIt is important to note that the exact values of these parameters can vary depending on the specific characteristics of the material, such as its temperature and the frequency of the applied field."
"Here is the detailed information of RemoveNulledSubcarriers:   \n  \n[sample-source](https://nvlabs.github.io/sionna/_modules/sionna/ofdm/preprocess.html#RemoveNulledSubcarriers)  \n  \nRemoves nulled guard and/or DC subcarriers from a OFDM resource grid.\n  \n**Parameters**\n\n- `with_dc` (bool): Indicates if the DC carrier is enabled or not. Defaults to True.\n- `num_guard_carriers` (int): Indicates the number of guard carriers at the beginning and end of the resource grid. Defaults to 0.\n- `resource_grid` (ResourceGrid): An instance of ResourceGrid from which to remove nulled subcarriers.\n\n**Input**\n\n- `y` ([batch_size, num_tx, num_streams_per_tx, num_ofdm_symbols, fft_size], tf.complex): Full resource grid.\n\n**Output**\n\n- `y` ([batch_size, num_tx, num_streams_per_tx, num_ofdm_symbols, num_effective_subcarriers], tf.complex): Resource grid without nulled subcarriers. The DC subcarrier, if enabled, is also removed."
"Here is the definition of LLRs2SymbolLogits: sion.mimo.mapping.LLRs2SymbolLogits(num_bits_per_symbol, hard_out=False, dtype=tf.float32, **kwargs)  \n  \nSource code:  \n```python\nclass LLRs2SymbolLogits(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    LLRs2SymbolLogits(num_bits_per_symbol, hard_out=False, dtype=tf.float32, **kwargs)\n\n    Computes logits or hard-decisions for constellation symbols, from a tensor\n    of LLRs (log likelihood ratios) on bits:\n\n    .. math::\n        \\ell^{(s)} = f \\left( \\sum_{k=1}^n \\ell^{(b)}_k \\right)\n\n    where :math:`\\ell^{(b)}_k` is the LLR for the :math:`k\\text{th}` bit soft-\n    input and :math:`\\ell^{(s)}` is the :math:`s\\text{th}` logit (or hard-decision)\n    on the constellation symbol.\n\n    Parameters\n    ----------\n    num_bits_per_symbol : int\n        The number of bits per constellation symbol, e.g., 4 for QAM16.\n\n    hard_out : bool\n        If `True`, the layer provides hard-decided symbols instead of soft-\n        likelihoods/logits. Defaults to `False`.\n\n    dtype : tf.Dtype\n        The dtype for the input and output. Defaults to `tf.float32`.\n\n    Input\n    -----\n    llrs : [...,n/num_bits_per_symbol], tf.float\n        LLRs for every bit received from the channel.\n\n    Output\n    ------\n    : [...,n], tf.float\n        Logits or hard-decisions for the constellation symbols.\n\n    Note\n    ----\n    If the input logits are not `0.0` or `10.0`, then an approximative,\n    but fast method is used to compute the logits.\n    \"\"\"\n\n    def __init__(self, num_bits_per_symbol, hard_out=False,\n                 dtype=tf.float32, **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        _check_qam_source_code(self.dtype)\n        self._qam = QAM(num_bits_per_symbol, hard_out, self.dtype)\n\n    def call(self, inputs):\n        x = self._qam.llrs2data(inputs)\n        if self._qam._hard_out: # hard decisions\n            x = _to_hard_decisions(x, self._qam, tf.as_dtype(self.dtype))\n        return x\n```\n  \n**Source code information:**  \nThe source code of LLRs2SymbolLogits is from sion.  \n  \n\n  \n  "
"To configure the notebook to use only a single GPU and allocate only as much memory as needed, follow these steps:\n\n1. Import the necessary modules:\n```python\nimport os\nimport tensorflow as tf\n```\n\n2. Specify the GPU you want to use. If you want to use the first GPU, set `gpu_num` to 0. If you want to use the CPU, set `gpu_num` to an empty string (\"\").\n```python\ngpu_num = 0\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n```\n\n3. Set the GPU options to be in a compliant runtime mode. This will cause TensorFlow to only allocate as much GPU memory as needed.\n```python\ngpu_options = tf.config.experimental.GpuOptions(allow_growth=True)\nsession = tf2.compat.v1.Session(config=tf.config.experimental.set_memory_growth(*gpu_options))\ntf.config.experimental.set_memory_growth(tf.get_default_graph(), 1024MB)\n```\n\n4. List the physical devices you can use. If a GPU is available, you should see it listed in the output.\n```python\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n```\n\n5. To avoid warnings from TensorFlow, set the log level to 'ERROR'.\n```python\ntf.get_logger().setLevel('ERROR')\n```\n\nAfter following these steps, your notebook should be configured to use only a single GPU and allocate only as much memory as needed."
"To compute codeword bits from information bits using the `LinearEncoder` provided in the `Sionna` package, you need to follow a specific procedure. First, make sure you have imported the necessary classes from the `sionna.fec.linear.encoding` module. These classes include `LinearEncoder` and `BinarySource`. \n\nThe `BinarySource` is used to generate a tensor with random 0s and 1s, which will serve as the input tensor for the `LinearEncoder`. The `LinearEncoder` is instantiated with either a generator matrix or a parity-check matrix (also known as a PCM). If a parity-check matrix is used, it must be of full rank and binary.\n\nHere is a step-by-step demonstration of how to compute codeword bits from information bits using the `LinearEncoder`:\n\n1. Import the necessary classes from the Sionna package.\n\n```python\nfrom sionna.fec.linear.encoding import LinearEncoder\nfrom sionna.fec.linear.encoding import BinarySource\n```\n\n2. Define the number of information bits per codeword and the code length.\n\n```python\nk = 100  # Number of information bits per codeword\nn = 200  # Code length\n```\n\n3. Create a `BinarySource` and generate random information bits.\n\n```python\nsource = BinarySource()\nu = source([batch_size, k])  # Generate random information bits\n```\n\n4. Instantiate the `LinearEncoder` and set the `enc_mat` parameter to the desired generator matrix (or parity-check matrix).\n\n```python\nencoder = LinearEncoder(enc_mat=pcm, is_pcm=True)  # `pcm` is the parity-check matrix\n```\n\n5. Call the `encoder` with the information bits `u` to obtain the codeword bits.\n\n```python\nc = encoder(u)\n```\n\nThe resulting `c` is a tensor containing the codeword bits. It should be noted that the `encoder` used must implement the corresponding matrix multiplication for the specific input tensor shape, as is the case with `LinearEncoder`. If the provided matrix does not match the shape of the input `u`, a different encoder must be used or the input tensor `u` must be appropriately shaped."
"Here is the detailed information of sionna.utils.flatten_last_dims:  \n\n[sionna.utils.flatten_last_dims(tensor, num_dims)](https://nvlabs.github.io/sionna/_modules/sionna/utils/tensors.html#flatten_last_dims)\n\nFlattens the last `num_dims` dimensions of a tensor.\n\nThis operation flattens the last num_dims dimensions of a tensor. For example, if a tensor input of shape [...,a,b,c,d] is given where num_dims=2, the operation will flatten the last two dimensions so that the final shape becomes [...,a,bcd].\n\n### Parameters\n\n- **tensor**: A tensor of any shape.\n- **num_dims** (int): The number of dimensions to merge into one. Must be smaller than or equal to the rank of the tensor.\n\n### Returns\n\n- A tensor of the same type as `tensor` with `num_dims` less dimensions, obtained by flattening the last `num_dims` dimensions of `tensor`.\n\nsource code:\n```python\ndef flatten_last_dims(tensor, num_dims):\n    \"\"\"\n    Flattens the last `num_dims` dimensions of a tensor.\n\n    This operation flattens the last `num_dims` dimensions of a tensor. For\n    example, if a tensor ``input`` of shape `[...,a,b,c,d]` is given where\n    ``num_dims``=2, the operation will flatten the last two dimensions so that\n    the final shape becomes `[...,a,bcd]`.\n\n    Args:\n        tensor : A tensor.\n        num_dims (int) : The number of dimensions to merge into one. Must be\n            smaller than the rank of ``tensor``.\n\n    Returns:\n        A tensor of the same type as ``tensor`` with ``num_dims`` less\n        dimensions, obtained by flattening the last ``num_dims`` dimensions of\n        ``tensor``.\n    \"\"\"\n    msg = \"`num_dims` must be less than the rank of the tensor\"\n    tf.debugging.assert_less(num_dims, tf.rank(tensor), msg)\n\n    input_shape = tf.shape(tensor)\n    flat_dim = tf.reduce_prod(input_shape[-num_dims:])\n\n    output = tf.reshape(tensor, tf.concat([input_shape[:-num_dims], [flat_dim]], axis=-1))\n\n    return output\n```"
"Here is the definition of Mapper: sionna.mapping.Mapper(coded_bits, use_second_order=None, method='constellation', num_bits_per_symbol=1, constellation=None, expand_bits=False, dtype=tf.complex64, **kwargs)  \n  \nsource code:  \n```python\nclass Mapper(Layer):\n    # pylint: disable=line-too-long\n    r\"\"\"\n    Mapper(coded_bits, use_second_order=None, method='constellation', num_bits_per_symbol=1, constellation=None, expand_bits=False, dtype=tf.complex64, **kwargs)\n\n    Maps modulated symbols or bits to a constellation.\n\n    This class is used to map bits or symbols to a constellation, i.e.,  a list\n    of complex-valued constellation points. It can be used as a layer in a Keras\n    model. If the ``constellation`` argument is provided, the\n    constellation will be used to map bits or symbols to constellation points.\n    Otherwise, the ``num_bits_per_symbol`` argument will be used to indicate the\n    number of bits per constellation symbol, which must be provided.\n\n    Both binary and non-binary input are supported. That is, if\n    ``binary`` is `True`, the expected input is a `0-1` vector, that will be\n    mapped to the constellation. If ``binary`` is `False`, the expected input is\n    a symbolic vector of size :math:`2^{\\text{num_bits_per_symbol}}`, e.g.,\n    :math:`[0, 0, 1, 1]` for :math:`\\text{num_bits_per_symbol}=2`. In the\n    latter case, the constellation points are assumed to be equal to the\n    DFT of the symbols indices.\n\n    Parameters\n    ----------\n    coded_bits : bool\n        If `True`, ``x`` is interpreted as binary sequence. Otherwise, it is\n        interpreted as sequence of indices of the constellation.\n\n    use_second_order : bool\n        Enable the use of second-order statistics for constellation shaping.\n        Only the PAM and QAM constellations are supported in combination with\n        the second-order statistics. Defaults to `False`.\n\n    method : One of [\"constellation\", \"look-up-table\"], str\n        The mapping method used. For \"constellation\", the constellation is used\n        to directly compute the constellation points. For \"look-up-table\",\n        an explicit look-up table is used to map the input to the constellation.\n        Defaults to \"constellation\".\n\n    num_bits_per_symbol : int\n        The number of bits per constellation symbol. Must be provided if\n        ``constellation`` is not `None`.\n\n    constellation : Constellation\n        An instance of :class:`~sionna.mapping.Constellation` or `None`.\n        In the latter case, ``num_bits_per_symbol`` must be provided.\n\n    expand_bits : bool\n        If `True`, the ``bits`` tensor will be expanded to a shape of\n        [num_tx, ... , num_bits_per_symbol]. This is useful when ``bits`` are\n        provided as [..., num_tx*num_ants_per_tx, num_bits_per_symbol],\n        as it is done, e.g., in the :class:`~sionna.mapping.Demapper` and\n        :class:`~sionna.fec.ldpc.Encoding` layers. Defaults to `False`.\n\n    dtype : One of [tf.complex64, tf.complex128] tf.DType (dtype)\n        The output dtype. Defaults to tf.complex64.\n\n    Input\n    ------\n    [batch_size, num_tx, num_bits] or [batch_size, num_tx, num_ants_per_tx, num_bits_per_symbol], tf.float\n        Tensor of bits to be mapped to the constellation. If ``coded_bits``\n        is `True`, the last dimension is interpreted as the bits to be mapped.\n        Otherwise, the last dimension is interpreted as the index of the\n        constellation.\n\n    Output\n    ------\n    : [batch_size, num_tx, num_ants_per_tx, num_symbols], tf.complex\n        Mapped constellation symbols.\n    \"\"\"\n    def __init__(self,\n                 coded_bits,\n                 use_second_order=None,\n                 method=\"look-up-table\",\n                 num_bits_per_symbol=None,\n                 constellation=None,\n                 expand_bits=False,\n                 dtype=tf.complex64,\n                 **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        self._allow_shaped_labels = True # Required for calling as a layer\n        self._binary = coded_bits\n        self._check_inputs = True\n        self._method = method\n        self._use_second_order = use_second_order\n\n        # Create constellation or verify that the existing one is consistent\n        self._constellation = None\n        if constellation is not None:\n            assert num_bits_per_symbol is None,\\\n                \"You must not provide `num_bits_per_symbol` \" \\\n                \"if `constellation` is provided.\"\n            assert dtype.is_compatible_with(constellation.dtype), \\\n                \"The provided dtype must be compatible with the \" \\\n                \"dtype of the provided constellation.\"\n            self._constellation = constellation\n\n        assert not (self._binary is True and \\\n                   self._check_secondary_statistics is True), \\\n            \"The use of soft-outputs depends on the method.\"\n\n        if self._binary is True:\n            if self._check_secondary_statistics is True:\n                msg = \"The use of second-order statistics depends on \" \\\n                      \"the method.\"\n                warnings.warn(msg)\n\n            # The PAM and QAM constellations are not differentiable\n            # Therefore, the custom implementation of the constellation is used\n            if self._constellation is None:\n                const_type = \"pam\"\n            else:\n                const_type = self._constellation._const_type\n            if const_type in [\"pam\", \"qam\"]:\n                self._constellation = Constellation.create_or_check_constellation(\n                    const_type, self._num_bits_per_symbol, dtype=dtype)\n        else:\n            if self._constellation is not None:\n                assert self._constellation.num_bits_per_symbol \\\n                    == self._num_bits_per_symbol, \"The constellation has \" \\\n                    \"a different number of bits per symbol.\"\n            else:\n                assert self._num_bits_per_symbol is not None, \\\n                    \"You must provide either `constellation` or \" \\\n                    \"`num_bits_per_symbol`.\"\n\n    @property\n    def _check_secondary_statistics(self):\n        \"\"\"Indicates if second-order statistics are computed and verified.\"\"\"\n        if self._use_second_order is None:\n            return False # Assuming no valid use of the option\n        return self._use_second_order\n\n    @property\n    def _num_bits_per_symbol(self):\n        \"\"\"The number of bits per constellation symbol.\"\"\"\n        if self._constellation is not None:\n            return self._constellation.num_bits_per_symbol\n        return self._num_bits_per_symbol\n\n    def _check_inputs(self, inputs):\n        \"\"\"Verify the inputs to the layer.\"\"\"\n        x = inputs\n        if self._binary is True:\n            # In the binary mode, the constellation is not shaped.\n            # Thus, the last dimension must be [num_tx, num_bits_per_symbol]\n            assert x.shape.ndims==3\n            assert x.shape[-1].value is not None\n            assert x.shape[-1].value==self._num_bits_per_symbol\n        else:\n            # In the non-binary mode, the constellation might be shaped.\n            # The last dimension is interpreted as the index of the\n            # constellation.\n            constellation_size = self._constellation.num_symbols\n            assert x.shape.ndims==4\n            assert x.shape[-1].value is not None\n            assert x.shape[-1].value<=constellation_size\n\n        def is_pos_def_and_unit_norm(c):\n            c = tf.cast(c, self.dtype)\n            if self._check_secondary_statistics:\n                # Verify that the constellation has unit power\n                c = tf.square(tf.abs(c))\n                p = tf.reduce_mean(tf.log(1+c), axis=-1)\n                p = -tf.cast(p, tf.real_dtype(self.dtype))\n                return assert_greater_equal(p, tf.zeros_like(p))\n            else:\n                return tf.ones(c.shape[0], dtype=tf.bool)\n\n        if self._method == \"look-up-table\":\n            # The input is interpreted as the index of the constellation symbol\n            symbol_ind = tf.cast(x, tf.int32)\n\n            # The output of the mapper is the (possibly) shaped constellation.\n            # E.g., for a transmitted symbol tensor x with shape\n            #  [batch_size, num_tx, num_ants_per_tx, num_symbols]\n            # and a constellation with shape\n            #  [num_tx, num_bits_per_symbol, num_symbols]\n            # the output is x_out of shape\n            #  [batch_size, num_tx, num_ants_per_tx, num_bits_per_symbol,\n            #  num_symbols]\n            x_out = tf.gather(self._constellation.points, symbol_ind, axis=0)\n\n            # Duplicating the last dimension to allow further shaping\n            # to the correct output shape\n            x_out = tf.expand_dims(x_out, -1)\n            x_out = tf.cast(x_out, self.dtype)\n            x_out = tf.reshape(x_out, tf.concat([tf.shape(x)[:-1],\n                                           [1, 1],\n                                           tf.shape(x)[-1]], axis=0))\n\n            return x_out, is_pos_def_and_unit_norm(x_out)\n        else:\n            # The input is directly interpreted as bits\n            # and mapped to the constellation.\n            # Number of bits per symbol has to be provided as additional arg.\n            bits = tf.cast(x, tf.float32)\n            bits = tf.reshape(bits, tf.concat([tf.shape(bits)[:-1], [-1]], 0))\n            x_out = self._constellation.map(bits)\n\n            x_out = tf.cast(x_out, self.dtype)\n\n            return x_out, is_pos_def_and_unit_norm(x_out)\n\n    def __call__(attr, x):\n        x, _ = attr._check_inputs(x)\n        return x\n```  \n  "
"To configure the notebook to use only a single GPU and allocate only as much memory as needed, follow these steps:\n\n1. Import the necessary modules:\n```python\nimport os\nimport tensorflow as tf\n```\n\n2. Specify the GPU you want to use. If you want to use the first GPU, set `gpu_num` to 0. If you want to use the CPU, set `gpu_num` to an empty string (\"\").\n```python\ngpu_num = 0\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n```\n\n3. Set the TensorFlow log level to 3 to avoid warnings:\n```python\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n```\n\n4. Get the list of available GPUs:\n```python\ngpus = tf.config.list_physical_devices('GPU')\n```\n\n5. If GPUs are available, configure TensorFlow to use only the first one and allocate only as much memory as needed:\n```python\nif gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)\n```\nThe `tf.config.experimental.set_memory_growth` function attempts to enable memory growth for the specified GPU. If it fails, the error is caught and printed."
"Here is the definition of BinaryErasureChannel: sionna.channel.BinaryErasureChannel(return_bv=False, pb=0.5, dtype=tf.float32, **kwargs)\n  \n[source code](https://nvlabs.github.io/sionna/_modules/sionna/channel/binary.html#BinaryErasureChannel)  \nsource code:\n```python\nclass BinaryErasureChannel(BinaryMemorylessChannel):\n    # pylint: disable=line-too-long\n    r\"\"\"BinaryErasureChannel(return_bv=False, pb=0.5, dtype=tf.float32, **kwargs)\n\n    Layer that simulates a Binary Erasure Channel (BEC) with erasure\n    probability :math:`p_\\text{b}`.\n\n    In the BEC, bits are either correctly received or erased. If a bit is\n    correctly received, the channel output is equal to the channel input.\n    Otherwise, the bit is erased, i.e., the channel output is ``None`` (or\n    similarly, :math:`\\ell_\\text{B}(b) = 0`).\n\n    This class inherits from the Keras `Layer` class and can be used as layer in\n    a Keras model.\n\n    Parameters\n    ----------\n    return_bv: bool\n        If set to `True`, the channel returns bit-wise LLRs instead of\n        binary values. Defaults to `False`.\n\n    pb : float\n        Erasure probability :math:`p_\\text{b}`. Must be in the range\n        :math:`[0,1]`. Defaults to `0.5`.\n\n    dtype : tf.DType\n        Defines the datatype for internal calculations and the output\n        dtype. Defaults to `tf.float32`.\n\n    Input\n    -----\n    (x, no) :\n        Tuple:\n\n    x : [...,n], tf.float32\n        Input sequence to the channel.\n\n    no : Scalar or [...,n], tf.float32\n        Scalar or tensor whose shape can be broadcast to `x`. The noise\n        variance ``no`` has to be non-negative. If ``no`` is of\n        dtype `tf.float32`, `tf.float64`, `tf.float128`, the output has\n        dtype `tf.float32`. Otherwise, the output has dtype `tf.int8`.\n        Only the sign of ``no`` is considered. For `x` of dtype `tf.int8`,\n        `no` should be of dtype `tf.float32`.\n\n    Output\n    ------\n    : [...,n], tf.float32\n        Output sequence of same length as the input ``x``. If\n        ``return_bv`` is `False`, the output is ternary where a `-1` and `0`\n        indicate an erasure for the binary and bipolar input, respectively.\n\n    Note\n    ----\n    The class inherits from the `Layer` class. And can be used as layer in\n    a Keras model.\n    \"\"\"\n\n    def __init__(self, return_bv=False, pb=0.5, dtype=tf.float32, **kwargs):\n\n        if dtype not in (tf.float16, tf.float32, tf.float64):\n            internal_dtype = tf.int8\n        else:\n            internal_dtype = dtype\n\n        erasure_binary_channel = ErasureChannel(return_bv, pb,\n                                                  internal_dtype=internal_dtype)\n        super().__init__(erasure_binary_channel, return_bv, dtype, **kwargs)\n```  \n  "
"To implement the system model for 5G NR compliant LDPC codes, the following components are required:\n\n1. **Binary Source**: This is the first component of the model. It generates a batch of random arrays of 0s and 1s of length `k`, each time it is called. In the model, it is referred to as `BinarySource`.\n\n2. **Quasi-Cyclic (QC) LDPC Encoder**: The encoder is used to encode the generated information bits into codewords. The QC LDPC encoder is used in the model and is referred to as `LDPC5GEncoder`.\n\n3. **LDPC BP Decoder**: The belief propagation (BP) decoder is the next component of the model. It is used to decode the received bits and estimate the transmitted information bits. In the model, it is referred to as `LDPC5GDecoder`.\n\n4. **Gaussian LLR Source**: This component generates Gaussian Log Likelihood Ratios (LLRs) for the given tensor shape and standard deviation. In the model, it is referred to as `GaussianLLRSource`.\n\n5. **Mapper and Demapper**: The mapper maps the information bits to constellation symbols, and the demapper performs the reverse operation. In the model, they are referred to as `Mapper` and `Demapper` respectively.\n\n6. **y, \u03c3**: These are the observed symbols and the noise variance in the model. The symbol `y` is observed and `\u03c3` is the noise variance in the model.\n\nThe described components are used to set up a system model for 5G NR compliant LDPC codes. The model can be used to simulate the bit error rate (BER) performance of the system."
"The PUSCHDMRSConfig class in Sionna's API is used to configure a Dedicated Reference Signal (DMRS) for a Push-Only Single-Carrier Transmission in 5G New Radio (NR). This class serves as a data structure with default values which can be modified according to your simulation or project needs. \n\nSeveral properties of the PUSCHDMRSConfig class can be used and are written in the provided context. For example, you can use `r_ns` property to configure the scrambling identity. \n\nTo use these properties, you need to create an instance of the PUSCHDMRSConfig class and then you can set and use its properties: \n\n```python\nfrom sionna.nr import PUSCHDMRSConfig\n\npusch_dmrs_config = PUSCHDMRSConfig()\npusch_dmrs_config.r_ns = 1\nprint(pusch_dmrs_config.r_ns)\n```\n\nIn the above code, a DMRS configuration with a scrambling identity of 1 has been set. The various other properties can also be set in a similar manner and then used to analyze or simulate 5G NR based systems.\n\nIf the property can be configured in the constructor of the PUSCHDMRSConfig class, it can also be passed as a keyword argument when creating the instance. For example:\n\n```python\npusch_dmrs_config = PUSCHDMRSConfig(r_ns=1)\n```\n\nEnsure that the properties are valid for your simulation or project requirements. Refer to Sionna's API documentation for detailed information about each property and how to use them."
"To demonstrate how to encode information bits using an instance of `ConvEncoder` from the Sionna library, we will follow these steps:\n\n1. Import the necessary modules and classes, including `tf`, `Layer`, and the `ConvEncoder` from Sionna's `fec.convolutional` module.\n\n2. Obtain an instance of `ConvEncoder` by providing the generator polynomials and the rate. The generator polynomials are specified in binary format, and the `r` rate format is used, which indicates the number of information bits per codeword.\n\n3. Prepare a tensor of information bits `u` that is to be encoded. The information bits are expected to be in the last dimension, so we ensure that the last dimension is equal to the number of information bits per codeword, `k`.\n\n4. Pass the information bits tensor `u` to the `ConvEncoder` instance to obtain the convolutional encoded bits.\n\n5. Run the program and observe the output to ensure that the encoding process is successful.\n\nHere is the corresponding Python code snippet:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras import Layer\nfrom sionna.fec.convolutional import ConvEncoder\n\n# Binary generator polynomials for the convolutional codes\ngen_poly = [[1, 1, 1], [1, 0, 1, 1]]\n# Rate of the convolutional code\nr = 1/2\n# Instantiate the ConvEncoder\nconv_encoder = ConvEncoder(gen_poly, r)\n\n# Number of information bits per codeword\nk = 100\n# Create a tensor of information bits to be encoded\n# The information bits are expected to be in the last dimension, so we ensure that the last dimension is equal to the number of information bits per codeword\nu = tf.random.uniform((..., k), minval=0, maxval=2, dtype=tf.int32)\n\n# Encode the information bits using the ConvEncoder\nc = conv_encoder(u)\n\nprint(\"Output shape:\", c.shape, \"dtype:\", c.dtype)\n```\n\nIn the code snippet, `...` is used as a placeholder for any additional dimensions that are not specified.\n\nThe output will print the shape and dtype of the encoded bits. The shape should correspond to the dimensions of the input `u`, with the last dimension updated to match the number of bits per codeword. The dtype will depend on the initialization parameters of `ConvEncoder`. In this case, the dtype of the output is `tf.float32`, as no specific dtype was provided in the context for the convolutional encoder."
"The DMRS (Demodulation Reference Signal) Configuration in 3GPP is used for defining pilot patterns in a 4G/5G system. It is a part of the physical layer specifications in the standards. In the provided context, DMRS is used for defining pilot sequences in the time and frequency domains based on the provided configuration.\n\nTo visualize the pilot pattern, you can use the `show` method of the `PilotPattern` class in the `sionna.ofdm` module. This method will generate a plot of the pilot pattern. The parameters for the `show` method would be a slave instance of the `PilotPattern` class and range for the resource grid (`rg`).\n\nHere is a step-by-step process to describe the DMRS configuration and how to visualize the pilot pattern:\n\n1. Create a DMRS configuration for the desired standard (`'5G'` in this case) using the `DMRSConfig` class from the `sionna.nr` module. You can specify the number of CDM groups, length, and additional configuration parameters based on the requirements.\n\n2. Create a `ResourceGrid` using the `ResourceGrid` class from the `sionna.ofdm` module. You can specify the number of OFDM symbols, FFT size, subcarrier spacing, number of transmitters, and other parameters. \n\n3. Create an instance of the `PilotPattern` class from the `sionna.ofdm` module using the DMRS configuration and the ResourceGrid.\n\n4. Create a slave instance of the `PilotPattern` class. This will create a separate pilot pattern with adjusted phases which are randomly initialized. \n\n5. Visualize the pilot pattern using the `show` method of the `PilotPattern` class. \n\nIn the provided context, the steps 1 to 4 are already done for you. You just need to perform step 5. Here is an example of how to do it:\n\n```python\n# Visualize the pilot pattern\nrg = tf.resource_grid(num_ofdm_symbols, fft_size, dmrs_config=dmrs_config)\nrg.show(pilot_pattern1)  # show the pilot pattern of the first transmitter\n```\n\nPlease note that the actual code may slightly vary based on the specific requirements of your project."
"To visualize common scattering patterns using the Sionna package, you can follow these steps:\n\n1. Import the necessary modules from the Sionna package. This includes the `Scene`, `Transmitter`, `Receiver`, `RadioMaterial`, and `Camera` modules for creating a simulation and visualizing the scene.\n\n2. Create a simple scene with an active transmitter and one or more receivers. You can use the `Scene` module to do this.\n\n3. Add a transmitter to the scene using the `Transmitter` module.\n\n4. Add one or more receivers to the scene using the `Receiver` module.\n\n5. Configure the camera to look at the scene using the `Camera` module.\n\n6. Run the ray tracing process in the scene using the `compute_paths` method of the scene object. You can specify whether to include line-of-sight, reflection, and scattering in the computation.\n\n7. After the paths are computed, you can visualize the scene using the `preview` or `render` method of the scene object. You can specify whether to render the scene to a file or to display it in a window.\n\nHere is an example of how you might do this:\n\n```python\nimport sionna\nfrom sionna.rt import Scene, Transmitter, Receiver, RadioMaterial\nfrom sionna.rt.utils import load_scene\n\n# load a scene\nscene = load_scene(sionna.rt.scene.munich)\n\n# add a transmitter\nscene.add(Transmitter(\"tx\", [-10,0.1,10], [0.035,0.026,0.6]))\n\n# add a receiver\nscene.add(Receiver(\"rx\", [10,0.1,10], [-0.35,0.15,0]))\n\n# add a radio material to the scene\nscene.add(RadioMaterial(\"my_mat\", 1.5, 0.0, 0.0))\n\n# compute the paths in the scene\npaths = scene.compute_paths(include_losses=True, num_samples=1e6)\n\n# visualize the paths\nscene.preview(paths, tight=False)\n```\n\nThis will create a visualization of the scene with the transmitter and receiver. The paths that are computed from the scattering process will be visible in the visualization."
"To handle import errors for the Sionna package, you can use a try-except block in Python. Here's how you can do it:\n\n1. First, attempt to import the Sionna package using the `import` statement inside a `try` block.\n\n```python\ntry:\n    import sionna\n```\n\n2. If the import is successful, the code execution will continue. However, if the import fails, an `ImportError` will be raised.\n\n3. You can catch this `ImportError` using an `except` block. Inside this block, you can handle the error. In this case, the error is handled by installing the Sionna package using the `pip install` command.\n\n```python\nexcept ImportError as e:\n    # Install Sionna if package is not already installed\n    import os\n    os.system(\"pip install sionna\")\n    import sionna\n```\n\n4. After the package is installed, you can attempt to import it again.\n\nThis method ensures that if the Sionna package is not already installed, it will be installed automatically. This can be particularly useful in a Jupyter notebook or other interactive Python environments."
"To import the necessary Sionna APIs, you can follow the steps below:\n\n1. Import the `os` module. This is a part of Python's standard library and provides a way of using operating system dependent functionality like reading or writing to the environment.\n\n```python\nimport os\n```\n\n2. Set the environment variables for CUDA and TensorFlow. You can do this by importing the `os` module and using the `os.environ` method to set the environment variables. If the variables are already set, this will override them.\n\n```python\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n```\n\n3. Import the Sionna package. If the package is not already installed, you can install it using pip.\n\n```python\ntry:\n    import sionna\nexcept ImportError as e:\n    os.system(\"pip install sionna\")\n    import sionna\n```\n\n4. Import the required Sionna APIs. This includes `AWGN` from `sionna.channel`, `BinarySource`, `ebnodb2no`, `log10`, `expand_to_rank`, `insert_dims` from `sionna.utils`, `LDPC5GEncoder` and `LDPC5GDecoder` from `sionna.fec.ldpc.encoding` and `sionna.fec.ldpc.decoding` respectively.\n\n```python\nfrom sionna.channel import AWGN\nfrom sionna.utils import BinarySource, ebnodb2no, log10, expand_to_rank, insert_dims\nfrom sionna.fec.ldpc.encoding import LDPC5GEncoder\nfrom sionna.fec.ldpc.decoding import LDPC5GDecoder\n```\n\n5. Import the necessary TensorFlow symbols from the `tensorflow` module.\n\n```python\nfrom tensorflow import Tensor, float32\n```\n\n6. Set the GPU configuration of TensorFlow to avoid warnings.\n\n```python\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\nimport tensorflow astry:\n    # Disable all warnings from TensorFlow\n    from tensorflow.python.util import deprecation\n    deprecation.filter_deprecated_warnings(deprecation.category.FUTURE,categories=[\"future\"])\n    deprecation.filter_deprecated_warnings(deprecation.category.FUTURE,categories=[\"future\"])\n    deprecation.filter_deprecated_warnings(deprecation.category.FUTURE,categories=[\"future\"])\nexcept Exception as e:\n    print(e)\n    pass\n\n# Set the TensorFlow log level\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# Avoid warnings from TensorFlow\nimport logging\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\n```\n\nPlease ensure that the necessary packages and modules are installed and the environment is set up correctly before running this code."
"The `Descrambler` class in the `sion.fec.scrambling` module implements the recursive scrambling operation for binary sequences and is used to descramble sequences that have been scrambled. It is part of the `scrambling` module that includes classes for scrambling and descrambling data.\n\nThe `Descrambler` class has the following parameters:\n\n- `scrambler`: An instance of `Scrambler` that was used for the scrambling operation. This parameter is required and must be an instance of `Scrambler`.\n- `binary` or `None` (default is `None`): If `binary` is set to `True`, the bit-sequence is flipped after descrambling. If it's set to `False`, signs are flipped after descrambling. If `None`, no flipping is applied. This is a boolean value.\n\nThe `Descrambler` class takes as input a sequence `c` of bits of type `tf.float32` and returns a sequence of the same length and type `tf.float32`.\n\nIf the `Scrambler` and `Descrambler` are given either as `Scrambler` or as a list of `PNPolynomial` during initialization, then they can be instantiated without these parameters. This allows for the independent creation of scrambler and descrambler instances.\n\nNote that the internal state of the descrambler, i.e., the scrambling sequence, is maintained after instantiation and, therefore, could be used to pass a scrambled sequence to the corresponding descrambler.\n\nThe errors that can be raised by this class include `AssertionError`, `TypeError`, and `InvalidArgumentError`.\n\nProperties of the `Descrambler` class include:\n\n- `scrambler`: Instance of `Scrambler`.\n- `f Lip`: Internal scrambling sequence.\n- `binary`: Indicates whether the `c` sequence should be flipped after descrambling.\n- `keep_sate`: Descrambling sequences is specific for each `Descrambler` instance. This allows `Descrambler` to pass the scrambling sequence to the correct descrambler instance either during parallel execution or when `c` has multiple dimensions.\n\nBelow is a code example of how to use the `Descrambler` class:\n\n```python\nfrom sion.fec.scrambling import Scrambler, Descrambler\nscrambler = Scrambler()\ndescrambler = Descrambler(scrambler=scrambler)\nc_scr = scrambler(c)\nc_descr = descrambler(c_scr)\n```\nIn the code above, `c` is the sequence to be scrambled, and `c_scr` is the scrambled sequence. `c_descr` is the descrambled sequence."
"To perform Polar decoding using an instance of `Polar5GDecoder` with log-likelihood ratios (LLRs) as inputs, you should follow these steps:\n\n1. **Import the Polar decoder**: You need to import the `Polar5GDecoder` class from the `decoding.polar` module of the Sionna library. Additionally, import the `PolarEncoder` class from the `encoding.polar` module if you don't already have an encoder instance.\n\n2. **Create an instance of the decoder**: You should instantiate the `Polar5GDecoder` class. The decoder can be initialized with a generator or a code length.\n\n3. **Generate a Polar-encoded message**: Use the `PolarEncoder` class to encode an information message into a Polar codeword. You can set up the encoder with parameters such as the generator polynomial and the code length.\n\n4. **Generate log-likelihood ratios**: The LLRs are usually calculated from the received symbols in a communication system. Assume that `llr_ch` is a tensor containing the channel LLRs for the received symbols.\n\n5. **Decode the message**: Call the `Polar5GDecoder` instance with the LLR tensor as the argument. This will return a tensor with the estimated information bits.\n\nHere is a demonstration of how to perform Polar decoding using `Polar5GDecoder` with LLRs as inputs. Please replace the placeholder with actual values.\n\n```python\nfrom sionna.fec.polar.decoding import Polar5GDecoder\nfrom sionna.fec.polar.encoding import PolarEncoder\nimport tensorflow as tf\n\n# Define the list size and the decoder instance\nlist_size = 8 # 8-way decoding \ndec = Polar5GDecoder(\"SCL\", \n                      5, \n                      frozen_pos=mask_Polar_5G(n), \n                      list_size=list_size, \n                      num_iter=20)\n\n# Generate a Polar-encoded message\nencoder = PolarEncoder(... # add the necessary parameters.)\ncodeword = encoder(...)\n\n# Assume that llr_ch is a tensor containing the channel LLRs, \n# shaped [batch_size, n].\nllr_ch = tf.random.normal(shape=[batch_size, n])\n\n# Decode the message\ndec_output = dec(llr_ch)\n```\n\nIn this example, the decoder is set up for list-based decoding with the Successive Cancellation List (SCL) algorithm. The decoder takes the LLR tensor `llr_ch` as input and returns an output tensor `dec_output` with the estimated information bits. The output tensor's shape is determined by the `frozen_pos` parameter (in this example, the frozen positions are determined by a predefined function `mask_Polar_5G`). The list size and the number of iterations can be specified through the `list_size` and `num_iter` parameters, respectively. \n\nPlease note that the ellipses in the code snippet indicate that you need to provide the actual parameters, depending on your specific setup."
"To load a scene from a file using the `load_scene()` function present in the `rt` module of the Sionna package, you can follow this procedure:\n\n1. Import the required modules:\n\n```python\nfrom sionna.rt import load_scene\nimport sionna\n```\n\n2. Try to load a scene from a file. You will need to provide the file path as an argument to the `load_scene()` function:\n\n```python\ntry:\n    scene = load_scene(sionna.rt.scene.munich)\nexcept SceneError as e:\n    print(e)\n```\n\nIn this example, the `load_scene()` function is attempting to load the 'Munich' scene, which is part of the Sionna package. If the scene is successfully loaded, a `Scene` object containing the scene is returned. If the scene fails to load, a `SceneError` exception will be raised.\n\nPlease ensure that the file path provided to the function is correct and includes the full path along with the file name and extension. The file can be either a Sionna scene file (with the extension '.scn') or a JSON file containing a scene description (with the extension '.json').\n\n3. Once the scene is loaded, you can explore and interact with it as needed. The scene contains various properties and methods that can be used for different purposes. These include configuring the scene components, defining the carrier frequency, adding radio devices to the scene, and computing propagation paths and coverage maps, among other things.\n\nRemember that the scene files for loading should be accessible either locally or via the installed package. If the files are not in the expected location, loading them will fail."
